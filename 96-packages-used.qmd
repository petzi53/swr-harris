# Packages used {#sec-annex-a}

## BSDA

:::::{.my-resource}
:::{.my-resource-header}
BSDA: Basic Statistics and Data Analysis 
:::
::::{.my-resource-container}

***

::: {#pak-BDSA}

***

{**BSDA**}: [Basic Statistics and Data Analysis](https://alanarnholt.github.io/BSDA/) [@BSDA]

(*There is no hexagon logo for {**BSDA**} available*)

Functions and data sets for the text Basic Statistics and Data Analysis (BSDA) [@kitchens2002].


{**BSDA**}: Basic Statistics and Data Analysis
:::

***
::::
:::::

## broom

:::::{.my-resource}
:::{.my-resource-header}
broom: Convert Statistical Objects into Tidy Tibbles 
:::
::::{.my-resource-container}

***

::: {#pak-broom}

***

{**broom**}: [Convert Statistical Objects into Tidy Tibbles](https://broom.tidymodels.org/) [@broom]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap09/logoi/logo-broom-min.png){width="176"}


Summarizes key information about statistical objects in tidy tibbles. This makes it easy to report results, create plots and consistently work with large numbers of models at once.

:::

{**broom**} provides three verbs to make it convenient to interact with model objects:

- `tidy()` summarizes information about model components
- `glance()` reports information about the entire model
- `augment()` adds informations about observations to a dataset

For a detailed introduction, please see [Introduction to broom](https://broom.tidymodels.org/articles/broom.html).

broom tidies 100+ models from popular modelling packages and almost all of the model objects in the stats package that comes with base R. 

The vignette [Available methods](https://broom.tidymodels.org/articles/available-methods.html) lists method availability.

{**broom**}: Convert Statistical Objects into Tidy Tibbles
:::


***
::::
:::::



## car

:::::{.my-resource}
:::{.my-resource-header}
car: Companion to Applied Regression 
:::
::::{.my-resource-container}

***

::: {#pak-car}

***

{**car**}: [Companion to Applied Regression](https://www.john-fox.ca/Companion/index.html) [@car]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap06/logoi/logo-car-min.png){width="176"}

Functions to Accompany J. Fox and S. Weisberg, An R Companion to Applied Regression, Third Edition, Sage, 2019. [@fox2018]

:::
An R Companion to Applied Regression is a broad introduction to the R statistical computing environment in the context of applied regression analysis. The book provides a step-by-step guide to using the free statistical software R, and emphasizes integrating statistical computing in R with the practice of data analysis.  The R packages car and effects, written to facilitate the application and interpretation of regression analysis, are extensively covered in the book.

{**car**}: Companion to Applied Regression
:::

***
::::
:::::



## colorblindcheck

:::::{.my-resource}
:::{.my-resource-header}
colorblindcheck: Check Color Palettes for Problems with Color Vision Deficiency
:::
::::{.my-resource-container}

***

::: {#pak-colorblindcheck}

***

{**colorblindcheck**}: [Check Color Palettes for Problems with Color Vision Deficiency](https://jakubnowosad.com/colorblindcheck/) [@colorblindcheck]

{*There is no hexagon sticker available for {**colorblindcheck**}.*}

Compare color palettes with simulations of color vision deficiencies - deuteranopia, protanopia, and tritanopia. It includes calculation of distances between colors, and creating summaries of differences between a color palette and simulations of color vision deficiencies.

Deciding if a color palette is a colorblind friendly is a hard task. This cannot be done in an entirely automatic fashion, as the decision needs to be confirmed by visual judgments. The goal of {**colorblindcheck**} is to provide tools to decide if the selected color palette is colorblind friendly, including:

- `palette_dist()` - Calculation of the distances between the colors in the input palette and between the colors in simulations of the color vision deficiencies: deuteranopia, protanopia, and tritanopia.
- `palette_plot()` - Plotting of the original input palette and simulations of color vision deficiencies: deuteranopia, protanopia, and tritanopia.
- `palette_check()` - Creating summary statistics comparing the original input palette and simulations of color vision deficiencies: deuteranopia, protanopia, and tritanopia.



{**colorblindcheck**}: A Package for Check Color Palettes for Problems with Color Vision Deficiency
:::


***
::::
:::::


## colorblindr

:::::{.my-resource}
:::{.my-resource-header}
colorblindr: Simulate colorblindness in R figures 
:::
::::{.my-resource-container}

***

::: {#pak-colorblindr}

***

{**colorblindr**}: [Simulate colorblindness in R figures](https://github.com/clauswilke/colorblindr) [@colorblindr]


(*There is no hexagon logo for {**colorblindr**} available*)

Provides a variety of functions that are helpful to simulate the effects of colorblindness in R figures. Complete figures can be modified to simulate the effects of various types of colorblindness. The resulting figures are standard grid objects and can be further manipulated or outputted as usual.

{**colorblindr**}: A Package for Simulate colorblindness in R figures
:::

***
::::
:::::

## colorspace

:::::{.my-resource}
:::{.my-resource-header}
colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes 
:::
::::{.my-resource-container}

***

::: {#pak-colorspace}

***

{**colorspace**}: [A Toolbox for Manipulating and Assessing Colors and Palettes](URL to package) [biblio]

(*There is no hexagon logo for {**colorspace**} available*)

The colorspace package provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualizations.

At the core of the package there are various utilities for computing with color spaces (as the name of the package conveys). Thus, the package helps to map various three-dimensional representations of color to each other. A particularly important mapping is the one from the perceptually-based and device-independent color model HCL (Hue-Chroma-Luminance) to standard Red-Green-Blue (sRGB) which is the basis for color specifications in many systems based on the corresponding hex codes (e.g., in HTML but also in R). For completeness further standard color models are included as well in the package: polarLUV() (= HCL), LUV(), polarLAB(), LAB(), XYZ(), RGB(), sRGB(), HLS(), HSV().

The HCL space (= polar coordinates in CIELUV) is particularly useful for specifying individual colors and color palettes as its three axes match those of the human visual system very well: Hue (= type of color, dominant wavelength), chroma (= colorfulness), luminance (= brightness).

There is extensive documentation available. See also the website on [HCL Color Space](https://hclwizard.org/):

> The hclwizard provides tools for manipulating and assessing colors and palettes based on the underlying colorspace software (available in R and Python). It leverages the HCL color space: a color model that is based on human color perception and thus makes it easy to choose good color palettes by varying three color properties: Hue (= type of color, dominant wavelength) - Chroma (= colorfulness) - Luminance (= brightness). As shown in the color swatches below each property can be varied while keeping the other two properties fixed.

{**colorspace**}: A Toolbox for Manipulating and Assessing Colors and Palettes
:::

This toolbox package is very important: All of the other color palette related package uses {**colorspace**} as a bases for their functionality. 

***
::::
:::::


## cowplot

::: my-resource
::: my-resource-header
cowplot: Streamlined Plot Theme and Plot Annotations for 'ggplot2'
:::

::: my-resource-container

------------------------------------------------------------------------

::: {#pak-cowplot}

------------------------------------------------------------------------

{**cowplot**}: [Streamlined Plot Theme and Plot Annotations for
'ggplot2'](https://wilkelab.org/cowplot/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap03/logoi/logo-cowplot-min.png){width="176"}

The {**cowplot**} package provides various features that help with
creating publication-quality figures, such as a set of themes, functions
to align plots and arrange them into complex compound figures, and
functions that make it easy to annotate plots and or mix plots with
images. The package was originally written for internal use in the Wilke
lab, hence the name (Claus O. Wilke’s plot package). It has also been
used extensively in the book [Fundamentals of Data
Visualization](https://www.amazon.com/gp/product/1492031089).
:::

There are several packages that can be used to align plots. The most
widely used ones beside {**cowplot**} are {**egg**} and {**patchwork**}
(see @pak-patchwork). All these packages use slightly different
approaches to plot alignment, and the respective approaches have
different strengths and weaknesses. If you cannot achieve your desired
result with one of these packages try another one.

Most importantly, while {**egg**} and {**patchwork**} align and arrange
plots at the same time, {**cowplot**} aligns plots independently of how
they are arranged. This makes it possible to align plots and then
reproduce them separately, or even overlay them on top of each other.

The {**cowplot**} package now provides a set of complementary themes
with different features. I now believe that there isn’t one single theme
that works for all figures, and therefore I recommend that you always
explicitly set a theme for every plot you make.

{**cowplot**}: A Package for Streamlined Plot Themes and Plot
Annotations for {**ggplot2**}
:::

:::
:::

## cranlogs

:::::{.my-resource}
:::{.my-resource-header}
cranlogs: Download Logs from the 'RStudio' 'CRAN' Mirror 
:::
::::{.my-resource-container}

***

::: {#pak-cranlogs}

***

{**cranlogs**}: [Download Logs from the RStudio CRAN Mirror](https://r-hub.github.io/cranlogs/) [@cranlogs]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap05/logoi/logo-cranlogs-min.png){width="176"}


`r glossary("APIx", "API")` to the database of `r glossary("CRAN")` package downloads from the RStudio CRAN mirror. The database itself is at <http://cranlogs.r-pkg.org>,
    see <https://github.com/r-hub/cranlogs.app> for the raw API.

:::

RStudio publishes the download logs from their CRAN package mirror daily at http://cran-logs.rstudio.com.

This R package queries a web API maintained by R-hub that contains the daily download numbers for each package.

The RStudio CRAN mirror is not the only CRAN mirror, but it’s a popular one: it’s the default choice for RStudio users. The actual number of downloads over all CRAN mirrors is unknown.


{**cranlogs**}: Download Logs from the 'RStudio' 'CRAN' Mirror
:::

***
::::
:::::


## crosstable

:::::{.my-resource}
:::{.my-resource-header}
crosstable: Crosstables for Descriptive Analyses 
:::
::::{.my-resource-container}

***

::: {#pak-crosstable}

***

{**crosstable**}: [Crosstables for Descriptive Analyses](https://danchaltiel.github.io/crosstable/) [@crosstable]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap05/logoi/logo-crosstable-min.png){width="176"}


Crosstable is a package centered on a single function, crosstable, which easily computes descriptive statistics on datasets. It can use the {**tidyverse**} syntax and is interfaced with the package {**officer**} to create automatized reports.

:::

Create descriptive tables for continuous and categorical variables. Apply summary statistics and counting function, with or without a grouping variable, and create beautiful reports using {**rmarkdown**} or {**officer**}. You can also compute effect sizes and statistical tests if needed.

{**crosstable**}: Crosstables for Descriptive Analyses
:::

I believe that the main usage for this package is to prepare ready-to-print tables. Similar like {**gtsummary**} (see @pak-gtsummary) it provides some descriptive statistics with many display options. But I got the impression that analysis of data is not the main usage of these packages. 

For instance you could use `crosstable::display_test(chisq.test(x, y))` to get as result a string, for instance: p value: <0.0001 \n(Pearson's Chi-squared test)". This is nice to include for a table, but for the analysis one would also need the values of the different cells.

***
::::
:::::



## curl

:::::{.my-resource}
:::{.my-resource-header}
curl: A Modern and Flexible Web Client for R 
:::
::::{.my-resource-container}

***

::: {#pak-curl}

***

{**curl**}: [A Modern and Flexible Web Client for R](https://cran.r-project.org/web/packages/curl/vignettes/intro.html) [@curl]


(*There is no hexagon logo for {**curl**} available*)

The `curl()` and `curl_download(`) functions provide highly configurable drop-in replacements for base `url()` and `download.file()` with better performance, support for encryption (https, ftps), gzip compression, authentication, and other 'libcurl' goodies. 

The core of the package implements a framework for performing fully customized requests where data can be processed either in memory, on disk, or streaming via the callback or connection interfaces. Some knowledge of 'libcurl' is recommended; for a more-user-friendly web client see the 'httr' package which builds on this package with http specific tools and logic.

{**curl**}: A Modern and Flexible Web Client for R        
:::


***
::::
:::::



## data.table

:::::{.my-resource}
:::{.my-resource-header}
data.table: Extension of `data.frame` 
:::
::::{.my-resource-container}

***

::: {#pak-data-table}

***

{**data.table**}: [Extension of `data.frame`](URL to package) [biblio]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-data.table-min.png){width="176"}
{**data.table**} provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed.

:::

Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, friendly and fast character-separated-value read/write. Offers a natural and flexible syntax, for faster development.

**Features**

- fast and friendly delimited file reader: `data.table::fread()`, see also convenience features for small data
- fast and feature rich delimited file writer: `data.table::fwrite()`
- low-level parallelism: many common operations are internally parallelized to use multiple CPU threads
- fast and scalable aggregations; e.g. 100GB in RAM (see benchmarks on up to two billion rows)
- fast and feature rich joins: ordered joins (e.g. rolling forwards, backwards, nearest and limited staleness), overlapping range joins (similar to IRanges::findOverlaps), non-equi joins (i.e. joins using operators >, >=, <, <=), aggregate on join (by=.EACHI), update on join
- fast add/update/delete columns by reference by group using no copies at all
- fast and feature rich reshaping data: `data.table::dcast()` (pivot/wider/spread) and `data.table::melt()` (unpivot/longer/gather)
- any R function from any R package can be used in queries not just the subset of functions made available by a database backend, also columns of type list are supported
- has no dependencies at all other than base R itself, for simpler production/maintenance
- the R dependency is as old as possible for as long as possible, dated April 2014, and we continuously test against that version; e.g. v1.11.0 released on 5 May 2018 bumped the dependency up from 5 year old R 3.0.0 to 4 year old R 3.1.0

{**data.table**}: A Package for the Extension of `data.frame`
:::

I believe the most important application of {**data.table**} is working with huge amount of data (several GB). In the book SwR it is used in this first chapter with the `data.table::fread()` function. I have used here the `readr::read_csv()` as part of the {**tidyverse**} collection, because the dataset is very small (29 kB).


With {**DT**} there is another package that seems important. It is a wrapper of the JavaScript library 'DataTables' (See @pak-DT). I was using already {**DT**} to display interactive tables on websites, but I do not understand completely the difference between {**data.table**} and {**DT**}. 
As far as I understood:

- Using {**data.table**} is very fast but you will loose the inutitive function of the tidyverse.
- {**DT**} is for the presentation of (large) interactive tables whereas {**data.table**} is for the fast manipulation of huge data sets.

***
::::
:::::

## datawizard

:::::{.my-resource}
:::{.my-resource-header}
datawizard: Easy Data Wrangling and Statistical Transformations 
:::
::::{.my-resource-container}

***

::: {#pak-datawizard}

***

{**datawizard**}: [Easy Data Wrangling and Statistical Transformations](https://easystats.github.io/datawizard/) [@datawizard]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap06/logoi/logo-datawizard-min.png){width="176"}



{**datawizard**} is a lightweight package to easily manipulate, clean, transform, and prepare your data for analysis. It is part of the {**easystats**} ecosystem, a suite of R packages to deal with your entire statistical analysis, from cleaning the data to reporting the results.

:::

{**datawizard**} covers two aspects of data preparation:

- **Data manipulation**: datawizard offers a very similar set of functions to that of the tidyverse packages, such as a {**dplyr**} and {**tidyr**}, to select, filter and reshape data, with a few key differences. 
    1) All data manipulation functions start with the prefix `data_*` (which makes them easy to identify). 
    2) Although most functions can be used exactly as their tidyverse equivalents, they are also string-friendly (which makes them easy to program with and use inside functions). 
    3) Finally, datawizard is super lightweight (no dependencies, similar to {**poorman**}), which makes it awesome for developers to use in their packages.
- Statistical transformations: {**datawizard**} also has powerful functions to easily apply common data transformations, including standardization, normalization, rescaling, rank-transformation, scale reversing, recoding, binning, etc.

{**datawizard**}: Easy Data Wrangling and Statistical Transformations
:::


***
::::
:::::



## descr

:::::{.my-resource}
:::{.my-resource-header}
descr: Descriptive Statistics
:::
::::{.my-resource-container}
***

::: {#pak-descr}

***

{**descr**}: [Descriptive Statistics](https://github.com/jalvesaq/descr)

There is no logo available for {**descr**}


Weighted frequency and contingency tables of categorical variables and of the comparison of the mean value of a numerical variable by the levels of a factor, and methods to produce xtable objects of the tables and to plot them. There are also functions to facilitate the character encoding conversion of objects, to quickly convert fixed width files into csv ones, and to export a data.frame to a text file with the necessary R and SPSS codes to reread the data. [@descr]

Package for Descriptive Statistics
:::

***

::::
:::::

## DescTools

:::::{.my-resource}
:::{.my-resource-header}
DescTools: Tools for Descriptive Statistics 
:::
::::{.my-resource-container}

***

::: {#pak-DescTools}

***

{**DescTools**}: [Tools for Descriptive Statistics](https://andrisignorell.github.io/DescTools/) [@DescTools]


(*There is no hexagon logo for {**DescTools**} available*)

:::

A collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data. The author's intention was to create a toolbox, which facilitates the (notoriously time consuming) first descriptive tasks in data analysis, consisting of calculating descriptive statistics, drawing graphical summaries and reporting the results. 

The package contains furthermore functions to produce documents using MS Word (or PowerPoint) and functions to import data from Excel. Many of the included functions can be found scattered in other packages and other sources written partly by Titans of R. The reason for collecting them here, was primarily to have them consolidated in ONE instead of dozens of packages (which themselves might depend on other packages which are not needed at all), and to provide a common and consistent interface as far as function and arguments naming, NA handling, recycling rules etc. are concerned. Google style guides were used as naming rules (in absence of convincing alternatives). The 'BigCamelCase' style was consequently applied to functions borrowed from contributed R packages as well.

{**DescTools**}: Tools for Descriptive Statistics
:::

***
::::
:::::







## dichromat

:::::{.my-resource}
:::{.my-resource-header}
dichromat: Color Schemes for Dichromats 
:::
::::{.my-resource-container}

***

::: {#pak-dichromat}

***

{**dichromats**}: [Color Schemes for Dichromats](https://cran.r-project.org/package=dichromat) [@dichromat]


Collapse red-green or green-blue distinctions to simulate the effects of different types of color-blindness.

{**dichromat**}: A Package for Color Schemes for Dichromats
:::

<Other text not included in the package reference>

***
::::
:::::




## dplyr

:::::{.my-resource}
:::{.my-resource-header}
dplyr:: A Grammar of Data Manipulation
:::
::::{.my-resource-container}



***

::: {#pak-dplyr}

***

{**package**}: [A Grammar of Data Manipulation](https://dplyr.tidyverse.org/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-dplyr-min.png){width="176"}

{**dplyr**} is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges:
- mutate() adds new variables that are functions of existing variables
- select() picks variables based on their names.
- filter() picks cases based on their values.
- summarise() reduces multiple values down to a single summary.
- arrange() changes the ordering of the rows. [@dplyr]

:::

These all combine naturally with group_by() which allows you to perform any operation “by group”. You can learn more about them in [vignette("dplyr")](https://dplyr.tidyverse.org/articles/dplyr.html). As well as these single-table verbs, dplyr also provides a variety of two-table verbs, which you can learn about in [vignette("two-table")](https://dplyr.tidyverse.org/articles/two-table.html). [@dplyr]


Package for Data Manipulation
:::





::::
:::::

## DT

:::::{.my-resource}
:::{.my-resource-header}
DT: A Wrapper of the JavaScript Library 'DataTables'
:::
::::{.my-resource-container}

***

::: {#pak-DT}

***

{**DT**}: [https://github.com/rstudio/DT](https://rstudio.github.io/DT/)

(*There is no hexagon icon available for {**DT**}*)

Data objects in R can be rendered as HTML tables using the JavaScript library [DataTables](https://datatables.net/) (typically via R Markdown or Shiny). The 'DataTables' library has been included in this R package. The package name {**DT**} is an abbreviation of 'DataTables'.

{**DT**}: A Package as Wrapper of the JavaScript Library 'DataTables'
:::

What I do not understand: What is the relationship of {**DT**} to the {**data.table**} package?
***
::::
:::::

## dunn.test

:::::{.my-resource}
:::{.my-resource-header}
dunn.test: Dunn's Test of Multiple Comparisons Using Rank Sums 
:::
::::{.my-resource-container}

***

::: {#pak-dunn.test}

***

{**dunn.test**}: [Dunn's Test of Multiple Comparisons Using Rank Sums](https://cran.r-project.org/package=dunn.test) [@dunn.test]

Computes Dunn's test [@dunn1964] for stochastic dominance and reports the results among multiple pairwise comparisons after a Kruskal-Wallis test for stochastic dominance among k groups [@kruskal1952. The interpretation of stochastic dominance requires an assumption that the CDF of one group does not cross the CDF of the other. 

{**dunn.test**} makes k(k-1)/2 multiple pairwise comparisons based on Dunn's z-test-statistic approximations to the actual rank statistics. The null hypothesis for each pairwise comparison is that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group equals one half; this null hypothesis corresponds to that of the `r glossary("Mann-Whitney", "Wilcoxon-Mann-Whitney rank-sum test")`. Like the rank-sum test, if the data can be assumed to be continuous, and the distributions are assumed identical except for a difference in location, Dunn's test may be understood as a test for median difference. {**dunn.test**} accounts for tied ranks.

{**dunn.test**}: Dunn's Test of Multiple Comparisons Using Rank Sums
:::

***
::::
:::::


## e1071

:::::{.my-resource}
:::{.my-resource-header}
e1071: Misc. functions 
:::
::::{.my-resource-container}

***

::: {#pak-e1071}

***

{**e1071**}: [Misc. functions](https://cran.r-project.org/web/packages/e1071/index.html) [@e1071]


(*There is no hexagon logo for {**e1071**} available*)

Functions for latent class analysis, short time Fourier transform, fuzzy clustering, support vector machines, shortest path computation, bagged clustering, naive Bayes classifier, generalized k-nearest neighbour ...


{**e1071**}: Misc. functions
:::


***
::::
:::::





## effectsize

:::::{.my-resource}
:::{.my-resource-header}
effectsize: Indices of Effect Size 
:::
::::{.my-resource-container}

***

::: {#pak-effectsize}

***

{**effectsize**}: [Indices of Effect Size](https://easystats.github.io/effectsize/) [@effectsize]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap06/logoi/logo-effectsize-min.png){width="176"}

The goal of this package is to provide utilities to work with indices of effect size and standardized parameters, allowing computation and conversion of indices such as Cohen’s d, r, odds-ratios, etc.

:::

Provide utilities to work with indices of effect size for a wide  variety of models and hypothesis tests (see list of supported models using the function 'insight::supported_models()'), allowing computation of and  conversion between indices such as Cohen's d, r, odds, etc.

{**effectsize**}: Indices of Effect Size
:::



***
::::
:::::




## fmsb

:::::{.my-resource}
:::{.my-resource-header}
fmsb: Functions for Medical Statistics Book with some Demographic Data 
:::
::::{.my-resource-container}

***

::: {#pak-fmsb}

***

{**fmsb**}: [Functions for Medical Statistics Book with some Demographic Data](https://cran.r-project.org/package=fmsb) [@fmsb]

(*There is no hexagon logo for {**fmsb**} available*)


Several utility functions for the book entitled "Practices of Medical and Health Data Analysis using R" (Pearson Education Japan, 2007) with Japanese demographic data and some demographic analysis related functions.

{**fmsb**}: Functions for Medical Statistics Book with some Demographic Data
:::

***
::::
:::::


## forcats

:::::{.my-resource}
:::{.my-resource-header}
forcats: Tools for Working with Categorical Variables
:::
::::{.my-resource-container}



***

::: {#pak-forcats}

***

{**forcats**}: [Tools for Working with Categorical Variables (Factors)](https://forcats.tidyverse.org/)

{**forcats**} provide a suite of useful tools that solve common problems with factors.
"Forcats" is an anagram of "factors" and part of the {**tidyverse**} suite of packages.

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-forcats-min.png){width="176"}

(1) reordering factor levels
    - moving specified levels to front, 
    - ordering by first appearance, 
    - reversing, and 
    - randomly shuffling
(2) tools for modifying factor levels
    - collapsing rare levels into other, 
    - 'anonymizing', and
    - manually 'recoding'
:::

forcats: A Package with Tools for Working with Categorical Variables

:::

***


::::
:::::

## GGally

:::::{.my-resource}
:::{.my-resource-header}
{**GGally**}: Extension to {**ggplot2**} 
:::
::::{.my-resource-container}

***

::: {#pak-GGally}

***

{**GGally**}: [Extension to {**ggplot2**}](https://ggobi.github.io/ggally/) [@GGally]

The R package {**ggplot2**} is a plotting system based on the grammar of graphics. {**GGally**} extends {**ggplot2**} by adding several functions to reduce the complexity of combining geometric objects with transformed data. Some of these functions include 

- a pairwise plot matrix, 
- a two group pairwise plot matrix, 
- a parallel coordinates plot, 
- a survival plot, 
- and several functions to plot networks.

{**GGally**}: Extension to {**ggplot2**}
:::

***
::::
:::::


## ggmosaic

::: my-resource
::: my-resource-header
{**ggmosaic**}: Mosaic Plots in the {**ggplot2**} Framework
:::

::: my-resource-container

------------------------------------------------------------------------

::: {#pak-ggmosaic}

------------------------------------------------------------------------

{**ggmosaic**}: [Mosaic Plots in the {**ggplot2**}
Framework](https://haleyjeppson.github.io/ggmosaic/) [@ggmosaic]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap03/logoi/logo-ggmosaic-min.png){width="176"}

{**ggmosaic**} is designed to create visualizations of categorical data
and is capable of producing bar charts, stacked bar charts, mosaic
plots, and double decker plots and therefore offers a wide range of
potential plots.
:::

Furthermore, {**ggmosaic**} allows various features to be customized:

-   the order of the variables,
-   the formula setup of the plot,
-   faceting,
-   the type of partition, and
-   the space between the categories.

{**ggmosaic**}: A Package for Mosaic Plots in the {**ggplot2**} Framework
:::


:::
:::

## ggokabeito

:::::{.my-resource}
:::{.my-resource-header}
ggokabeito: 'Okabe-Ito' Scales for {**ggplot2**} and {**ggraph**} 
:::
::::{.my-resource-container}

***

::: {#pak-ggokabeito}

***

{**ggokabeito**}: ['Okabe-Ito' Scales for {**ggplot2**} and {**ggraph**}](https://malcolmbarrett.github.io/ggokabeito/index.html) [@ggokabeito]

(*There is no hexagon logo for {**ggokabeito**} available*)

Discrete scales for the colorblind-friendly `Okabe-Ito` palette, including 'color', 'fill', and 'edge_colour'. {**ggokabeito**} provides {**ggplot2**} and {**ggraph**} scales to easily use the discrete, colorblind-friendly ‘Okabe-Ito’ palette in your data visualizations.

Currently, {**ggokabeito**} provides the following scales:

- `scale_color_okabe_ito(`)/`scale_colour_okabe_ito()`
- `scale_fill_okabe_ito()`
- `scale_edge_color_okabe_ito()`/`scale_edge_colour_okabe_ito()`


{**ggokabeito**}: `Okabe-Ito` Scales for {**ggplot2**} and {**ggraph**}
:::

***
::::
:::::

## ggplot2

:::::{.my-resource}
:::{.my-resource-header}
ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics
:::
::::{.my-resource-container}


***

::: {#pak-ggplot2}

***

{**ggplot2**}: [Create Elegant Data Visualisations Using the Grammar of Graphics](https://ggplot2.tidyverse.org/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-ggplot2-min.png){width="176"}

{**ggplot2**} is a system for declaratively creating graphics, based on [The Grammar of Graphics](https://link.springer.com/book/10.1007/0-387-28695-0). You provide the data, tell {**ggplot2**} how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. [@ggplot2]

:::


Package for Creating Data Visualisations
:::

***


::::
:::::

## gplots

:::::{.my-resource}
:::{.my-resource-header}
gplots: Various R Programming Tools for Plotting Data 
:::
::::{.my-resource-container}

***

::: {#pak-gplots}

***

{**gplots**}: [Various R Programming Tools for Plotting Data](https://github.com/talgalili/gplots) [@gplots]


(*There is no hexagon logo for {**gplots**} available*)

Various R programming tools for plotting data, including:

- calculating and plotting locally smoothed summary function as
('bandplot', 'wapply'),
- enhanced versions of standard plots ('barplot2', 'boxplot2', 
'heatmap.2', 'smartlegend'),
- manipulating colors ('col2hex', 'colorpanel', 'redgreen',
'greenred', 'bluered', 'redblue', 'rich.colors'),
- calculating and plotting two-dimensional data summaries 
('ci2d', 'hist2d'),
- enhanced regression diagnostic plots ('lmplot2', 'residplot'),
- formula-enabled interface to 'stats::lowess' function ('lowess'),
- displaying textual data in plots ('textplot', 'sinkplot'),
- plotting a matrix where each cell contains a dot whose size 
reflects the relative magnitude of the elements ('balloonplot'),
- plotting "Venn" diagrams ('venn'),
- displaying Open-Office style plots ('ooplot'),
- plotting multiple data on same region, with separate axes ('overplot'),
- plotting means and confidence intervals ('plotCI', 'plotmeans'),
- spacing points in an x-y plot so they don't overlap ('space').

{**gplots**}: Various R Programming Tools for Plotting Data
:::

<Other text not included in the package reference>

***
::::
:::::



## gridExtra

::: my-resource
::: my-resource-header
gridExtra: Miscellaneous Functions for "Grid" Graphics
:::

::: my-resource-container

------------------------------------------------------------------------

::: {#pak-gridExtra}

------------------------------------------------------------------------

{**gridExtra**}: [Miscellaneous Functions for "Grid"
Graphics](https://cran.r-project.org/package=gridExtra)

(*There is no hexagon logo for {**gridExtra**} available*)

Provides a number of user-level functions to work with "grid" graphics,
notably to arrange multiple grid-based plots on a page, and draw tables.

The {**grid**) package (= part of the R system library) provides
low-level functions to create graphical objects (`grobs`), and position
them on a page in specific viewports. The {**gtable**} package
introduced a higher-level layout scheme, arguably more amenable to
user-level interaction. With the `gridExtra::arrangeGrob()` /
`gridExtra::grid.arrange()` pair of functions, {**gridExtra**} builds
upon {**gtable**} to arrange multiple `grobs` on a page.

{**gridExtra**}: A Package for Miscellaneous Functions for "Grid"
Graphics
:::

:::
:::



## ggrepel

:::::{.my-resource}
:::{.my-resource-header}
ggrepel: Automatically Position Non-Overlapping Text Labels with 'ggplot2' 
:::
::::{.my-resource-container}

***

::: {#pak-ggrepel}

***

{**ggrepel**}: [Automatically Position Non-Overlapping Text Labels with 'ggplot2'](https://ggrepel.slowkow.com/) [@ggrepel]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap03/logoi/logo-ggrepel-min.png){width="176"}


Provides text and label geoms for 'ggplot2' that help to avoid overlapping text labels. Labels repel away from each other and away from the data points.

:::

{**ggrepel**} provides two geoms for {**ggplot2**} to repel overlapping text labels:

- `ggrepel::geom_text_repel()`
- `ggrepel::geom_label_repel()`

{**ggrepel**}: A Package for Automatically Position Non-Overlapping Text Labels with 'ggplot2'
:::


***
::::
:::::


## ggtext

::: my-resource
::: my-resource-header
ggtext: Improved Text Rendering Support for 'ggplot2'
:::

::: my-resource-container

------------------------------------------------------------------------

::: {#pak-ggtext}

------------------------------------------------------------------------

{**ggtext**}: [Improved Text Rendering Support for
'ggplot2'](https://wilkelab.org/ggtext/)

(*There is no hexagon logo for {**ggtext**} available*)

The ggtext package provides simple Markdown and HTML rendering for
{**ggplot2.**} Under the hood, the package uses the {**gridtext**}
package for the actual rendering, and consequently it is limited to the
[feature set provided by gridtext](https://wilkelab.org/gridtext/).

Support is provided for Markdown both in theme elements (plot titles,
subtitles, captions, axis labels, legends, etc.) and in geoms (similar
to `ggplot2::geom_text()`). In both cases, there are two alternatives,
one for creating simple text labels and one for creating text boxes with
word wrapping.

{**ggtext**}: A Package for Improved Text Rendering Support for
'ggplot2'
:::

:::
:::







## glue

:::::{.my-resource}
:::{.my-resource-header}
glue: Interpreted String Literals
:::
::::{.my-resource-container}

***

::: {#pak-glue}

***

{**glue**}: [Interpreted String Literals](https://glue.tidyverse.org/)

An implementation of interpreted string literals, inspired by Python's Literal String Interpolation

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap02/logoi/logo-glue-min.png){width="176"}

Glue offers interpreted string literals that are small, fast, and dependency-free. Glue does this by embedding R expressions in curly braces which are then evaluated and inserted into the argument string.

:::

{**glue**}: A Package for Interpreting Literal Strings
:::



***
::::
:::::


## gssr

:::::{.my-resource}
:::{.my-resource-header}
gssr: US General Social Survey (GSS) Data for R
:::
::::{.my-resource-container}

***

::: {#pak-gssr}

***

{**gssr**}: [US General Social Survey (GSS) Data for R](https://kjhealy.github.io/gssr/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-gssr-min.png){width="376"}

[GSSR Package](https://kjhealy.github.io/gssr/): The General Social
Survey Cumulative Data (1972-2022) and Panel Data files packaged for
easy use in R. {**gssr**} is a data package, developed and maintained by
[Kieran Healy](https://kieranhealy.org/), the author of [Data
Visualization](https://kieranhealy.org/publications/dataviz/). The
package bundles several datasets into a convenient format. Because of
its large size {**gssr**} is not hosted on CRAN but as a [GitHub
repository](https://github.com/kjhealy/gssr/).

:::

Instead of browsing and examining the complex dataset with the [GSS Data
Explorer](https://gssdataexplorer.norc.org/) or [download datasets
directly](https://gss.norc.org/Get-The-Data) from the The National
Opinion Research Center ([NORC](http://norc.org/)) you can now just work
inside R. The current package 0.4 (see: [gssr
Update](https://kieranhealy.org/blog/archives/2023/12/02/gssr-update/))
provides the GSS Cumulative Data File (1972-2022), three GSS Three Wave
Panel Data Files (for panels beginning in 2006, 2008, and 2010,
respectively), and the 2020 panel file.

Version 0.40 also integrates survey code book information about
variables directly into R’s help system, allowing them to be accessed
via the help browser or from the console with ?, as if they were
functions or other documented objects.

Package for Getting the US General Social Survey (GSS) Data More Easily

:::
***


::::
:::::

## gt

:::::{.my-resource}
:::{.my-resource-header}
gt: Easily Create Presentation-Ready Display Tables
:::
::::{.my-resource-container}

***

::: {#pak-gt}

***

{**gt**}: [Easily Create Presentation-Ready Display Tables](https://gt.rstudio.com)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap02/logoi/logo-gt-min.png){width="176"}

With the {**gt**} package, anyone can make wonderful-looking tables using the R programming language. The gt philosophy: we can construct a wide variety of useful tables with a cohesive set of table parts. These include the table header, the stub, the column labels and spanner column labels, the table body, and the table footer.



:::

{**gt**}: A Package for Presentation-Ready Display Tables
:::

***
::::
:::::


## gtsummary

:::::{.my-resource}
:::{.my-resource-header}

gtsummary: Presentation-Ready Data Summary and Analytic Result Tables

:::
::::{.my-resource-container}

***

::: {#pak-gtsummary}

***

{**gtsummary**}: [Presentation-Ready Data Summary and Analytic Result Tables](https://www.danieldsjoberg.com/gtsummary/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-gtsummary-min.png){width="176"}

Creates presentation-ready tables summarizing data sets, regression models, and more. The code to create the tables is concise and highly customizable. Data frames can be summarized with any function, e.g. mean(), median(), even user-written functions. Regression models are summarized and include the reference rows for categorical variables. Common regression models, such as logistic regression and Cox proportional hazards regression, are automatically identified and the tables are pre-filled with appropriate column headers.



:::

- Summarize data frames or tibbles easily in R. Perfect for creating a `r glossary("Table 1")`. 
- Summarize regression models in R and include reference rows for categorical variables. 
- Customize {**gtsummary**} tables using a growing list of formatting/styling functions. 
- Report statistics inline from summary tables and regression summary tables in R markdown. Make your reports completely reproducible!

By leveraging {**broom**}, {**gt**}, and {**labelled**} packages, {**gtsummary**} creates beautifully formatted, ready-to-share summary and result tables in a single line of R code!

Package for Presentation-Ready Data Summary and Analytic Result Tables

:::



***



::::
:::::



## haven

:::::{.my-resource}
:::{.my-resource-header}
haven: Import and Export 'SPSS', 'Stata' and 'SAS' Files
:::
::::{.my-resource-container}

***

::: {#pak-haven}

***

{**haven**}: [Import and Export 'SPSS', 'Stata' and 'SAS'
Files](https://haven.tidyverse.org/index.html)

{**haven**} enables R to read and write various data formats used by
other statistical packages. Currently it supports
[SAS](https://www.sas.com/en_us/home.html),
[SPSS](https://www.ibm.com/spss) and [STATA](https://www.stata.com/).
{**haven**} output object has four important features:


::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-haven-min.png){width="176"}

(1) It creates `tibbles::tibble()` which a better print method for very
    long and very wide files.
(2) Dates and times are converted to R date/time classes.
(3) Character vectors are not converted to factors.
(4) Value labels are translated into a new `haven::labelled()` class,
    which preserves the original semantics and can easily be coerced to
    factors with `haven::as_factor()`. Special missing values are
    preserved. See details in the vignette [Conversion
    semantics](https://haven.tidyverse.org/articles/semantics.html).
:::

haven: A Package for Import and Export of 'SPSS', 'Stata' and 'SAS'
Files
:::

I am here interested especially in the fourth feature.

::::
:::::

## Hmisc

:::::{.my-resource}
:::{.my-resource-header}
Hmisc: Harrell Miscellaneous
:::
::::{.my-resource-container}
***

::: {#pak-Hmisc}

***

{**Hmisc**}: [Harrell Miscellaneous](https://hbiostat.org/r/hmisc/)

There is no hexagon sticker available for {**Hmisc**}.

The {**Hmisc**} has it names from Frank Harrell Jr. It contains many functions useful for 

- data analysis, 
- high-level graphics, 
- utility operations, 
- computing sample size and power, 
- simulation, 
- importing and annotating datasets,
- imputing missing values, 
- advanced table making, 
- variable clustering,
- character string manipulation, 
- conversion of R objects to {{< latex >}} and HTML code,
- recoding variables, 
- caching, 
- simplified parallel computing, 
- encrypting and decrypting data using a safe workflow, 
- general moving window statistical estimation, 
- assistance in interpreting principal component analysis [@Hmisc]


This is big variety of functions. In contrast to other packages that are specific directed to solve one problem {**Hmisc**} seems to be an all-in-one-solution.


{**Hmisc**}: A package for miscellaneous functions for data analysis and related tasks

:::

To learn more about I should visit Frank E. Harrell's Jr [Hmisc start page](https://hbiostat.org/r/hmisc/). Especially his [book on R Workflow for Reproducible Data Analysis and Reporting](https://hbiostat.org/rflow/) seems to me very interesting!

***

::::
:::::

## httr2

::: my-resource
::: my-resource-header
httr2: Perform HTTP Requests and Process the Responses
:::

::: my-resource-container

------------------------------------------------------------------------

::: {#pak-httr2}

------------------------------------------------------------------------

{**httr2**}: [Perform HTTP Requests and Process the
Responses](https://httr2.r-lib.org/)

Tools for creating and modifying HTTP requests, then performing them and
processing the results.

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap03/logoi/logo-httr2-min.png){width="176"}

{**httr2**} (pronounced hitter2) is a ground-up rewrite of {**httr**}
that provides a pipeable `r glossary("APIx", "API")` with an explicit request
object that solves more problems felt by packages that wrap APIs (e.g.
built-in rate-limiting, retries, OAuth, secure secrets, and more). ---
{**httr2**} is designed to map closely to the underlying
`r glossary("HTTP")` `r glossary("protocol")`. For more details, read
[An overview of
HTTP](https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview) from
`r glossary("MDN")`.
:::

{**httr2**}: A Package to Perform HTTP Requests and Process the
Responses
:::

:::
:::

## janitor

::: my-resource
::: my-resource-header
janitor: Simple Tools for Examining and Cleaning Dirty Data
:::

::: my-resource-container

------------------------------------------------------------------------

::: {#pak-janitor}

------------------------------------------------------------------------

{**janitor**}: [Simple Tools for Examining and Cleaning Dirty
Data](https://sfirke.github.io/janitor/) [@janitor]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap03/logoi/logo-janitor-min.png){width="176"}

{**janitor**} has simple functions for examining and cleaning dirty
data. It was built with beginning and intermediate R users in mind and
is optimized for user-friendliness. Advanced R users can perform many of
these tasks already, but with janitor they can do it faster and save
their thinking for the fun stuff.
:::

**The main janitor functions:**

-   perfectly format data.frame column names;
-   create and format frequency tables of one, two, or three variables -
    think an improved `base::table()`; and
-   provide other tools for cleaning and examining data.frames.

The tabulate-and-report functions approximate popular features of SPSS
and Microsoft Excel.

{**janitor**} is a {**tidyverse**}-oriented package. Specifically, it
plays nicely with the `%>%` pipe and is optimized for cleaning data
brought in with the {**readr**} and {**readxl**} packages.

{**janitor**}: A Package for Simple Tools for Examining and Cleaning
Dirty Data
:::

------------------------------------------------------------------------

I am using {**janitor**} mostly in two ways:

1.  as better `base::table()` function, using `janitor::tabyl()`
    -   `base::table()` doesn't accept data.frames and is therefore not
        compatible with the pipe
    -   `base::table()` doesn't output data.frames
    -   `base::table()` results are hard to format (the most annoying
        "feature" for me)

-   to add information and formatting to the table with the
    `janitor::adorn_*` functions
    -   `janitor::adorn_totals()`
    -   `janitor::adorn_percentages()`
    -   `janitor::adorn_pct_formatting()`
    -   `janitor::adorn_rounding()`
    -   `janitor::adorn_ns()` (adding Ns = number of counts)
    -   `janitor::adorn_title()`

You could also use {**tidyverse**} commands (for instance for a two
table `dplyr::count()` followed by `tidyr::pivot_wider()`) but the many
`adorn_*`-functions make it easy to enhance the results. BTW: The prefix
`adorn` comes from 'adornment' (ornament, decoration).
:::
:::



## kableExtra

:::::{.my-resource}
:::{.my-resource-header}
kableExtra: Construct Complex Table with `knitr::kable()` and Pipe Syntax
:::
::::{.my-resource-container}

***

::: {#pak-kableExtra}

***

{**kableExtra**}: [Construct Complex Table with 'kable' and Pipe Syntax](URL to package)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap02/logoi/logo-kableExtra-min.png){width="176"}

Build complex HTML or {{< latex >}} tables using `kable()` from {**knitr**} and the piping syntax from {**magrittr**} Function `kable()` is a light weight table generator coming from {**knitr**}. This package simplifies the way to manipulate the HTML or {{< latex >}} codes generated by `kable()` and allows users to construct complex tables and customize styles using a readable syntax. 

:::

{**pkg-name**}: A Package to Construct Complex Table with 'kable' and Pipe Syntax
:::

***
::::
:::::


## knitr

:::::{.my-resource}
:::{.my-resource-header}
knitr: A General-Purpose Package for Dynamic Report Generation in R
:::
::::{.my-resource-container}

***

::: {#pak-knitr}

***

{**knitr**}: [A General-Purpose Package for Dynamic Report Generation in R](https://yihui.org/knitr/)

(*There is no hexagon logo available*)

Provides a general-purpose tool for dynamic report generation in R using Literate Programming techniques.

{**knitr**}: A Package for Dynamic Report Generation in R
:::


***
::::
:::::


## labelled

:::::{.my-resource}
:::{.my-resource-header}
labelled: Manipulating Labelled Data
:::
::::{.my-resource-container}

***

::: {#pak-labelled}

***
{**labelled**}: [Manipulating Labelled
Data](https://larmarange.github.io/labelled/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-labelled-min.png){width="176"}

Work with labelled data imported from [IBM
SPSS](https://www.ibm.com/spss) or [STATA](https://www.stata.com/) with
{**haven**} or {**foreign**}. This package provides useful functions to
deal with "haven_labelled" and "haven_labelled_spss" classes introduced
by {**haven**} package. (With the free [gnu
PSPP](https://www.gnu.org/software/pspp/) exists also a SPSS like open
source version.) See details in the vignette [Introduction to
labelled](https://larmarange.github.io/labelled/articles/intro_labelled.html)
and the [GitHub website for
labelled](https://larmarange.github.io/labelled/). There are other
vignettes as well and a [cheat sheet as PDF for
download](https://github.com/larmarange/labelled/raw/main/cheatsheet/labelled_cheatsheet.pdf).

:::

labelled: A Package for Manipulating Labelled Data
:::


::::
:::::

## lmtest

:::::{.my-resource}
:::{.my-resource-header}
lmtest: Testing Linear Regression Models 
:::
::::{.my-resource-container}

***

::: {#pak-lmtest}

***

{**lmtest**}: [Testing Linear Regression Models](https://cran.r-project.org/package=lmtest) [@lmtest]

A collection of tests, data sets, and examples for diagnostic checking in linear regression models. Furthermore, some generic tools for inference in parametric models are provided.

Vignette [Diagnostic Checking in Regression Relationships](https://cran.r-project.org/web/packages/lmtest/vignettes/lmtest-intro.pdf)

{**lmtest**}: Testing Linear Regression Models
:::

***
::::
:::::


## lsr

:::::{.my-resource}
:::{.my-resource-header}
lsr: Companion to "Learning Statistics with R" 
:::
::::{.my-resource-container}

***

::: {#pak-lsr}

***

{**lsr**}: [Companion to "Learning Statistics with R"](https://learningstatisticswithr.com/) [@lsr]

(*There is no hexagon logo for {**lsr**} available*)

A collection of tools intended to make introductory statistics easier to teach, including wrappers for common hypothesis tests and basic data manipulation. It accompanies Navarro, D. J. (2015). Learning Statistics with R: A Tutorial for Psychology Students and Other Beginners, Version 0.6. 


{**lsr**}: Companion to "Learning Statistics with R"
:::

***
::::
:::::






## modeest

:::::{.my-resource}
:::{.my-resource-header}
modeest: Mode Estimation
:::
::::{.my-resource-container}

***

::: {#pak-modeest}

***

{**modeest**}: [Mode Estimation](https://github.com/paulponcet/modeest)

There is no hexagon sticker available for {**modeest**}.


The {**modeest**} package provides estimators of the mode of univariate unimodal (and sometimes multimodal) data and values of the modes of usual probability distributions.

{**modeest**} is a package specialized for mode estimation. It implements many different mode estimation reported in scientific articles. There is a long [list of references](https://www.rdocumentation.org/packages/modeest/versions/2.4.0/topics/modeest) on different methods of mode estimations.

{**modeest**}: A Package for Mode Estimation
:::


::::
:::::


## moments

:::::{.my-resource}
:::{.my-resource-header}
moments: Moments, Cumulants, Skewness, Kurtosis and Related Tests 
:::
::::{.my-resource-container}

***

::: {#pak-moments}

***

{**moments**}: [Moments, Cumulants, Skewness, Kurtosis and Related Tests](https://cran.r-project.org/package=moments) [@moments]



(*There is no hexagon logo for {**moments**} available*)

Functions to calculate: moments, Pearson's kurtosis, Geary's kurtosis and skewness; tests related to them.


{**moments**}: Moments, Cumulants, Skewness, Kurtosis and Related Tests
:::

***
::::
:::::

## misty

:::::{.my-resource}
:::{.my-resource-header}
misty: Miscellaneous Functions 'T. Yanagida'  
:::
::::{.my-resource-container}

***

::: {#pak-misty}

***

{**misty**}: [Miscellaneous Functions 'T. Yanagida' ](https://cran.r-project.org/package=misty) [@misty]

Miscellaneous functions for 

- (1) data management (e.g., grand-mean and group-mean centering, coding variables and reverse coding items, scale and cluster scores, reading and writing Excel and SPSS files), 
- (2) descriptive statistics (e.g., frequency table, cross tabulation, effect size measures), 
- (3) missing data (e.g., descriptive statistics for missing data, missing data pattern, Little's test of Missing Completely at Random, and auxiliary variable analysis), 
- (4) multilevel data (e.g., multilevel descriptive statistics, within-group and between-group correlation matrix, multilevel confirmatory factor analysis, level-specific fit indices, cross-level measurement equivalence evaluation,  multilevel composite reliability, and multilevel R-squared measures), 
- (5) item analysis (e.g., confirmatory factor analysis, coefficient alpha and omega, between-group and longitudinal measurement equivalence evaluation), and 
- (6) statistical analysis (e.g., confidence intervals, collinearity and residual diagnostics, dominance analysis, between- and within-subject analysis of variance, latent class analysis, t-test, z-test, sample size determination).

{**misty**}: Miscellaneous Functions 'T. Yanagida' 
:::


***
::::
:::::



## naniar

:::::{.my-resource}
:::{.my-resource-header}
naniar: Data Structures, Summaries, and Visualisations for Missing Data 
:::
::::{.my-resource-container}

***

::: {#pak-naniar}

***

{**naniar**}: [https://github.com/njtierney/naniar](https://naniar.njtierney.com/) [@naniar]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap05/logoi/logo-naniar-min.png){width="176"}


{**naniar**} provides principled, tidy ways to summarise, visualise, and manipulate missing data with minimal deviations from the workflows in ggplot2 and tidy data.

:::

Missing values are ubiquitous in data and need to be explored and handled in the initial stages of analysis. {**naniar**} provides data structures and functions that facilitate the plotting of missing values and examination of imputations. This allows missing data dependencies to be  explored with minimal deviation from the common work patterns of 'ggplot2' and tidy data. The work is fully discussed in Tierney & Cook [-@tierney2023].

{**naniar**}: https://github.com/njtierney/naniar
:::


***
::::
:::::

## nhanesA

:::::{.my-resource}
:::{.my-resource-header}
nhanesA: NHANES Data Retrieval 
:::
::::{.my-resource-container}

***

::: {#pak-nhanesA}

***

{**nhanesA**}: [NHANES Data Retrieval](https://github.com/cjendres1/nhanes/) [@nhanesA]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap06/logoi/logo-nhanesA-min.png){width="176"}


Utility to retrieve data from the National Health and Nutrition Examination Survey (NHANES) website <https://www.cdc.gov/nchs/nhanes/index.htm>.

:::

{**nhanesA**} is an R package for browsing and retrieving data from the National Health And Nutrition Examination Survey (NHANES). This package is designed to be useful for research and instructional purposes.

The functions in the {**nhanesA**} package allow for fully customizable selection and import of data directly from the NHANES website thus it is essential to have an active network connection.

There are other similar packages also available, but the are more restricted as newer data than 2014 cant be downloaded: 

- {**NHANES**}: For the years 2009-2012
- {**RNHANES**}: For the years 1999-2014


{**nhanesA**}: NHANES Data Retrieval
:::

See for my other reflection of packages for downloading NHANES data in @not-chap01-nhanesA-pkg and @sec-chap03-rnhanes.

***
::::
:::::

## nortest

:::::{.my-resource}
:::{.my-resource-header}
nortest: Tests for Normality 
:::
::::{.my-resource-container}

***

::: {#pak-nortest}

***

{**nortest**}: [Tests for Normality](https://cran.r-project.org/package=nortest) [@nortest]


(*There is no hexagon logo for {**nortest**} available*)

Five omnibus tests for testing the composite hypothesis of normality.

{**nortest**}: Tests for Normality
:::

***
::::
:::::

## onewaytests

:::::{.my-resource}
:::{.my-resource-header}
onewaytests: One-Way Tests in Independent Groups Designs 
:::
::::{.my-resource-container}

***

::: {#pak-onewaytests}

***

{**onewaytests**}: [One-Way Tests in Independent Groups Designs](https://cran.r-project.org/package=onewaytests) [@onewaytests]

Performs one-way tests in independent groups designs including homoscedastic and heteroscedastic tests. These are 

- one-way analysis of variance (ANOVA), 
- Welch's heteroscedastic F test, 
- Welch's heteroscedastic F test with trimmed means and Winsorized variances,
- Brown-Forsythe test, 
- Alexander-Govern test, 
- James second order test, 
- Kruskal-Wallis test, 
- Scott-Smith test, 
- Box F test, 
- Johansen F test, 
- Generalized tests equivalent to Parametric Bootstrap and Fiducial tests, 
- Alvandi's F test, 
- Alvandi's generalized p-value, 
- approximate F test, 
- B square test, 
- Cochran test, 
- Weerahandi's generalized F test, 
- modified Brown-Forsythe test, 
- adjusted Welch's heteroscedastic F test, 
- Welch-Aspin test, 
- Permutation F test.

The package performs pairwise comparisons and graphical approaches. 

Also, the package includes 

- Student's t test, 
- Welch's t test and 
- Mann-Whitney U test for two samples. 

Moreover, it assesses variance homogeneity and normality of data in each group via tests and plots (Dag et al., 2018, <https://journal.r-project.org/archive/2018/RJ-2018-022/RJ-2018-022.pdf>).

{**onewaytests**}: One-Way Tests in Independent Groups Designs
:::

***
::::
:::::




## openintro

:::::{.my-resource}
:::{.my-resource-header}
openintro: Data Sets and Supplemental Functions from 'OpenIntro' Textbooks and Labs
:::
::::{.my-resource-container}

***

::: {#pak-openintro}

***

{**openintro**}: [Data Sets and Supplemental Functions from 'OpenIntro' Textbooks and Labs](https://openintrostat.github.io/openintro/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap02/logoi/logo-openintro-min.png){width="176"}

The package contains data sets used in our open-source textbooks along with custom plotting functions for reproducing book figures. The package also contains the datasets used in {**OpenIntro**} labs. Note that many functions and examples include color transparency; some plotting elements may not show up properly (or at all) when run in some versions of Windows operating system.

:::

{**openintro**}: A Package to Supplement 'OpenIntro' Textbooks and Labs
:::


***
::::
:::::


## paletteer

:::::{.my-resource}
:::{.my-resource-header}
paletteer: Comprehensive Collection of Color Palettes 
:::
::::{.my-resource-container}

***

::: {#pak-paletteer}

***

{**paletteer**}: [Comprehensive Collection of Color Palettes](https://emilhvitfeldt.github.io/paletteer/index.html) [@paletteer]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap03/logoi/logo-paletteer-min.png){width="176"}


The choices of color palettes in R can be quite overwhelming with palettes spread over many packages with many different API's. This packages aims to collect all color palettes across the R ecosystem under the same package with a streamlined API.

:::

The palettes are divided into 2 groups; discrete and continuous. For discrete palette you have the choice between the fixed width palettes and dynamic palettes.

1. discrete
    - fixed width palettes: These are the most common discrete palettes. They have a set amount of colors which doesn’t change when the number of colors requested vary.
    - dynamic palettes: The colors of dynamic palettes depend on the number of colors you need.
2. continuous: These palettes provides as many colors as you need for a smooth transition of color.

This package includes 2759 palettes from 75 different packages and information about these can be found in the following data.frames: `palettes_c_names`, `palettes_d_names` and `palettes_dynamic_names`. Additionally this [github repo](https://github.com/EmilHvitfeldt/r-color-palettes/blob/main/README.md) showcases all the palettes included in the package and more.

{**paletteer**}: A Package for Comprehensive Collection of Color Palettes
:::


***
::::
:::::




## patchwork

::: my-resource
::: my-resource-header
patchwork: The Composer of Plots
:::

::: my-resource-container

------------------------------------------------------------------------

::: {#pak-patchwork}

------------------------------------------------------------------------

{**patchwork**}: [The Composer of
Plots](https://patchwork.data-imaginist.com/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap03/logoi/logo-patchwork-min.png){width="176"}

The goal of {**patchwork**} is to make it ridiculously simple to combine
separate `ggplots` into the same graphic. As such it tries to solve the
same problem as `gridExtra::grid.arrange()` and `cowplot::plot_grid` but
using an `r glossary("APIx", "API")` that incites exploration and iteration, and
scales to arbitrarily complex layouts.
:::

The {**ggplot2**} package provides a strong API for sequentially
building up a plot, but does not concern itself with composition of
multiple plots. {**patchwork**} is a package that expands the API to
allow for arbitrarily complex composition of plots by, among others,
providing mathematical operators for combining multiple plots. Other
packages that try to address this need (but with a different approach)
are {**gridExtra**} and {**cowplot**} (see @pak-gridExtra and
@pak-cowplot).

Before plots can be laid out, they have to be assembled. Arguably one of
patchwork’s biggest selling points is that it expands on the use of `+`
in ggplot2 to allow plots to be added together and composed, creating a
natural extension of the {**ggplot2**} API.

While quite complex compositions can be achieved using `+`, `|`, and
`/`, it may be necessary to take even more control over the layout. All
of this can be controlled using the `patchwork::plot_layout()` function
along with a couple of special placeholder objects.

{**patchwork**}: A Package for Composing Plots
:::

:::
:::

## nhanesA

:::::{.my-resource}
:::{.my-resource-header}
nhanesA: NHANES Data Retrieval 
:::
::::{.my-resource-container}

***

::: {#pak-nhanesA}

***

{**nhanesA**}: [NHANES Data Retrieval](https://github.com/cjendres1/nhanes/) [@nhanesA]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap06/logoi/logo-nhanesA-min.png){width="176"}


Utility to retrieve data from the National Health and Nutrition Examination Survey (NHANES) website <https://www.cdc.gov/nchs/nhanes/index.htm>.

:::

{**nhanesA**} is an R package for browsing and retrieving data from the National Health And Nutrition Examination Survey (NHANES). This package is designed to be useful for research and instructional purposes.

The functions in the {**nhanesA**} package allow for fully customizable selection and import of data directly from the NHANES website thus it is essential to have an active network connection.

There are other similar packages also available, but the are more restricted as newer data than 2014 cant be downloaded: 

- {**NHANES**}: For the years 2009-2012
- {**RNHANES**}: For the years 1999-2014


{**nhanesA**}: NHANES Data Retrieval
:::

See for my other reflection of packages for downloading NHANES data in @not-chap01-nhanesA-pkg and @sec-chap03-rnhanes.

***
::::
:::::

## nortest

:::::{.my-resource}
:::{.my-resource-header}
nortest: Tests for Normality 
:::
::::{.my-resource-container}

***

::: {#pak-nortest}

***

{**nortest**}: [Tests for Normality](https://cran.r-project.org/package=nortest) [@nortest]


(*There is no hexagon logo for {**nortest**} available*)

Five omnibus tests for testing the composite hypothesis of normality.

{**nortest**}: Tests for Normality
:::

***
::::
:::::

## ppcor

:::::{.my-resource}
:::{.my-resource-header}
ppcor: Partial and Semi-Partial (Part) Correlation 
:::
::::{.my-resource-container}

***

::: {#pak-ppcor}

***

{**ppcor**}: [Partial and Semi-Partial (Part) Correlation](https://cran.r-project.org/package=ppcor) [@ppcor]

Calculates partial and semi-partial (part) correlations along with p value. [@kim2015]


{**ppcor**}: Partial and Semi-Partial (Part) Correlation
:::



***
::::
:::::


## psych

:::::{.my-resource}
:::{.my-resource-header}
psych: Procedures for Psychological, Psychometric, and Personality Research 
:::
::::{.my-resource-container}

***

::: {#pak-psych}

***

{**psych**}: [Procedures for Psychological, Psychometric, and Personality Research](https://personality-project.org/r/psych/) [@psych]


(*There is no hexagon logo for {**psych**} available*)

A general purpose toolbox developed originally for personality, psychometric theory and experimental psychology.  

- Functions are primarily for multivariate analysis and scale construction using factor analysis, principal component analysis, cluster analysis and reliability analysis, although others provide basic descriptive statistics. 
- Item Response Theory is done using factor analysis of tetrachoric and polychoric correlations. 
- Functions for analyzing data at multiple levels include within and between group statistics, including correlations and factor analysis.  
- Validation and cross validation of scales developed using basic machine learning algorithms are provided, as are functions for simulating and testing particular item and test structures. 
- Several functions serve as a useful front end for structural equation modeling. 
- Graphical displays of path diagrams, including mediation models, factor analysis and structural equation models are created using basic graphics. 
- Some of the functions are written to support a book on psychometric theory as well as publications in personality research.

{**psych**}: Procedures for Psychological, Psychometric, and Personality Research
:::



***
::::
:::::


## purrr

:::::{.my-resource}
:::{.my-resource-header}
purrr: Functional Programming Tools
:::
::::{.my-resource-container}

***

::: {#pak-purrr}

***

{**purrr**}: [Functional Programming Tools](https://purrr.tidyverse.org/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap02/logoi/logo-purrr-min.png){width="176"}

{**purrr**} enhances R’s functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. If you’ve never heard of FP before, the best place to start is the family of `purrr::map()` functions which allow you to replace many for loops with code that is both more succinct and easier to read. The best place to learn about the `purrr::map()` functions is the [iteration chapter](https://r4ds.had.co.nz/iteration.html) in R for data science.

:::


{**purrr**}: A Package for Functional Programming Tools
:::

***
::::
:::::

## rcompanion

:::::{.my-resource}
:::{.my-resource-header}
rcompanion: Functions to Support Extension Education Program Evaluation 
:::
::::{.my-resource-container}

***

::: {#pak-rcompanion}

***

{**rcompanion**}: [Functions to Support Extension Education Program Evaluation](https://rcompanion.org/handbook/) [@rcompanion-2]

Functions and datasets to support Summary and Analysis of Extension Program Evaluation in Companion for the Handbook of Biological Statistics. Vignettes are available at <https://rcompanion.org>. (See also the [PDF book](https://rcompanion.org/documents/RHandbookProgramEvaluation.pdf) [@mangiafico2023].)

{**rcompanion**}: Functions to Support Extension Education Program Evaluation
:::

This book explains many different tests as you can see from the table of content:

**TABLE OF CONTENT**

 

**Introduction**

- Purpose of this Book
- Author of this Book
- Using R
- [Statistics Textbooks and Other Resources](https://rcompanion.org/handbook/A_04.html)
 

**Statistics for Educational Program Evaluation**

- Why Statistics?
- Evaluation Tools and Surveys
 

**Variables, Descriptive Statistics, and Plots**

- Types of Variables
- Descriptive Statistics
- Confidence Intervals
- Basic Plots
 

**Understanding Statistics and Hypothesis Testing**

- Hypothesis Testing and p-values
- Reporting Results of Data and Analyses
- [Choosing a Statistical Test](https://rcompanion.org/handbook/D_03.html)
- Independent and Paired Values
 

**Likert Data**

- Introduction to Likert Data
- Descriptive Statistics for Likert Item Data
- Descriptive Statistics with the likert Package
- Confidence Intervals for Medians
- Converting Numeric Data to Categories
 

**Traditional Nonparametric Tests**

- Introduction to Traditional Nonparametric Tests
- One-sample Wilcoxon Signed-rank Test
- Sign Test for One-sample Data
- Two-sample Mann–Whitney U Test
- Mood’s Median Test for Two-sample Data
- Two-sample Paired Signed-rank Test
- Sign Test for Two-sample Paired Data
- Kruskal–Wallis Test
- Mood’s Median Test
- Friedman Test
- Quade Test
- Scheirer–Ray–Hare Test
- Aligned Ranks Transformation ANOVA
- Nonparametric Regression and Local Regression
- Nonparametric Regression for Time Series
 

**Permutation Tests**

- Introduction to Permutation Tests
- One-way Permutation Test for Ordinal Data
- One-way Permutation Test for Paired Ordinal Data
- Permutation Tests for Medians and Percentiles
 

**Tests for Ordinal Data in Tables**

- Association Tests for Ordinal Tables
- Measures of Association for Ordinal Tables
 

**Concepts for Linear Models**

- Introduction to Linear Models
- Using Random Effects in Models
- What are Estimated Marginal Means?
- Estimated Marginal Means for Multiple Comparisons
- Factorial ANOVA: Main Effects, Interaction Effects, and Interaction Plots
- p-values and R-square Values for Models
- Accuracy and Errors for Models
 

**Ordinal Tests with Cumulative Link Models**

- Introduction to Cumulative Link Models (CLM) for Ordinal Data
- Two-sample Ordinal Test with CLM
- Two-sample Paired Ordinal Test with CLMM
- One-way Ordinal Regression with CLM
- One-way Repeated Ordinal Regression with CLMM
- Two-way Ordinal Regression with CLM
- Two-way Repeated Ordinal Regression with CLMM
 

**Tests for Nominal Data**

- Introduction to Tests for Nominal Variables
- Confidence Intervals for Proportions
- Goodness-of-Fit Tests for Nominal Variables
- Association Tests for Nominal Variables
- Measures of Association for Nominal Variables
- Tests for Paired Nominal Data
- Cochran–Mantel–Haenszel Test for 3-Dimensional Tables
- Cochran’s Q Test for Paired Nominal Data
- Models for Nominal Data
 

**Parametric Tests**

- Introduction to Parametric Tests
- One-sample t-test
- Two-sample t-test
- Paired t-test
- One-way ANOVA
- One-way ANOVA with Blocks
- One-way ANOVA with Random Blocks
- Two-way ANOVA
- Repeated Measures ANOVA
- Correlation and Linear Regression
- Advanced Parametric Methods
- Transforming Data
- Normal Scores Transformation
 

**Analysis of Count Data and Percentage Data**

- Regression for Count Data
- Beta Regression for Percent and Proportion Data
 

**Other Books**

An [R Companion](https://rcompanion.org/rcompanion/) for the [Handbook of Biological Statistics](https://www.biostathandbook.com/)

***
::::
:::::



## readr 

:::::{.my-resource}
:::{.my-resource-header}
readr: Read Rectangular Text Data
:::
::::{.my-resource-container}


***

::: {#pak-readr}

***

{**readr**}: [Read Rectangular Text Data](https://readr.tidyverse.org/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-readr-min.png){width="176"}

The goal of readr is to provide a fast and friendly way to read rectangular data from delimited files, such as comma-separated values (CSV) and tab-separated values (TSV). It is designed to parse many types of data found in the wild, while providing an informative problem report when parsing leads to unexpected results. [@readr] 


:::

{**readr**} supports the following formats:

- read_csv(): comma-separated values (CSV)
- read_tsv(): tab-separated values (TSV)
- read_csv2(): semicolon-separated values with , as the decimal mark
- read_delim(): delimited files (CSV and TSV are important special cases)
- read_fwf(): fixed-width files
- read_table(): whitespace-separated files
- read_log(): web log files

Package for Reading Rectangular Data

:::


***



::::
:::::




## readxl

::: my-resource
::: my-resource-header
readxl: Read Excel Files
:::

::: my-resource-container

------------------------------------------------------------------------

::: {#pak-readxl}

------------------------------------------------------------------------

{**readxl**}: [Read Excel Files](https://readxl.tidyverse.org/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap03/logoi/logo-readxl-min.png){width="176"}

The readxl package makes it easy to get data out of Excel and into R.
Compared to many of the existing packages (e.g. {**gdata**}, {**xlsx**},
{**xlsReadWrite**}) {**readxl**} has no external dependencies, so it’s
easy to install and use on all operating systems. It is designed to work
with *tabular* data. Works on Windows, Mac and Linux without external
dependencies.
:::

{**readxl**} supports both the legacy `.xls` format and the modern
xml-based `.xlsx` format. The embedded
[libxls](https://github.com/libxls/libxls) C library is used to support
`.xls`, which abstracts away many of the complexities of the underlying
binary format. To parse `.xlsx`, we use the
[RapidXML](https://rapidxml.sourceforge.net/) C++ library.

{**readxl**}: A Package to Read Excel Files
:::

:::
:::



## report

:::::{.my-resource}
:::{.my-resource-header}
Report: Automated Reporting of Results and Statistical Models
:::
::::{.my-resource-container}
***

::: {#pak-report}

***

{**report**}: [Automated Reporting of Results and Statistical Models](URL to package)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap02/logoi/logo-report-min.png){width="176"}

The primary goal of {**report**} is to bridge the gap between R’s output and the formatted results contained in your manuscript. It automatically produces reports of models and data frames according to best practices guidelines (e.g., APA’s style), ensuring standardization and quality in results reporting.

:::

{**Report**} A Package for Automated Reporting of Results and Statistical Models
:::

***


::::
:::::

## RNHANES {#sec-chap03-rnhanes}

::: my-resource
::: my-resource-header
RNHANES: Facilitates Analysis of CDC NHANES Data
:::

::: my-resource-container

------------------------------------------------------------------------

::: {#pak-RNHANES}

------------------------------------------------------------------------

{**RNHANES**}: [Facilitates Analysis of CDC NHANES
Data](https://wwww.silentspring.org/RNHANES/index.html)

(*There is no hexagon logo available for RNHANES*)

RNHANES is an R package for accessing and analyzing `r glossary("CDC")`
`r glossary("NHANES")` (National Health and Nutrition Examination
Survey) data that was developed by [Silent Spring
Institute](https://silentspring.org/).

{**RNHANES**}: A Package for Facilitating Analysis of CDC NHANES Data
:::

:::::{.my-watch-out}
:::{.my-watch-out-header}
WATCH OUT! {**RNHANES**} downloads data only until 2013-2014
:::
::::{.my-watch-out-container}

The CRAN version of {**RNHANES**} only works with data before 2015. It is said in the book that for the year 2015-2016 you could use the GitHub developer version. But this didn't work for me.

The problem is the function `RNHANES::validate_year()` that is not up-to-date. It has the valid years included as fixed strings which I see as bad programming. (One could generate these pair of years programmatically, checking with the modulo operator `%%`, subtracting from the current year 4 years, because the data has to be prepared to make it public available.)

```{r}
#| label: rnhanes-validate-year.function

RNHANES:::validate_year
```

I therefore used in @lst-chap03-get-NHANES-2018-data code to download data directly from the website. Currently I learned that there is another --- more updated --- {**nhanesA**} packages that I am going to test in chapter 6, where I need NHANES data again.

::::
:::::



:::
:::

## rstatix

:::::{.my-resource}
:::{.my-resource-header}
rstatix: Pipe-Friendly Framework for Basic Statistical Tests 
:::
::::{.my-resource-container}

***

::: {#pak-rstatix}

***

{**rstatix**}: [Pipe-Friendly Framework for Basic Statistical Tests](https://rpkgs.datanovia.com/rstatix/) [@rstatix]


(*There is no hexagon logo for {**rstatix**} available*)

Provides a simple and intuitive pipe-friendly framework, coherent with the {**tidyverse**} design philosophy, for performing basic statistical tests, including t-test, Wilcoxon test, ANOVA, Kruskal-Wallis and correlation analyses.

The output of each test is automatically transformed into a tidy data frame to facilitate visualization.

Additional functions are available for reshaping, reordering, manipulating and visualizing correlation matrix. Functions are also included to facilitate the analysis of factorial experiments, including purely ‘within-Ss’ designs (repeated measures), purely ‘between-Ss’ designs, and mixed ‘within-and-between-Ss’ designs.

It’s also possible to compute several effect size metrics, including “eta squared” for ANOVA, “Cohen’s d” for t-test and “Cramer’s V” for the association between categorical variables. The package contains helper functions for identifying univariate and multivariate outliers, assessing normality and homogeneity of variances.

{**rstatix**}: Pipe-Friendly Framework for Basic Statistical Tests
:::

***
::::
:::::







## rvest

:::::{.my-resource}
:::{.my-resource-header}
rvest: Easily Harvest (Scrape) Web Pages 
:::
::::{.my-resource-container}

***

::: {#pak-rvest}

***

{**rvest**}: [Easily Harvest (Scrape) Web Pages](https://rvest.tidyverse.org/) [@rvest]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap04/logoi/logo-rvest-min.png){width="176"}


Wrappers around the {**xml2**} and {**httr**} packages to make it easy to download, then manipulate, `r glossary("HTML")` and `r glossary("XML")`. --- {**rvest**} helps you scrape (or harvest) data from web pages. It is designed to work with {**magrittr**} to make it easy to express common web scraping tasks, inspired by libraries like [beautiful soup](https://www.crummy.com/software/BeautifulSoup/) and [RoboBrowser](https://robobrowser.readthedocs.io/en/latest/readme.html).

:::

If you’re scraping multiple pages, hadley Wickham highly recommends using {**rvest**} in concert with [{**polite**}](https://dmi3kno.github.io/polite/). The {**polite**} package ensures that you’re respecting the [robots.txt](https://en.wikipedia.org/wiki/Robots_exclusion_standard) and not hammering the site with too many requests.

{**rvest**}: A Package for Easily Harvest (Scrape) Web Pages
:::

It is important to read the introductory vignette [Web scraping 101](https://rvest.tidyverse.org/articles/rvest.html). It introduces you to the basics of web scraping with {**rvest**}. You’ll first learn the basics of HTML and how to use `r glossary("CSS")` selectors to refer to specific elements, then you’ll learn how to use {**rvest**} functions to get data out of `r glossary("HTML")` and into R.

A very important tool to get the appropriate CSS selector is SelectorGadget. To learn how to install and to use this tool read the [SelectorGadget help page]https://rvest.tidyverse.org/articles/selectorgadget.html) of {**rvest**}.

***



::::
:::::


## scales

:::::{.my-resource}
:::{.my-resource-header}
scales: Scale Functions for Visualization
:::
::::{.my-resource-container}

***

::: {#pak-scales}

***

{**scales**}: [Scale Functions for Visualization](https://scales.r-lib.org/)

Graphical scales map data to aesthetics, and provide methods for automatically determining breaks and labels for axes and legends.

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap02/logoi/logo-scales-min.png){width="176"}

One of the most difficult parts of any graphics package is scaling, converting from data values to perceptual properties. The inverse of scaling, making guides (legends and axes) that can be used to read the graph, is often even harder! The scales packages provides the internal scaling infrastructure used by ggplot2, and gives you tools to override the default breaks, labels, transformations and palettes.

:::

{**scales**}: A Package for Scaling Functions for Visualization
:::


***
::::
:::::

## scico

:::::{.my-resource}
:::{.my-resource-header}
scico: Colour Palettes Based on the Scientific Colour-Maps 
:::
::::{.my-resource-container}

***

::: {#pak-scico}

***

{**scico**}: [Colour Palettes Based on the Scientific Colour-Maps](https://github.com/thomasp85/scico) [@scico]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap03/logoi/logo-scico-min.png){width="176"}


Color choice in information visualization is important in order to avoid being mislead by inherent bias in the used color palette. The {**scico**} package provides access to the perceptually uniform and color-blindness friendly palettes developed by [Fabio Crameri](https://www.fabiocrameri.ch/colourmaps/) and released under the "Scientific Color-Maps" moniker. The package contains 39 different palettes and includes both diverging and sequential types. It uses more or less the same `r glossary("APIx", "API")` as {**viridis**} and provides scales for {**ggplot2**} without requiring {**ggplot2**} to be installed.

:::

**Features of {scico}**

- Perceptually uniform
- Perceptually ordered
- Color-vision-deficiency (`r glossary("CVD")`) friendly
- Readable in black & white prints
- All color map types & classes in all major formats
- Citable & reproducible


{**scico**}: A Package for Colour Palettes Based on the Scientific Colour-Maps
:::

::::
:::::





## semTools

:::::{.my-resource}
:::{.my-resource-header}
semTools:: Useful Tools for Structural Equation Modeling
:::
::::{.my-resource-container}


***

::: {#pak-semTools}

***

{**semTools**}: [Useful Tools for Structural Equation Modeling](https://github.com/simsem/semTools/wiki)

(For {**semTools**} there is currently no logo available.)

Provides tools for structural equation modeling, many of which extend the {**lavaan**} package; for example, to pool results from multiple imputations, probe latent interactions, or test measurement invariance.
***

{**semTools**}: A Package for Useful Tools for Structural Equation Modeling
:::

This is a very specialized package. I believe I will not use it at the moment besides the function `semTools::skew()` and `semTools::kurtosis()`.

***


::::
:::::


## sjlabelled

:::::{.my-resource}
:::{.my-resource-header}
sjlabelled; 
:::
::::{.my-resource-container}

***

::: {#pak-sjlabelled}

***

{**sjlabelled**}: [Labelled Data Utility
Functions](https://strengejacke.github.io/sjlabelled/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-sjlabelled-min.png){width="176"}

Basically, this package covers reading and writing data between other
statistical packages (like ‘SPSS’) and R, based on the haven and foreign
packages; hence, this package also includes functions to make working
with labelled data easier. This includes easy ways to get, set or change
value and variable label attributes, to convert labelled vectors into
factors or numeric (and vice versa), or to deal with multiple declared
missing values.
:::

The prefix `sj` in {**sjlabelled**} (= in German Strenge Jacke, "strict
jacket") refers to other work of [Daniel
Lüdecke](https://www.uke.de/kliniken-institute/institute/medizinische-soziologie/team/profil_daniel_luedecke_ims.html),
who has developed many R packages. All the `sj`-packages support labelled data.

sjlabelled: A Package for Labelled Data Utility Functions

:::

***

His packages are divided in two approaches:

1.  Most packages are part pf the project
    [EasyStats](https://easystats.github.io/easystats/), that provides
    with 11 packages "An R Framework for Easy Statistical Modeling,
    Visualization, and Reporting", similar to the {**tidyverse**}
    collection. The {**easystats**} collection is orientated more to
    statistics, whereas {**tidyverse**} is more directed to data
    science.
2.  The other line of package development supports labelled data in
    combination with different R task like
    -   [Data and Variable Transformation
        Functions](https://cran.r-project.org/web/packages/sjmisc/index.html)
        {**sjmisc**},
    -   [Data Visualization for Statistics in Social
        Science](https://cran.r-project.org/web/packages/sjPlot/index.html)
        {**sjPlot**} and a
    -   [Collection of Convenient Functions for Common Statistical
        Computations](https://cran.r-project.org/web/packages/sjstats/index.html)
        {**sjStats**}. -Additionally there exists {**sjtable2df**} a
        package to [Convert 'sjPlot' HTML-Tables to R
        'data.frame'](https://cran.r-project.org/web/packages/sjtable2df/index.html).





::::
:::::

## sjPlot

:::::{.my-resource}
:::{.my-resource-header}
sjPlot: Data Visualization for Statistics in Social Science 
:::
::::{.my-resource-container}

***

::: {#pak-sjPlot}

***

{**sjPlot**}: [Data Visualization for Statistics in Social Science](https://strengejacke.github.io/sjPlot/) [@sjPlot]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap05/logoi/logo-sjPlot-min.png){width="176"}

Collection of plotting and table output functions for data visualization. Results of various statistical analyses (that are commonly used in social sciences) can be visualized using this package, including simple and cross tabulated frequencies, histograms, box plots, (generalized) linear models, mixed effects models, PCA and correlation matrices, cluster analyses, scatter plots, Likert scales, effects plots of interaction terms in regression models, constructing index or score variables and much more.

:::

{**sjPlot**}: Data Visualization for Statistics in Social Science
:::

The standard plot versions are easy to create, but to adapt the resulted graph is another issue. Although {**sjPlot**} uses in the background the {**ggplot2**} package, you can’t specify changes with ggplot2 commands. I tried it and it produced two different plots. 

To customize plot appearance you have to learn the many arguments of of `sjPlot:set_theme()` and `sjPlot::plot_grpfrq()`. See the documentation of the [many specialized functions](https://strengejacke.github.io/sjPlot/reference/index.html#plot-customization) to tweak the default values.


***
::::
:::::







## sjstats

:::::{.my-resource}
:::{.my-resource-header}
sjstats: Collection of Convenient Functions for Common Statistical Computations 
:::
::::{.my-resource-container}

***

::: {#pak-sjstats}

***

{**sjstats**}: [Collection of Convenient Functions for Common Statistical Computations](https://strengejacke.github.io/sjstats/) [@sjstats]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap04/logoi/logo-sjstats-min.png){width="176"}

Collection of convenient functions for common statistical computations, which are not directly provided by R’s base or stats packages.

:::

This package aims at providing, *first*, shortcuts for statistical measures, which otherwise could only be calculated with additional effort (like Cramer’s V, Phi, or effict size statistics like Eta or Omega squared), or for which currently no functions available.

*Second*, another focus lies on weighted variants of common statistical measures and tests like weighted standard error, mean, t-test, correlation, and more.

The comprised tools include:

- Especially for mixed models: design effect, sample size calculation
- Weighted statistics and tests for: mean, median, standard error, standard deviation, correlation, Chi-squared test, t-test, Mann-Whitney-U-test


{**sjstats**}: A Collection of Convenient Functions for Common Statistical Computations
:::


***
::::
:::::


## skimr



:::::{.my-resource}
:::{.my-resource-header}
skimr: Compact and Flexible Summaries of Data
:::
::::{.my-resource-container}

***

::: {#pak-skimr}

***

{**skimr**}: [Compact and Flexible Summaries of Data](https://docs.ropensci.org/skimr/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-skimr-min.png){width="176"}

A simple to use summary function that can be used with pipes and displays nicely in the console. The default summary statistics may be modified by the user as can the default formatting.  Support for data frames and vectors is included, and users can implement their own skim methods for specific object types as described in a vignette. Default summaries include support for inline spark graphs. Instructions for managing these on specific operating systems are given in the [Using skimr](https://docs.ropensci.org/skimr/articles/skimr.html) vignette and the [README](https://github.com/ropensci/skimr/#skimr-).


:::

At the moment I am just using the `skimr::skim()` function. I believe most of the many other functions for adaption are oriented to developers. But still: I need to have a closer look to this package.


A Package for Compact and Flexible Summaries of Data
:::

***
::::
:::::


## statpsych

:::::{.my-resource}
:::{.my-resource-header}
statpsych: Statistical Methods for Psychologists
:::
::::{.my-resource-container}

***

::: {#pak-statpsych}

***

{**statpsych**}: [Statistical Methods for Psychologists](URL to package)

(*There is no hexagon logo for {**statpsych**} available.*)

{**statpsych**} implements confidence interval and sample size methods that are especially useful in psychological research. 

The methods can be applied in 1-group, 2-group, paired-samples, and multiple-group designs and to a variety of parameters including means, medians, proportions, slopes, standardized mean differences, standardized linear contrasts of means, plus several measures of correlation and association. 

The confidence intervals and sample size functions are applicable to single parameters as well as differences, ratios, and linear contrasts of parameters. 

The sample size functions can be used to approximate the sample size needed to estimate a parameter or function of parameters with desired confidence interval precision or to perform a variety of hypothesis tests (directional two-sided, equivalence, superiority, noninferiority) with desired power. For details, see: https://dgbonett.sites.ucsc.edu/.



{**statpsych**}: A Package for Statistical Methods for Psychologists
:::

***
::::
:::::

## stringi

:::::{.my-resource}
:::{.my-resource-header}
stringi: Fast and Portable Character String Processing Facilities 
:::
::::{.my-resource-container}

***

::: {#pak-stringi}

***

{**stringi**}: [Fast and Portable Character String Processing Facilities](https://stringi.gagolewski.com/) [@stringi]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-stringi-min.png){width="176"}


A collection of character string/text/natural language processing tools for pattern searching (e.g., with 'Java'-like regular expressions or the 'Unicode' collation  algorithm), random string generation, case mapping, string transliteration, concatenation, sorting, padding, wrapping, Unicode normalization, date-time formatting and parsing, and many more. 

:::

The {**stringi**} tools are fast, consistent, convenient, and - thanks to `r glossary("ICU")` ([International Components for Unicode](https://icu.unicode.org/home)) - portable across all locales and platforms. Documentation about {**stringi**} is provided via its website at <https://stringi.gagolewski.com/> and the paper by Gagolewski [-@gagolewski2022].

{**stringi**}: Fast and Portable Character String Processing Facilities
:::

***
::::
:::::


## stringr

:::::{.my-resource}
:::{.my-resource-header}
stringr: Simple, Consistent Wrappers for Common String Operations 
:::
::::{.my-resource-container}

***

::: {#pak-stringr}

***

{**stringr**}: [Simple, Consistent Wrappers for Common String Operations](https://stringr.tidyverse.org) [@stringr]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-stringr-min.png){width="176"}

A consistent, simple and easy to use set of wrappers around the fantastic 'stringi' package. All function and argument names (and positions) are consistent, all functions deal with "NA"'s and zero length vectors in the same way, and the output from one function is easy to feed into the input of another.

:::

Strings are not glamorous, high-profile components of R, but they do play a big role in many data cleaning and preparation tasks. The {**stringr**} package provides a cohesive set of functions designed to make working with strings as easy as possible. If you’re not familiar with strings, the best place to start is the [chapter on strings](https://r4ds.hadley.nz/strings) in R for Data Science.

{**stringr**} is built on top of {**stringi**}, which uses the [ICU](https://icu.unicode.org/) C library to provide fast, correct implementations of common string manipulations. {**stringr**} focusses on the most important and commonly used string manipulation functions whereas stringi provides a comprehensive set covering almost anything you can imagine. If you find that {**stringr**} is missing a function that you need, try looking in {**stringi**}. Both packages share similar conventions, so once you’ve mastered {**stringr**}, you should find {**stringi**} similarly easy to use.



{**stringr**}: Simple, Consistent Wrappers for Common String Operations
:::

Even if I had not used {**stringi**} in this book I will add this package profile into the appropriate section @pak-stringi.

***
::::
:::::


## tableone

:::::{.my-resource}
:::{.my-resource-header}
tableone: Create 'Table 1' to Describe Baseline Characteristics with or without Propensity Score Weights 
:::
::::{.my-resource-container}

***

::: {#pak-tableone}

***

{**tableone**}: [Create 'Table 1' to Describe Baseline Characteristics with or without Propensity Score Weights](https://github.com/kaz-yos/tableone) [@tableone]

Creates 'Table 1', i.e., description of baseline patient characteristics, which is essential in every medical research. Supports both continuous and categorical variables, as well as p-values and standardized mean differences. Weighted data are supported via the {**survey**} package.

{**tableone**}: Create 'Table 1' to Describe Baseline Characteristics with or without Propensity Score Weights
:::

Instead of using {**tableone**} I will use {**gtsummry**} in conjunction with {**gt**}.

***
::::
:::::


## tabulizer

:::::{.my-resource}
:::{.my-resource-header}
tabulizer: Bindings for 'Tabula' PDF Table Extractor Library 
:::
::::{.my-resource-container}

***

::: {#pak-tublizer}

***

{**tabulizer**}: [Bindings for 'Tabula' PDF Table Extractor Library](https://docs.ropensci.org/tabulizer) [@tabulizer]

(*There is no hexagon logo for {**tabulizer**} available*)


{**tabulizer**} provides R bindings to the [Tabula java library](https://github.com/tabulapdf/tabula-java/), which can be used to computationaly extract tables from PDF documents. The {**tabulizerjars**} package <https://github.com/ropensci/tabulizerjars> provides versioned 'Java' .jar files, including all dependencies, aligned to releases of 'Tabula'.

tabulizer depends on {**rJava**}, which implies a system requirement for Java. This can be frustrating, especially on Windows.

{**tabulizer**}: A Package for Bindings for 'Tabula' PDF Table Extractor Library
:::


:::
:::::

I just noticed that {**tabulizer**} was removed from the CRAN repository. But you can still install it from the [CRAN archive](https://cran.r-project.org/src/contrib/Archive/tabulizer/) --- or even better --- from the [GitHub site](https://github.com/ropensci/tabulizer/). I have installed it several years ago (version 0.2.3) and it works smoothly.

I have looked and tested alternatives, but nothing worked satisfactorily:


[{**pdftool**}](https://docs.ropensci.org/pdftools/): A great tool to scrap text from PDFs, but not so good with tables: "It is possible to use {**pdftools**} with some creativity to parse tables from PDF documents, which does not require Java to be installed." 

An example how to do that is explained in [How to extract data tables from PDF in r Tutorial](How to extract data tables from PDF in r Tutorial), a video by Data Centrics Inc. Another approach can be found on [StackOverflow](https://stackoverflow.com/questions/60127375/using-the-pdf-data-function-from-the-pdftools-package-efficiently?rq=2). But both procedures are way to complex and I have to say that it does not repays the effort, especially with the small example table in the video tutorial. It would be much easier to use other tools, for instance on macOS with the app [TextSniper](https://www.textsniper.app/en) or even input the figures manually.

[{**PDE**}](https://cran.r-project.org/web/packages/PDE/vignettes/PDE.html) PDF Data extractor (PDE) seems the right tool for the task because it should "Extract Tables and Sentences from PDFs with User Interface". I couldn't work with interactive user interface because I has many different options and I didn't have time to study them thoroughly. But I succeeded with the programming interface, although the result had some errors. Part of columns appeared in extra columns. So these errors were easy to detect and to repair.

With the following code I could extract all 13 tables from the ATF document and could also scrap the pictures in the PDF and convert them to PNGs.

```
atf_tables <- PDE::PDE_pdfs2table(
    pdfs = "data/chap03/firearms_commerce_2019.pdf",
    out = "data/chap03/test/",
    table.heading.words = "Exhibit",
    out.table.format = ".csv (macintosh)"
)
```
It took me about 20-30 seconds and I got the following message:

> Following file is processing: 'firearms_commerce_2019.pdf'
No filter words chosen for analysis.
13 table(s) found in 'firearms_commerce_2019.pdf'.
Analysis of 'firearms_commerce_2019.pdf' complete.

Maybe the interactive UI would also work, but as I am very content with {**tabulizer**} I did not delve deeply into {**PDE**}.

:::::{.my-important}
:::{.my-important-header}
My recommendation: As the first choice try to install and use {**tabulizer**}. If this does not work for you, try {**PDE**}. 

***
::::
:::::





## tibble 

:::::{.my-resource}
:::{.my-resource-header}
tibble: Simple Data Frames
:::
::::{.my-resource-container}


***

::: {#pak-tibble}

***

{**tibble**}: [Simple Data Frames](https://tibble.tidyverse.org/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-tibble-min.png){width="176"}

A tibble, or `tbl_df`, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. Tibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced `print()` method which makes them easier to use with large datasets containing complex objects. [@tibble]

:::


Package for Creating Simple Data Frames
:::

***


::::
:::::

## tidyr 

:::::{.my-resource}
:::{.my-resource-header}
tidyr: Tidy Messy Data
:::
::::{.my-resource-container}


***

::: {#pak-tidyr}

***

{**tidyr**}: [Tidy Messy Data](https://tidyr.tidyverse.org/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-tidyr-min.png){width="176"}

The goal of tidyr is to help you create tidy data. Tidy data describes a standard way of storing data that is used wherever possible throughout the tidyverse. If you ensure that your data is tidy, you’ll spend less time fighting with the tools and more time working on your analysis. [@tidyr] Tidy data is data where:
- Every column is a variable.
- Every row is an observation.
- Every cell is a single value.

:::

Packages for Tidying Messy Data
:::


***

::::
:::::




## tidyselect

:::::{.my-resource}
:::{.my-resource-header}
tidyselect: Select from a Set of Strings
:::
::::{.my-resource-container}

***

::: {#pak-tidyselect}

***

{**tidyselect**}: [Select from a Set of Strings](https://tidyselect.r-lib.org/index.html)

{*There is no hexagon logo for {**tidyselect**} available*}

The {**tidyselect**} package is the backend of functions like `dplyr::select()` or `dplyr::pull()` as well as several {**tidyr**} verbs. It allows you to create selecting verbs that are consistent with other {**tidyverse**} packages.

To learn about the selection syntax as a user of {**dplyr**} or {**tidyr**}, read the user-friendly [?language](https://tidyselect.r-lib.org/reference/language.html) reference.

To learn how to implement tidyselect in your own functions, read [vignette("tidyselect")](https://tidyselect.r-lib.org/articles/tidyselect.html).

To learn exactly how the {**tidyselect**} syntax is interpreted, read the technical description in [vignette("syntax")](https://tidyselect.r-lib.org/articles/syntax.html).

{**tidyselect**}: A Package to Select from a Set of Strings
:::

***
::::
:::::








## tidyverse

:::::{.my-resource}
:::{.my-resource-header}
tidyverse: Easily Install and Load the 'Tidyverse'
:::
::::{.my-resource-container}

***

::: {#pak-tidyverse}

***

{**tidyverse**}: [Easily Install and Load the 'Tidyverse'](https://www.tidyverse.org/)

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap01/logoi/logo-tidyverse-min.png){width="176"}

The {**tidyverse**} is an opinionated [collection of R packages](https://www.tidyverse.org/packages/) designed for data science. All packages share an underlying design philosophy, grammar, and data structures [@tidyverse]. Read more about the philosophy and purpose: [The tidy tools manifesto](https://tidyverse.tidyverse.org/articles/manifesto.html) and [Welcome to the {**tidyverse**}](https://tidyverse.tidyverse.org/articles/paper.html) 

:::

In this book I am not going to load {**tidyverse**} with all its packages. Instead I am using the `<package>::<function>` format to access the commands.



Metapackage: Easily Install and Load the 'Tidyverse'

***

:::

::::
:::::


## vcd

:::::{.my-resource}
:::{.my-resource-header}
vcd: Visualizing Categorical Data 
:::
::::{.my-resource-container}

***

::: {#pak-vcd}

***

{**vcd**}: [Visualizing Categorical Data](https://cran.r-project.org/web/packages/vcd/index.html) [@vcd]                      

(*There is no hexagon logo for {**vcd**} available*)

Visualization techniques, data sets, summary and inference procedures aimed particularly at categorical data. Special emphasis is given to highly extensible grid graphics. The package was package was originally inspired by the book "Visualizing Categorical Data" by Michael Friendly and is now the main support package for a new book, "Discrete Data Analysis with R" by Michael Friendly and David Meyer [-@friendly2015].

{**vcd**}: A Package for Visualizing Categorical Data
:::

***
::::
:::::

## vcdExtra

:::::{.my-resource}
:::{.my-resource-header}
vcdExtra: 'vcd' Extensions and Additions 
:::
::::{.my-resource-container}

***

::: {#pak-vcdExtra}

***

{**vcdExtra**}: ['vcd' Extensions and Additions](https://friendly.github.io/vcdExtra/) [@vcdExtra]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap03/logoi/logo-vcdExtra-min.png){width="176"}


This package provides additional data sets, documentation, and many functions designed to extend the vcd package for Visualizing Categorical Data and the {**gnm**} package for Generalized Nonlinear Models. In particular, {**vcdExtra**} extends mosaic, assoc and sieve plots from {**vcd**} to handle `stats::glm()` and `gnm::gnm()` models and adds a 3D version in `vcdExtra::mosaic3d()`.

:::

{**vcdExtra**} is a support package for the book Discrete Data Analysis with R (DDAR) by Michael Friendly and David Meyer [-@friendly2015]. There is also a [web site for DDAR](http://ddar.datavis.ca/) with all figures and code samples from the book. It is also used in my graduate course, [Psy 6136: Categorical Data Analysis](https://friendly.github.io/psy6136/).

{**vcdExtra**}: A Package for 'vcd' Extensions and Additions
:::

***
::::
:::::



## viridis

::: my-resource
::: my-resource-header
viridis: Colorblind-Friendly Color Maps for R
:::

::: my-resource-container

------------------------------------------------------------------------

::: {#pak-viridis}

------------------------------------------------------------------------

{**viridis**}: <https://sjmgarnier.github.io/viridis/> [@viridis]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap03/logoi/logo-viridis-min.png){width="176"}

{**viridis**}, and its companion package {**viridisLite**} provide a
series of color maps that are designed to improve graph readability for
readers with common forms of color blindness and/or color vision
deficiency. The color maps are also perceptually-uniform, both in
regular form and also when converted to black-and-white for printing.
:::

{**viridisLite**} provides the base functions for generating the color
maps in base R. The package is meant to be as lightweight and
dependency-free as possible for maximum compatibility with all the R
ecosystem. {**viridis**} provides additional functionalities, in
particular bindings for {**ggplot2**}.

{**viridis**}: A Package for Colorblind-Friendly Color Maps for R
:::

:::
:::

## waffle

::: my-resource
::: my-resource-header
waffle: Create Waffle Chart Visualizations
:::

::: my-resource-container

------------------------------------------------------------------------

::: {#pak-waffle}

------------------------------------------------------------------------

{**waffle**}: [Create Waffle Chart
Visualizations](https://cinc.rud.is/web/packages/waffle/index.html)

(*There is no hexagon logo for {**waffle**} available*)

Square pie charts (a.k.a. waffle charts) can be used to communicate
parts of a whole for categorical quantities. To emulate the percentage
view of a pie chart, a 10x10 grid should be used with each square
representing 1% of the total.

Modern uses of waffle charts do not necessarily adhere to this rule and
can be created with a grid of any rectangular shape.

Best practices suggest keeping the number of categories small, just as
should be done when creating pie charts.

Tools are provided to create waffle charts as well as stitch them
together, and to use glyphs for making isotype pictograms.

It uses {**ggplot2**} and returns a `ggplot2` object.

{**waffle**}: A Package for Creating Waffle Chart Visualizations


::::
:::::


:::

## withr

:::::{.my-resource}
:::{.my-resource-header}
withr: Run Code 'With' Temporarily Modified Global State 
:::
::::{.my-resource-container}

***

::: {#pak-withr}

***

{**withr**}: [Run Code 'With' Temporarily Modified Global State](https://withr.r-lib.org/) [@withr]

::: {layout="[10, 30]" layout-valign="center"}
![](img/chap03/logoi/logo-withr-min.png){width="176"}


A set of functions to run code with safely and temporarily modified global state, withr makes working with the global state, i.e. side effects, less error-prone.

:::

Pure functions, such as the `sum()` function, are easy to understand and reason about: they always map the same input to the same output and have no other impact on the workspace. In other words, pure functions have no *side effects*: they are not affected by, nor do they affect, the global state in any way apart from the value they return.

The purpose of the {**withr**} package is to help you manage side effects in your code. You may want to run code with secret information, such as an API key, that you store as an environment variable. You may also want to run code with certain options, with a given random-seed, or with a particular working-directory.

The {**withr**} package helps you manage these situations, and more, by providing functions to modify the global state temporarily, and safely. These functions modify one of the global settings for duration of a block of code, then automatically reset it after the block is completed.

{**withr**}: Run Code 'With' Temporarily Modified Global State
:::


***

:::
:::








## qualvar

:::::{.my-resource}
:::{.my-resource-header}
qualvar: Implements Indices of Qualitative Variation Proposed by Wilcox (1973)
:::
::::{.my-resource-container}

***

::: {#pak-qualvar}

***

{**qualvar**}: [Implements Indices of Qualitative Variation Proposed by Wilcox (1973)](http://joelgombin.github.io/qualvar/)

(*There is no hexagon log for {**qualvar**} available*)

In 1973, Wilcox published a paper presenting various indices of qualitative variation for social scientists. The problem is to find relevant statistical indices to measure the variation in nominal-scale (i.e. qualitative or categorical) data. Please see the Wilcox paper for more details on the rationale.

Wilcox presents six indices that can be used to measure qualitative variation. The {**qualvar**} package implements these indices so that R users can easily use them.

{**qualvar**}: A Package for Computing Indices of Qualitative Variation
:::



***
::::
:::::

