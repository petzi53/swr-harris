{
  "hash": "d4ce58a67f1830a6d771fa9d6f9ac874",
  "result": {
    "engine": "knitr",
    "markdown": "# Chi-Squared Test {#sec-chap05}\n\n\n\n\n\n## Achievements to unlock\n\n::: my-objectives\n::: my-objectives-header\nObjectives\n:::\n\n::: my-objectives-container\n**SwR Achievements**\n\n-   **Achievement 1**: Understanding the relationship between two\n    categorical variables using bar charts, frequencies, and percentages (@sec-chap05-achievement1)\n-   **Achievement 2**: Computing and comparing observed and expected\n    values for the groups (@sec-chap05-achievement2)\n-   **Achievement 3**: Calculating the chi-squared statistic for the\n    test of independence (@sec-chap05-achievement3)\n-   **Achievement 4**: Interpreting the chi-squared statistic and making\n    a conclusion about whether or not there is a relationship (@sec-chap05-achievement4)\n-   **Achievement 5**: Using Null Hypothesis Significance Testing to\n    organize statistical testing (@sec-chap05-achievement5)\n-   **Achievement 6**: Using standardized residuals to understand which\n    groups contributed to significant relationships (@sec-chap05-achievement6)\n-   **Achievement 7**: Computing and interpreting effect sizes to\n    understand the strength of a significant chi-squared relationship (@sec-chap05-achievement7)\n-   **Achievement 8**: Understanding the options for failed chi-squared\n    assumptions (@sec-chap05-achievement8)\n:::\n:::\n\n## The voter fraud problem\n\nInformation from studies suggests that voter fraud does happen but it\nis rare. In contrast to these studies a great minority of people (20-30%) in\nthe US believe that voter fraud is a big problem. Many states are\nbuilding barriers to vote, and other states make voting more easily, for\ninstance with automatic voter registration bills.\n\n## Resources & Chapter Outline\n\n### Data, codebook, and R packages {#sec-chap05-data-codebook-packages}\n\n::: my-resource\n::: my-resource-header\nData, codebook, and R packages for learning about descriptive statistics\n:::\n\n::: my-resource-container\n**Data**\n\nTwo options for assessing the data:\n\n1.  Download the data set `pew_apr_19-23_2017_weekly_ch5.sav` from\n    <https://edge.sagepub.com/harris1e>\n2.  Download the data set from the <a class='glossary' title='The Pew Research Center (also simply known as Pew) is a nonpartisan American think tank based in Washington, D.C. It provides information on social issues, public opinion, and demographic trends shaping the United States and the world. It also conducts public opinion polling, demographic research, random sample survey research, and panel based surveys, media content analysis, and other empirical social science research. (Wikipedia)'>Pew Research Center</a>\n    website\n    (<https://www.people-press.org/2017/06/28/public-supports-aimof-making-it-easy-for-all-citizens-to-vote/>)\n\n**Codebook**\n\nTwo options for assessing the documentation:\n\n1.  Download the documentation files `pew_voting_april_2017_ch5.pdf`,\n    `pew_voting_demographics_april_2017_ch5.docx`, and\n    `pew_chap5_readme.txt` from <https://edge.sagepub.com/harris1e>\n2.  Download the data set from the [Pew Research Center website](https://www.pewresearch.org/download-datasets/) and the\n    documentation will be included with the zipped file.\n\n**Packages**\n\n1.  Packages used with the book (sorted alphabetically)\n\n-   {**desc**}: @pak-descr (Jakson Alves de Aquino)\n-   {**fmsb**}: @pak-fmsb (Minato Nakazawa)\n-   {**haven**}: @pak-haven (Hadley Wickham)\n-   {**lsr**}: @pak-lsr (Danielle Navarro[^05-chi-squared-1])\n-   {**tidyverse**}: @pak-tidyverse (Hadley Wickham)\n\n2.  My additional packages (sorted alphabetically)\n:::\n:::\n\n[^05-chi-squared-1]: Not Daniel Navarro as mentioned in the book.\n    Danielle has changed her gender.\n\n### Get data\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap05-get-data}\n: Get data for chapter 5\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Pew data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-get-pew-data}\n: Get pew data about public support for making it easy to vote\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## run only once manually #########\nvote <- haven::read_sav(\"data/chap05/pew_apr_19-23_2017_weekly_ch5.sav\")\n\nvote <- vote |> \n    labelled::remove_labels()\nsave_data_file(\"chap05\", vote, \"vote.rds\")\n```\n:::\n\n\n***\n\n(*For this R code chunk is no output available*)\n\n:::::{.my-note}\n:::{.my-note-header}\nRemoving labels\n:::\n::::{.my-note-container}\n`haven::zap_labels()` as used in the book removes value labels and not variable labels. The correct function would be `haven::zap_label()`. I have used the {**labelled**} package where you can use `labelled::remove_labels()` to delete both (variable & value labels).\n::::\n:::::\n\n::::\n:::::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nError message with labelled data\n:::\n::::{.my-watch-out-container}\nI have removed the labelled data immediately, because I got an error message caused by summary statistics (e.g., `base::summary()`, `skimr::skim()`, `dplyr::summarize()`) whenever I rendered the file (but not when I compiled the code chunk.) \n\nI didn't have time to look into this issue --- and I had to remove the labels anyway. \n\nWhat follows is the error message:\n\n```\nQuitting from lines 180-186 [show-pew-raw-data] (05-chi-squared.qmd)\nError in `dplyr::summarize()`:\nℹ In argument: `skimmed = purrr::map2(...)`.\nCaused by error in `purrr::map2()`:\nℹ In index: 1.\nℹ With name: character.\nCaused by error in `dplyr::summarize()`:\nℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n  mangled_skimmers$funs)`.\nCaused by error in `across()`:\n! Can't compute column `state_~!@#$%^&*()-+character.empty`.\nCaused by error in `as.character()`:\n! Can't convert `x` <haven_labelled> to <character>.\nBacktrace:\n  1. skimr::skim(vote)\n 28. skimr (local) `<fn>`(state)\n 29. x %in% empty_strings\n 31. base::mtfrm.default(`<hvn_lbll>`)\n 33. vctrs:::as.character.vctrs_vctr(x)\n ```\n::::\n:::::\n\n\n\n\n\n###### header2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-code-name-b}\n: Numbered R Code Title (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 + 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 2\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n***\n\n### Show raw data\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap05-show-data}\n: Show raw data for chapter 5\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Vote\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-show-pew-raw-data}\n: Get pew data about public support for making it easy to vote\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvote <-  base::readRDS(\"data/chap05/vote.rds\")\nskimr::skim(vote)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |vote |\n|Number of rows           |1028 |\n|Number of columns        |49   |\n|_______________________  |     |\n|Column type frequency:   |     |\n|character                |4    |\n|numeric                  |45   |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|case_id       |         0|             1|   8|   8|     0|     1028|          0|\n|state         |         0|             1|   2|   2|     0|       51|          0|\n|date          |         0|             1|   6|   6|     0|        5|          0|\n|pew1rot       |         0|             1|   5|   5|     0|      120|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|    sd|     p0|   p25|    p50|    p75| p100|hist  |\n|:-------------|---------:|-------------:|------:|-----:|------:|-----:|------:|------:|----:|:-----|\n|week          |         0|          1.00| 816.00|  0.00| 816.00| 816.0| 816.00| 816.00|  816|▁▁▇▁▁ |\n|metro         |         0|          1.00|   2.45|  1.61|   0.00|   1.0|   2.00|   3.00|    5|▇▃▅▁▅ |\n|region        |         0|          1.00|   2.62|  1.03|   1.00|   2.0|   3.00|   3.00|    4|▅▆▁▇▅ |\n|division      |         0|          1.00|   5.08|  2.48|   1.00|   3.0|   5.00|   7.00|    9|▆▇▆▅▇ |\n|pew1arot      |         0|          1.00|   1.53|  0.50|   1.00|   1.0|   2.00|   2.00|    2|▇▁▁▁▇ |\n|pew1a         |         0|          1.00|   1.72|  0.95|   1.00|   1.0|   2.00|   2.00|    9|▇▁▁▁▁ |\n|pew1brot      |         0|          1.00|   1.47|  0.50|   1.00|   1.0|   1.00|   2.00|    2|▇▁▁▁▇ |\n|pew1b         |         0|          1.00|   1.90|  0.90|   1.00|   2.0|   2.00|   2.00|    9|▇▁▁▁▁ |\n|pew1crot      |         0|          1.00|   1.49|  0.50|   1.00|   1.0|   1.00|   2.00|    2|▇▁▁▁▇ |\n|pew1c         |         0|          1.00|   1.63|  1.26|   1.00|   1.0|   1.00|   2.00|    9|▇▁▁▁▁ |\n|pew1drot      |         0|          1.00|   1.51|  0.50|   1.00|   1.0|   2.00|   2.00|    2|▇▁▁▁▇ |\n|pew1d         |         0|          1.00|   1.87|  1.02|   1.00|   1.0|   2.00|   2.00|    9|▇▁▁▁▁ |\n|pew1erot      |         0|          1.00|   1.50|  0.50|   1.00|   1.0|   2.00|   2.00|    2|▇▁▁▁▇ |\n|pew1e         |         0|          1.00|   1.78|  1.77|   1.00|   1.0|   1.00|   2.00|    9|▇▁▁▁▁ |\n|pew2rot       |         0|          1.00|   1.53|  0.50|   1.00|   1.0|   2.00|   2.00|    2|▇▁▁▁▇ |\n|pew2          |         0|          1.00|   1.42|  0.96|   1.00|   1.0|   1.00|   2.00|    9|▇▁▁▁▁ |\n|ownhome       |         0|          1.00|   1.48|  1.15|   1.00|   1.0|   1.00|   2.00|    9|▇▁▁▁▁ |\n|mstatus       |         0|          1.00|   3.22|  1.75|   1.00|   2.0|   3.00|   5.00|    9|▆▇▂▂▁ |\n|employ        |         0|          1.00|   2.53|  1.75|   1.00|   1.0|   2.00|   3.00|    9|▇▅▁▂▁ |\n|totper        |         0|          1.00|   2.66|  1.72|   1.00|   1.0|   2.00|   3.00|    9|▇▃▁▁▁ |\n|adults        |         0|          1.00|   2.23|  1.40|   1.00|   1.0|   2.00|   3.00|    9|▇▂▁▁▁ |\n|kids1217      |       791|          0.23|   0.77|  0.79|   0.00|   0.0|   1.00|   1.00|    4|▇▇▃▁▁ |\n|kids611       |       791|          0.23|   0.56|  0.75|   0.00|   0.0|   0.00|   1.00|    3|▇▃▁▂▁ |\n|kidsless6     |       791|          0.23|   0.58|  0.83|   0.00|   0.0|   0.00|   1.00|    4|▇▃▂▁▁ |\n|parent        |       791|          0.23|   1.26|  0.44|   1.00|   1.0|   1.00|   2.00|    2|▇▁▁▁▃ |\n|age           |         0|          1.00|  54.71| 21.35|  18.00|  37.0|  56.00|  70.00|   99|▆▆▇▆▃ |\n|age2          |       989|          0.04|   3.49|  1.85|   1.00|   2.0|   3.00|   4.00|    9|▅▇▁▁▁ |\n|totalage      |         0|          1.00|   2.77|  1.13|   1.00|   2.0|   3.00|   4.00|    9|▆▇▁▁▁ |\n|refage        |         0|          1.00|   2.91|  1.29|   1.00|   2.0|   3.00|   4.00|    9|▅▇▁▁▁ |\n|educ          |         0|          1.00|   5.55|  9.42|   1.00|   3.0|   4.00|   6.00|   99|▇▁▁▁▁ |\n|income        |         0|          1.00|  17.06| 30.62|   1.00|   3.0|   6.00|  12.00|   99|▇▁▁▁▁ |\n|race          |         0|          1.00|   4.48| 15.07|   1.00|   1.0|   1.00|   2.00|   99|▇▁▁▁▁ |\n|affilrot      |         0|          1.00|   1.50|  0.50|   1.00|   1.0|   2.00|   2.00|    2|▇▁▁▁▇ |\n|polparty      |         0|          1.00|   2.47|  1.65|   0.00|   1.0|   2.00|   3.00|    9|▃▇▁▁▁ |\n|polviewrot    |         0|          1.00|   1.49|  0.50|   1.00|   1.0|   1.00|   2.00|    2|▇▁▁▁▇ |\n|polview       |         0|          1.00|   3.29|  1.83|   1.00|   2.0|   3.00|   4.00|    9|▇▇▂▁▂ |\n|regvote       |         0|          1.00|   1.24|  0.72|   1.00|   1.0|   1.00|   1.00|    9|▇▁▁▁▁ |\n|c3a           |       384|          0.63|   2.13|  9.43|   0.00|   1.0|   1.00|   1.00|   99|▇▁▁▁▁ |\n|sex           |         0|          1.00|   1.52|  0.50|   1.00|   1.0|   2.00|   2.00|    2|▇▁▁▁▇ |\n|religion      |         0|          1.00|  34.01| 37.57|   1.00|   2.0|  15.00|  90.00|   99|▇▂▁▁▅ |\n|ident         |         0|          1.00|   1.68|  0.66|   1.00|   1.0|   2.00|   2.00|    4|▆▇▁▁▁ |\n|c1a           |        76|          0.93|   1.41|  1.22|   0.00|   1.0|   1.00|   1.00|    9|▇▂▁▁▁ |\n|bornus        |       878|          0.15|   1.99|  1.13|   1.00|   1.0|   1.50|   3.00|    9|▇▆▁▁▁ |\n|qnco3         |      1028|          0.00|    NaN|    NA|     NA|    NA|     NA|     NA|   NA|      |\n|popwght       |         0|          1.00|   1.00|  0.66|   0.25|   0.5|   0.84|   1.33|    4|▇▃▂▁▁ |\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n###### header2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-code-name-b}\n: Numbered R Code Title (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 + 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 2\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n***\n\n### Recode data for chapter 5\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap05-recode-data}\n: Numbered Example Title\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Pew data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-recode-pew-data}\n: Select some columns from the pew data set\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvote <-  base::readRDS(\"data/chap05/vote.rds\")\n\n## create vote_clean #############\nvote_clean <-  vote |> \n    dplyr::select(pew1a, pew1b, race, sex, \n                  mstatus, ownhome, employ, polparty) |> \n    labelled::remove_labels() |> \n    dplyr::mutate(dplyr::across(1:8, forcats::as_factor)) |> \n    naniar::replace_with_na(replace = list(\n        pew1a = c(5, 9),\n        pew1b = c(5, 9),\n        race = 99,\n        ownhome = c(8, 9)\n    )) |> \n    dplyr::mutate(pew1a = forcats::fct_recode(pew1a,\n             \"Register to vote\" = \"1\",\n             \"Make easy to vote\" = \"2\",\n             )) |> \n    dplyr::mutate(pew1b = forcats::fct_recode(pew1b,\n             \"Require to vote\" = \"1\",\n             \"Choose to vote\" = \"2\",\n             )) |> \n    dplyr::mutate(race = forcats::fct_recode(race,\n             \"White non-Hispanic\" = \"1\",\n             \"Black non-Hispanic\" = \"2\",\n             )) |> \n    dplyr::mutate(race = forcats::fct_collapse(race,\n             \"Hispanic\" = c(\"3\", \"4\", \"5\"),\n             \"Other\" = c(\"6\", \"7\", \"8\", \"9\", \"10\")\n    )) |> \n    dplyr::mutate(sex = forcats::fct_recode(sex,\n             \"Male\" = \"1\",\n             \"Female\" = \"2\",\n             )) |> \n    dplyr::mutate(ownhome = forcats::fct_recode(ownhome,\n             \"Owned\" = \"1\",\n             \"Rented\" = \"2\",\n             )) |> \n    dplyr::mutate(dplyr::across(1:8, forcats::fct_drop)) |> \n    dplyr::rename(ease_vote = \"pew1a\",\n                  require_vote = \"pew1b\")\n\nsave_data_file(\"chap05\", vote_clean, \"vote_clean.rds\")\n    \nskimr::skim(vote_clean)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |vote_clean |\n|Number of rows           |1028       |\n|Number of columns        |8          |\n|_______________________  |           |\n|Column type frequency:   |           |\n|factor                   |8          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                            |\n|:-------------|---------:|-------------:|:-------|--------:|:-------------------------------------|\n|ease_vote     |        27|          0.97|FALSE   |        2|Mak: 593, Reg: 408                    |\n|require_vote  |        17|          0.98|FALSE   |        2|Cho: 806, Req: 205                    |\n|race          |        25|          0.98|FALSE   |        4|Whi: 646, His: 150, Bla: 129, Oth: 78 |\n|sex           |         0|          1.00|FALSE   |        2|Fem: 533, Mal: 495                    |\n|mstatus       |         0|          1.00|FALSE   |        7|3: 422, 1: 229, 6: 139, 5: 126        |\n|ownhome       |        22|          0.98|FALSE   |        2|Own: 678, Ren: 328                    |\n|employ        |         0|          1.00|FALSE   |        9|1: 414, 3: 309, 2: 133, 6: 50         |\n|polparty      |         0|          1.00|FALSE   |        6|3: 398, 2: 314, 1: 249, 8: 31         |\n\n\n:::\n:::\n\n\n***\nI have used in this recoding R chunk several functions for the first time:\n\n- I turned all character columns into factor variables with just one line of code using `dplyr::across()` in combination with `forcats::as_factor()`.\n- I replaced missing values (NAs) with the `replace_with_na()` function of the {**naniar**} package (see @pak-naniar).\n- I combined several levels with `forcats::fct_collapse()`.\n- And finally I dropped all unused levels in the whole data.frame using `dplyr::across()` in conjunction with `forcats::fct_drop()`.\n\n\n::::\n:::::\n\n\n###### header2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-code-name-b}\n: Numbered R Code Title (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n:::\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n***\n\n## Achievement 1: Relationship of two categorical variables {#sec-chap05-achievement1}\n\n### Descriptive statistics\n\nFor better display I have reversed the order of the variables: Instead of grouping y ease of vote I will group by race/ethnicity. This will give a smaller table with only two columns instead of four that will not fit on the sceen without horizontal scrolling.\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap05-stats-voting-data}\n: Frequencies between two categorical variables\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### summarize()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-summarize-ease-voting}\n: Summarize relationship ease of vote and race/ethnicity\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## load vote_clean ##########\nvote_clean <-  base::readRDS(\"data/chap05/vote_clean.rds\")\n\nease_vote_sum <- vote_clean |> \n    tidyr::drop_na(ease_vote) |> \n    tidyr::drop_na(race) |> \n    dplyr::group_by(race, ease_vote) |> \n    ## either summarize\n    dplyr::summarize(n = dplyr::n(),\n                     .groups = \"keep\")\n    ## or count the observation in each group\n    # dplyr::count()\nease_vote_sum\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 8 × 3\n#> # Groups:   race, ease_vote [8]\n#>   race               ease_vote             n\n#>   <fct>              <fct>             <int>\n#> 1 White non-Hispanic Register to vote    292\n#> 2 White non-Hispanic Make easy to vote   338\n#> 3 Black non-Hispanic Register to vote     28\n#> 4 Black non-Hispanic Make easy to vote    98\n#> 5 Hispanic           Register to vote     51\n#> 6 Hispanic           Make easy to vote    97\n#> 7 Other              Register to vote     27\n#> 8 Other              Make easy to vote    46\n```\n\n\n:::\n:::\n\n***\nHere I used \"standard\" tidyverse code to count frequencies. Instead of the somewhat complex last code line I could have used just `dplyr::count()` with the same result.\n\n\n\n::::\n:::::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Prevent warning with `.groups` argument\n:::\n::::{.my-watch-out-container}\nBy using two variables inside `dplyr::group_by()` I got a warning message:\n\n> `summarise()` has grouped output by 'ease_vote'. \n> You can override using the `.groups` argument.\n\nAt first I had to set the chunk option `warning: false` to turn off this warning. But finally I managed to prevent the warning with R code. See the [summarize help page](https://dplyr.tidyverse.org/reference/summarise.html) under arguments `.groups`. Another option to suppress the warning would have been `options(dplyr.summarise.inform = FALSE)`. See also the two [comments in StackOverflow](https://stackoverflow.com/questions/71914704/override-using-groups-argument) and [r-stats-tips](https://rstats-tips.net/2020/07/31/get-rid-of-info-of-dplyr-when-grouping-summarise-regrouping-output-by-species-override-with-groups-argument/).\n::::\n:::::\n\n\n###### pivot_wider()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-pivot-wider-ease-voting}\n: Summarize by converting data from long to wide with `pivot_wider()` from {**tidyr**}\n::::::\n:::\n::::{.my-r-code-container}\n\n:::{#lst-chap05-pivot-wider-ease-voting}\n\n::: {.cell}\n\n```{.r .cell-code}\nease_vote_wider <- vote_clean |> \n    tidyr::drop_na(ease_vote) |> \n    tidyr::drop_na(race) |> \n    dplyr::group_by(race, ease_vote) |> \n    dplyr::summarize(\n        n = dplyr::n(),\n        .groups = \"keep\") |> \n    tidyr::pivot_wider(\n        names_from = ease_vote,\n        values_from = n\n    )\nease_vote_wider\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 4 × 3\n#> # Groups:   race [4]\n#>   race               `Register to vote` `Make easy to vote`\n#>   <fct>                           <int>               <int>\n#> 1 White non-Hispanic                292                 338\n#> 2 Black non-Hispanic                 28                  98\n#> 3 Hispanic                           51                  97\n#> 4 Other                              27                  46\n```\n\n\n:::\n:::\n\nSummarizing and converting data from long to wide with `pivot_wider()` from {**tidyr**}\n:::\n\n***\n\nWe get with `dplyr::pivot_wider()` a more neatly arranged table.\n::::\n:::::\n\n###### table()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-base-table-ease-voting}\n: Summarize with `base::table()`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nease_vote_table <- base::table(\n    vote_clean$race, \n    vote_clean$ease_vote,\n    dnn = c(\"Race\", \"Ease of voting\")\n)\nease_vote_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                     Ease of voting\n#> Race                 Register to vote Make easy to vote\n#>   White non-Hispanic              292               338\n#>   Black non-Hispanic               28                98\n#>   Hispanic                         51                97\n#>   Other                            27                46\n```\n\n\n:::\n:::\n\n***\n\nNote that NA's are automatically excluded from the table.\n\n\n\n\n\n::::\n:::::\n\nWith the simple `base::table()` we will get a very similar result as in the more complex `dplyr::pivot_wider()` code variant in @lst-chap05-pivot-wider-ease-voting. \n\nBut I prefer in any case the tidyverse version for several reasons:\n\n:::::{.my-note}\n:::{.my-note-header}\nSome deficiencies of `base::table()` \n:::\n::::{.my-note-container}\n\n- `table()` does not accept data.frame as input and you can't therefore chain several commands together with the ` |> ` pipe.\n- `table()` does not output data.frames\n- `table()` is very difficult to format and to make it print ready.\n::::\n:::::\n\n###### xtabs()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-base-xtabs-ease-voting}\n: Summarize with a `stats::xtabs()`\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nease_vote_xtabs <- stats::xtabs(n ~ race + ease_vote, data = ease_vote_sum)\nease_vote_xtabs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                     ease_vote\n#> race                 Register to vote Make easy to vote\n#>   White non-Hispanic              292               338\n#>   Black non-Hispanic               28                98\n#>   Hispanic                         51                97\n#>   Other                            27                46\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n###### tabyl()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-tabyl-voting-data}\n: Frequencies with `tabyl()` from {**janitor**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nease_vote_tabyl <- vote_clean |> \n    janitor::tabyl(race, ease_vote, show_na = FALSE)\nease_vote_tabyl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                race Register to vote Make easy to vote\n#>  White non-Hispanic              292               338\n#>  Black non-Hispanic               28                98\n#>            Hispanic               51                97\n#>               Other               27                46\n```\n\n\n:::\n:::\n\n***\n\n`janitor::tabyl()` prevents the weaknesses of the `base::table()` function. It works with data.frames, is tidyverse compatible and has many `adorn_*` functions (`adorn_` stands for \"adornment\") to format the output values.\n::::\n:::::\n\n###### prop.table()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-contingency-prop-table-voting-data}\n: Summarize with a base R proportion contingency table \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase::prop.table(\n    base::table(`Race / Ethnicity` = vote_clean$race,\n          `Ease of voting` = vote_clean$ease_vote), margin = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                     Ease of voting\n#> Race / Ethnicity     Register to vote Make easy to vote\n#>   White non-Hispanic        0.4634921         0.5365079\n#>   Black non-Hispanic        0.2222222         0.7777778\n#>   Hispanic                  0.3445946         0.6554054\n#>   Other                     0.3698630         0.6301370\n```\n\n\n:::\n:::\n\n***\nAll was I said about flaws for `base::table()` is of course valid for the `base::prop.table()` function as well.\n\n::::\n:::::\n\n###### tabyl() formatted\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-janitor-voting-data}\n: Frequencies with `tabyl()` from {**janitor**} formatted\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvote_clean |> \n    janitor::tabyl(race, ease_vote, show_na = FALSE) |> \n    janitor::adorn_percentages(\"row\")  |> \n    janitor::adorn_pct_formatting(digits = 2)  |> \n    janitor::adorn_ns() |> \n    janitor::adorn_title(row_name = \"Race / Ethnicity\",\n                         col_name = \"Ease of voting\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                       Ease of voting                  \n#>    Race / Ethnicity Register to vote Make easy to vote\n#>  White non-Hispanic     46.35% (292)      53.65% (338)\n#>  Black non-Hispanic     22.22%  (28)      77.78%  (98)\n#>            Hispanic     34.46%  (51)      65.54%  (97)\n#>               Other     36.99%  (27)      63.01%  (46)\n```\n\n\n:::\n:::\n\n***\n\nIn this example you can see the power of the {**janitor**} package. The main purpose of the {**janitor**} is data cleaning, but because counting is such a fundamental part of data cleaning and exploration the `tabyl()` and `adorn_*()` has been included in this package.\n::::\n:::::\n\n###### Ease of voting\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-ease-voting-data}\n: Ease of voting by race / ethnicity\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#lst-chap05-ease-voting}\n\n::: {.cell}\n\n```{.r .cell-code}\nvote_clean |> \n    janitor::tabyl(race, ease_vote, show_na = FALSE) |> \n    janitor::adorn_percentages(\"row\")  |> \n    janitor::adorn_pct_formatting(digits = 2)  |> \n    janitor::adorn_ns() |> \n    janitor::adorn_title(row_name = \"Race / Ethnicity\",\n                         col_name = \"Ease of voting\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                       Ease of voting                  \n#>    Race / Ethnicity Register to vote Make easy to vote\n#>  White non-Hispanic     46.35% (292)      53.65% (338)\n#>  Black non-Hispanic     22.22%  (28)      77.78%  (98)\n#>            Hispanic     34.46%  (51)      65.54%  (97)\n#>               Other     36.99%  (27)      63.01%  (46)\n```\n\n\n:::\n:::\n\n\nEase of voting by race / ethnicity\n:::\n\n***\n\n::: {.callout-tip}\nThe voting registration policy a person favors differed by race/ethnicity.\n\n- White non-Hispanic participants were fairly evenly divided between those who thought people should register if they want to vote and those who thought voting should be made as easy as possible.\n- The other three race-ethnicity groups had larger percentages in favor of making it as easy as possible to vote.\n- Black non-Hispanic participants have the highest percentage (77.78%) in favor of making it easy to vote.\n:::\n\n\n::::\n:::::\n\n###### Require to vote\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-require-voting-data}\n: Voting as requirement or free choice by race /ethnicity\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#lst-chap05-require-voting}\n\n::: {.cell}\n\n```{.r .cell-code}\nvote_clean |> \n    janitor::tabyl(race, require_vote, show_na = FALSE) |> \n    janitor::adorn_percentages(\"row\")  |> \n    janitor::adorn_pct_formatting(digits = 2)  |> \n    janitor::adorn_ns() |> \n    janitor::adorn_title(row_name = \"Race / Ethnicity\",\n                         col_name = \"Voting as citizen duty or as a free choice?\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                     Voting as citizen duty or as a free choice?               \n#>    Race / Ethnicity                             Require to vote Choose to vote\n#>  White non-Hispanic                                 15.02% (96)   84.98% (543)\n#>  Black non-Hispanic                                 32.28% (41)   67.72%  (86)\n#>            Hispanic                                 34.01% (50)   65.99%  (97)\n#>               Other                                 18.92% (14)   81.08%  (60)\n```\n\n\n:::\n:::\n\nVoting as requirement or free choice by race /ethnicity\n\n:::\n\n***\n\n::: {.callout-tip}\nDifferent ethnicities have distinct opinions about the character of voting. \n\n- About one-third of Black non-Hispanic and Hispanic believe that voting should be a requirement. But this means on the other hand, that at least two-third of both groups see voting as a free choice. \n- In contrast to this proportion are white non-Hispanic and other non-Hispanic ethnicities: In those groups more than 80% favor voting as a free choice.\n\n:::\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n:::::{.my-resource}\n:::{.my-resource-header}\nCross-Tabulation\n:::\n::::{.my-resource-container}\n\n- [Working with Tables in R](https://bookdown.org/kdonovan125/ibis_data_analysis_r4/working-with-tables-in-r.html) in [@donovan2019a].\n- [Cross-Tabulation in R](https://www.marsja.se/cross-tabulation-in-r-creating-interpreting-contingency-tables/): Creating & Interpreting Contingency Tables [@marsja2023].\n- [Tables in R](https://cran.r-project.org/web/packages/DescTools/vignettes/TablesInR.pdf): A Quick Practical Overview [@signorell2021], see also [@pak-DescTools].\n- [Introduction to Crosstable](https://cran.r-project.org/web/packages/crosstable/vignettes/crosstable.html) [@chalthiel2023], see also [@pak-crosstable].\n\n::::\n:::::\n\n\n### Graphs\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap05-descriptive-graphs}\n: Descriptive graphs\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### geom_col()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-pew-voting-geom-col-graph}\n: Visualizing opinions about ease of voting by race / ethnicity \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_ease_vote <- vote_clean |> \n    ## prepare data\n    tidyr::drop_na(ease_vote) |>\n    tidyr::drop_na(race) |>\n    dplyr::group_by(race, ease_vote) |>\n    dplyr::count() |>\n    dplyr::group_by(race) |>\n    dplyr::mutate(perc = n / base::sum(n)) |>\n    \n    ## draw graph\n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = race, \n            y = perc,\n            fill = ease_vote)\n    ) +\n    ggplot2::geom_col(position = \"dodge\") +\n    ggplot2::scale_y_continuous(labels = scales::percent) +\n    ggplot2::labs(\n        x = \"Race / Ethnicity\",\n        y = \"Percent\"\n    ) +\n    ggplot2::scale_fill_viridis_d(\n        name = \"Ease of voting\",\n        alpha = .8, # here alpha works!!\n        begin = .25,\n         end = .75,\n        direction = -1,\n        option = \"viridis\"\n    )\n\np_ease_vote\n```\n\n::: {.cell-output-display}\n![Opinion on ease of voting by race / ethnicity from a study of the Pew Research Center 2017 (n = 1,028)](05-chi-squared_files/figure-html/fig-pew-voting-geom-col-graph-1.png){#fig-pew-voting-geom-col-graph width=672}\n:::\n:::\n\n***\n\nI had several difficulties by drawing this graph:\n\n1. Most important: I did not know that the second variable `ease_vote` has to be included by the `fill` argument. That seems not logical but together with `position = dodge` it make sense.\n2. I didn't know that I have to group by race again (the line after `dplyr::count()`)\n3. I thought that I could calculate the percentages with `ggplot2::after_stat()`. The solution was more trivial: Creating a new column with the calculated percentages and using `geom_col()` instead of `geom_bar()`.\n\nInstead of the last line I could have used with the same result: `ggplot2::geom_bar(position = \"dodge\", stat = \"identity\")`. `geom_bar()` uses as standard option `ggplot2::stat_count()`. It is however possible to override the default value as was done in the book code. But it easier here to use `geom_col()` because it uses as default `stat_identity()` e.g., it leaves the data as is.\n\n::: {.callout-note}\n**Two additional remarks**:\n\n1. I have used here the percent scale from the {**scales**} package to get percent signs on the y-axis.\n2. I practiced my learnings from @sec-chap03 about adding a color-friendly palette (see @sec-chap03-practice-test). (See also my color test in @cnj-chap05-color-test-bw.)\n:::\n\n\n::::\n:::::\n\n###### geom_bar()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-pew-voting-geom-bar-graph}\n: Visualizing opinions about ease of voting by race / ethnicity \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvote_clean |> \n    tidyr::drop_na(ease_vote) |>\n    tidyr::drop_na(race) |>\n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = race, \n            fill = ease_vote\n        )\n    ) +\n    ggplot2::geom_bar(position = \"dodge\",\n        ggplot2::aes(\n            y = ggplot2::after_stat(count / base::sum(count))\n        )) +\n    ggplot2::scale_y_continuous(labels = scales::percent) +\n    ggplot2::labs(\n        x = \"Race / Ethnicity\",\n        y = \"Percent\"\n    ) +\n    ggplot2::scale_fill_viridis_d(\n        name = \"Ease of voting\",\n        alpha = .8, # here alpha works!!\n        begin = .25,\n         end = .75,\n        direction = -1,\n        option = \"viridis\"\n    )\n```\n\n::: {.cell-output-display}\n![Opinion on ease of voting by race / ethnicity from a study of the Pew Research Center 2017 (n = 1,028)](05-chi-squared_files/figure-html/fig-pew-voting-geom-bar-graph-1.png){#fig-pew-voting-geom-bar-graph width=672}\n:::\n:::\n\n***\n\nHere I have used `geom_bar()` with the `after_stat()` calculation. It turned out that the function computes the percentages of the different race categories for the two `ease_vote` values. This was not was I had intended.\n\nI tried for several hours to use `after_stat()` with the same result as in @cnj-chap05-pew-voting-geom-col-graph, but I didn't succeed. I do not know if the reason is my missing knowledge (for instance to generate another structure of the data.frame) or if you can't do that in general. \n\n::::\n:::::\n\n###### geom_col() with labels\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-pew-voting-geom-col-label-graph}\n: Visualizing opinions about ease of voting by race / ethnicity \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvote_clean |> \n    tidyr::drop_na(ease_vote) |>\n    tidyr::drop_na(race) |>\n    dplyr::group_by(race, ease_vote) |>\n    dplyr::count() |>\n    dplyr::group_by(race) |>\n    dplyr::mutate(perc = n / base::sum(n)) |>\n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = race, \n            y = perc,\n            fill = ease_vote)\n    ) +\n    ggplot2::geom_col(position = \"dodge\") +\n    ggplot2::geom_label(\n        ggplot2::aes(\n            x = race,\n            y = perc,\n            label = paste0(round(100 * perc, 1),\"%\"),\n            vjust = 1.5, hjust = -.035\n        ),\n        color = \"white\"\n    ) +\n    ggplot2::scale_y_continuous(labels = scales::percent) +\n    ggplot2::labs(\n        x = \"Race / Ethnicity\",\n        y = \"Percent\"\n    ) +\n    ggplot2::scale_fill_viridis_d(\n        name = \"Ease of voting\",\n        alpha = .8, # here alpha works!!\n        begin = .25,\n         end = .75,\n        direction = -1,\n        option = \"viridis\"\n    )\n```\n\n::: {.cell-output-display}\n![Opinion on ease of voting by race / ethnicity from a study of the Pew Research Center 2017 (n = 1,028)](05-chi-squared_files/figure-html/fig-pew-voting-geom-col-label-graph-1.png){#fig-pew-voting-geom-col-label-graph width=672}\n:::\n:::\n\n***\n\nHere I have experimented with labels. It seems that with the argument `position = dodge` the labels can't appear on each of the appropriate bars.\n\n::::\n:::::\n\n###### requirements\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-pew-voting-requirements-by-race}\n: Visualizing opinions about requirements of voting by race / ethnicity \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_require_vote <- vote_clean |> \n    ## prepare data\n    tidyr::drop_na(require_vote) |>\n    tidyr::drop_na(race) |>\n    dplyr::group_by(race, require_vote) |>\n    dplyr::count() |>\n    dplyr::group_by(race) |>\n    dplyr::mutate(perc = n / base::sum(n)) |>\n    \n    ## draw graph\n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = race, \n            y = perc,\n            fill = require_vote)\n    ) +\n    ggplot2::geom_col(position = \"dodge\") +\n    ggplot2::scale_y_continuous(labels = scales::percent) +\n    ggplot2::labs(\n        x = \"Race / Ethnicity\",\n        y = \"Percent\"\n    ) +\n    ggplot2::scale_fill_viridis_d(\n        name = \"Requirements of voting\",\n        alpha = .8, # here alpha works!!\n        begin = .25,\n         end = .75,\n        direction = -1,\n        option = \"viridis\"\n    )\n\np_require_vote\n```\n\n::: {.cell-output-display}\n![Opinion on voting requirements by race / ethnicity from a study of the Pew Research Center 2017 (n = 1,028)](05-chi-squared_files/figure-html/fig-pew-voting-requirements-by-race-1.png){#fig-pew-voting-requirements-by-race width=672}\n:::\n:::\n\n\n\n\n::::\n:::::\n\n###### Voting by race\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-voting-opinions-by-race}\n: Visualizing opinions about voting by race / ethnicity \n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\np_ease <- p_ease_vote +\n    ggplot2::labs(\n        x = \"\",\n        y = \"Percent within group\"\n    ) +\n    ggplot2::scale_fill_viridis_d(\n        name = \"Opinion on\\nvoter registration\",\n        alpha = .8, \n        begin = .25,\n        end = .75,\n        direction = -1,\n        option = \"viridis\"\n    ) +\n    ggplot2::theme(axis.text.x = ggplot2::element_blank())\n\np_require <- p_require_vote +\n    ggplot2::labs(y = \"Percent within group\") +\n    ggplot2::scale_fill_viridis_d(\n        name = \"Opinion on\\nvoting\",\n        alpha = .8,\n        begin = .25,\n        end = .75,\n        direction = -1,\n        option = \"viridis\"\n    )\n\ngridExtra::grid.arrange(p_ease, p_require, ncol = 1)\n```\n\n::: {.cell-output-display}\n![Opinion on ease of voting and voting requirements by race / ethnicity from a study of the Pew Research Center 2017 (n = 1,028)](05-chi-squared_files/figure-html/fig-pew-voting-by-race-1.png){#fig-pew-voting-by-race width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\n###### Color test\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-color-test-bw}\n: Test how the colors used for the graph race by ease of voting look for printing in black & white\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {#lst-chap05-color-test-bw}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npal_data <- list(names = c(\"Normal\", \"desaturated\"),\n    color = list(scales::viridis_pal(\n                                alpha = .8, \n                                begin = .25, \n                                 end = .75, \n                                direction = -1, \n                                option = \"viridis\")(2),\n    colorspace::desaturate(scales::viridis_pal(\n                                alpha = .8, \n                                begin = .25, \n                                end = .75, \n                                direction = -1, \n                                option = \"viridis\")(2)))\n    )\nlist_plotter(pal_data$color, pal_data$names, \n    \"Colors and black & white of graph race by ease of voting\")\n```\n\n::: {.cell-output-display}\n![Test if used colors of my graph race by ease of voting look are also readable for black & white printing](05-chi-squared_files/figure-html/fig-color-test-bw-1.png){#fig-color-test-bw width=672}\n:::\n:::\n\n\nTest how the colors I have used for my graphs about race by ease of voting look in black & white\n:::\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n\n## Achievement 2: Comparing groups {#sec-chap05-achievement2}\n\nThe <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a> test is useful for testing to see if there may be a statistical relationship between two categorical variables. The chi-squared test is based on the observed values, and the values expected to occur if there were no relationship between the variables.\n\n### Observed values\n\nWe will use the observed values from @lst-chap05-ease-voting and @lst-chap05-require-voting.\n\n### Expected values\n\nFor each cell in the table, multiply the row total for that row by the column total for that column and divide by the overall total.\n\nTo prevent manually computing the values I have used `CrossTable()` from the {**descr**} package (see @pak-descr and [StackOverflow](https://stackoverflow.com/a/34214973/7322615)).\n\n$$\n\\text{Expected Values} = \\frac{rowTotal \\times columnTotal}{Total}\n$$ {#eq-chap05-expected-values}\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap05-expected-values}\n: Show observed and expected values\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Ease\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-expected-ease-vote}\n: Ease of voting by race / ethnicity\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvote_clean <- base::readRDS(\"data/chap05/vote_clean.rds\")\n\nvote_opinions <- vote_clean |> \n    dplyr::select(race, ease_vote, require_vote) |>\n    tidyr::drop_na()\n\nct_ease <- descr::CrossTable(\n    x = vote_opinions$race,\n    y = vote_opinions$ease_vote,\n    dnn = c(\"Race\", \"Ease of voting\"),\n    prop.r = FALSE, \n    prop.c = FALSE, \n    prop.t = FALSE,\n    prop.chisq = FALSE,\n    expected = TRUE\n    )\nct_ease\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    Cell Contents \n#> |-------------------------|\n#> |                       N | \n#> |              Expected N | \n#> |-------------------------|\n#> \n#> ==================================================================\n#>                       Ease of voting\n#> Race                  Register to vote   Make easy to vote   Total\n#> ------------------------------------------------------------------\n#> White non-Hispanic                 292                 335     627\n#>                                  255.5               371.5        \n#> ------------------------------------------------------------------\n#> Black non-Hispanic                  27                  97     124\n#>                                   50.5                73.5        \n#> ------------------------------------------------------------------\n#> Hispanic                            50                  96     146\n#>                                   59.5                86.5        \n#> ------------------------------------------------------------------\n#> Other                               25                  45      70\n#>                                   28.5                41.5        \n#> ------------------------------------------------------------------\n#> Total                              394                 573     967\n#> ==================================================================\n```\n\n\n:::\n:::\n\n\n***\n\n::: {.callout-tip}\n\n- Some of the cells have observed and expected values that are very close to each other. For example, the observed number of Other race-ethnicity people who want to make it easy to vote is 46, while the expected is 43.3. \n- But other categories show bigger differences. For example, the observed number of Black non-Hispanics who think people should register to vote is 28, and the expected value is nearly twice as high at 51.3.\n:::\n\n::::\n:::::\n\n###### Require\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-exprected-race-by-require}\n: Status of voting by race / ethnicity\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nct_require <- descr::CrossTable(\n        x = vote_opinions$race,\n        y = vote_opinions$require_vote,\n        dnn = c(\"Race\", \"Status of voting\"),\n        prop.r = FALSE, \n        prop.c = FALSE, \n        prop.t = FALSE,\n        prop.chisq = FALSE,\n        expected = TRUE\n    )\nct_require\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    Cell Contents \n#> |-------------------------|\n#> |                       N | \n#> |              Expected N | \n#> |-------------------------|\n#> \n#> ==============================================================\n#>                       Status of voting\n#> Race                  Require to vote   Choose to vote   Total\n#> --------------------------------------------------------------\n#> White non-Hispanic                 95              532     627\n#>                                 128.4            498.6        \n#> --------------------------------------------------------------\n#> Black non-Hispanic                 40               84     124\n#>                                  25.4             98.6        \n#> --------------------------------------------------------------\n#> Hispanic                           50               96     146\n#>                                  29.9            116.1        \n#> --------------------------------------------------------------\n#> Other                              13               57      70\n#>                                  14.3             55.7        \n#> --------------------------------------------------------------\n#> Total                             198              769     967\n#> ==============================================================\n```\n\n\n:::\n:::\n\n\n***\n\n::: {.callout-tip}\nThe cell \"Other\" has similar observed and expected values, but the rest have bigger differences.\n:::\n\n::::\n:::::\n\n###### Both\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-expected-voting-data}\n: Computing ease and require of voting using the {**sjstats**} package\n::::::\n:::\n::::{.my-r-code-container}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## load vote_clean ##########\nvote_clean <-  base::readRDS(\"data/chap05/vote_clean.rds\")\n\nvote_clean2 <- vote_clean |> \n    dplyr::select(race, ease_vote, require_vote) |> \n    tidyr::drop_na()\n\nease_vote_n <- vote_clean2 |> \n    dplyr::select(race, ease_vote) |> \n    dplyr::group_by(race, ease_vote) |> \n    dplyr::summarize(n_ease = dplyr::n(),\n                     .groups = \"keep\")\n\nease_expected  <-  \n    tibble::as_tibble(\n        base::as.data.frame(\n            sjstats::table_values(\n                base::table(\n                    vote_clean$race, \n                    vote_clean$ease_vote)\n                )$expected,\n                .name_repair = \"unique\")) |> \n    dplyr::arrange(Var1)\n\n(\n    ease_expected2 <- dplyr::bind_cols(\n    ease_vote_n,\n    exp_ease = ease_expected$Freq)\n)\n\nglue::glue(\" \")\nglue::glue(\"**********************************************************\")\nglue::glue(\" \")\n\nrequire_vote_n <- vote_clean2 |> \n    dplyr::select(race, require_vote) |> \n    dplyr::group_by(race, require_vote) |> \n    dplyr::summarize(n_require = dplyr::n(),\n                     .groups = \"keep\")\n\nrequire_expected  <-  \n    tibble::as_tibble(\n        base::as.data.frame(\n            sjstats::table_values(\n                base::table(\n                    vote_clean$race, \n                    vote_clean$require_vote)\n                )$expected,\n                .name_repair = \"unique\")) |> \n    dplyr::arrange(Var1)\n\n(\n    require_expected2 <- dplyr::bind_cols(\n    require_vote_n,\n    exp_require = require_expected$Freq)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 8 × 4\n#> # Groups:   race, ease_vote [8]\n#>   race               ease_vote         n_ease exp_ease\n#>   <fct>              <fct>              <int>    <dbl>\n#> 1 White non-Hispanic Register to vote     292      257\n#> 2 White non-Hispanic Make easy to vote    335      373\n#> 3 Black non-Hispanic Register to vote      27       51\n#> 4 Black non-Hispanic Make easy to vote     97       75\n#> 5 Hispanic           Register to vote      50       60\n#> 6 Hispanic           Make easy to vote     96       88\n#> 7 Other              Register to vote      25       30\n#> 8 Other              Make easy to vote     45       43\n#>  \n#> **********************************************************\n#>  \n#> # A tibble: 8 × 4\n#> # Groups:   race, require_vote [8]\n#>   race               require_vote    n_require exp_require\n#>   <fct>              <fct>               <int>       <dbl>\n#> 1 White non-Hispanic Require to vote        95         130\n#> 2 White non-Hispanic Choose to vote        532         509\n#> 3 Black non-Hispanic Require to vote        40          26\n#> 4 Black non-Hispanic Choose to vote         84         101\n#> 5 Hispanic           Require to vote        50          30\n#> 6 Hispanic           Choose to vote         96         117\n#> 7 Other              Require to vote        13          15\n#> 8 Other              Choose to vote         57          59\n```\n\n\n:::\n:::\n\n\n***\n\nThe `sjstats::table_values()` function has the advantage that it can be converted to a data.frame. We can therefore manipulate the data and --- for example --- combine expected data for different variables.\n\n\n::::\n:::::\n\n###### Together\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-expected-vote-data}\n: : Combining ease and require of voting\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire_expected3 <- require_expected2 |> \n    dplyr::ungroup() |> \n    dplyr::select(-1)\n\nvote_expected <- dplyr::bind_cols(\n    ease_expected2,\n    require_expected3\n)\n\nvote_expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 8 × 7\n#> # Groups:   race, ease_vote [8]\n#>   race              ease_vote n_ease exp_ease require_vote n_require exp_require\n#>   <fct>             <fct>      <int>    <dbl> <fct>            <int>       <dbl>\n#> 1 White non-Hispan… Register…    292      257 Require to …        95         130\n#> 2 White non-Hispan… Make eas…    335      373 Choose to v…       532         509\n#> 3 Black non-Hispan… Register…     27       51 Require to …        40          26\n#> 4 Black non-Hispan… Make eas…     97       75 Choose to v…        84         101\n#> 5 Hispanic          Register…     50       60 Require to …        50          30\n#> 6 Hispanic          Make eas…     96       88 Choose to v…        96         117\n#> 7 Other             Register…     25       30 Require to …        13          15\n#> 8 Other             Make eas…     45       43 Choose to v…        57          59\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n:::::{.my-important}\n:::{.my-important-header}\nDifferences between observed values and expected indicates that there may be a relationship between the variables. \n:::\n:::::\n\n### Assumptions of the chi-squared test of independence\n\n***\n\n::: {#bul-chap05-assumptions-chi-squared}\n- **The variables must be nominal or ordinal (usually nominal)**. We have categorical data with no order, e.g., nominal data: *The assumption is met.*\n- **The expected values should be 5 or higher in at least 80% of groups**. We have 8 cells with values. None of these cells are 5 or lower: *The assumption is met.*\n- **The observations must be independent**. We have neither the same set of people asked before and after an intervention nor do are the respondents family members or other affiliated with each other: *The assumption is met. *\n\nAssumptions for the chi-squared test\n\n:::\n\n\n## Calculating the chi-squared statistic {#sec-chap05-achievement3}\n\nThe differences between observed values and expected values can be combined into an overall statistic. But adding (resp. subtracting) does not work as the result is always 0. So we will again --- like with the computation of the variance --- square the difference.\n\nTo prevent huge differences when observed and expected values are very large, there is an additional step in the computation of $\\chi^2$: Divide the squared differences by the expected value of the appropriate cells.\n\n$$\n\\chi^2 = \\sum\\frac{(observed - expected)^2}{expected}\n$$ {#eq-chap05-chi-squared}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-chi-squared-ease-voting}\n: Compute chi-squared for race by ease of voting\n::::::\n:::\n::::{.my-r-code-container}\n\n:::{#lst-chap05-chi-squared-ease-voting}\n\n::: {.cell}\n\n```{.r .cell-code}\nvote_clean <- base::readRDS(\"data/chap05/vote_clean.rds\")\n\nstats::chisq.test(\n    x = vote_clean$ease_vote,\n    y = vote_clean$race\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tPearson's Chi-squared test\n#> \n#> data:  vote_clean$ease_vote and vote_clean$race\n#> X-squared = 28.952, df = 3, p-value = 2.293e-06\n```\n\n\n:::\n:::\n\n\nChi-squared statistic for race by ease of voting\n:::\n::::\n:::::\n\n## Achievement 4: Interpreting the chi-squared statistic {#sec-chap05-achievement4}\n\nIn contrast to the binomial and normal distribution which both have two parameters (n and p, resp. $\\mu$ and $\\sigma$), the <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a> distribution has only one <a class='glossary' title='Unobserved variables are usually called Parameters. (SR2, Chap.2) A parameter is an unknown numerical characteristics of a population that must be estimated. (CDS). They are also numbers that govern statistical models (stats.stackexchange). A parameter is also a number that is a defining characteristic of some population or a feature of a population. (SwR, Glossary)'>parameter</a>: the <a class='glossary' title='Degree of Freedom (df) is the number of pieces of information that are allowed to vary in computing a statistic before the remaining pieces of information are known; degrees of freedom are often used as parameters for distributions (e.g., chi-squared, F). (SwR, Glossary)'>degrees of freedom</a>. The `df` can be used to find the population <a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a> for the distribution:\n\n$$\n\\sqrt{2df}\n$$ {#eq-chap05-pop-sd-df}\n\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap05-chi-squared-dist}\n: Chi-square probability distributions with different degrees of freedom\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### 4 $\\chi^2$ dist extra\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-chi-squared-separately}\n: Four chi-square probability distributions with different degrees of freedom\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define sequence of x-values\ntib <- tibble::tibble(x = seq(0, 30, length.out = 600))\n\ntib <- tib |> \n# Compute density values\n    dplyr::mutate(\n        y1 = stats::dchisq(x, df = 1),\n        y3 = stats::dchisq(x, df = 3),\n        y5 = stats::dchisq(x, df = 5),\n        y7 = stats::dchisq(x, df = 7)\n    )  \nchi_sq1 <- tib |> \n# Plot the Chi-square distribution: df = 1\n    ggplot2::ggplot(ggplot2::aes(x = x, y = y1)) +\n    ggplot2::geom_line(color = \"blue\") +\n    ggplot2::labs(x = \"x\", y = \"Density\", \n      title = paste(\"Chi-square with 1 degree of freedom\")) \n\nchi_sq3 <- tib |> \n# Plot the Chi-square distribution: df = 3\n    ggplot2::ggplot(ggplot2::aes(x = x, y = y3)) +\n    ggplot2::geom_line(color = \"blue\") +\n    ggplot2::labs(x = \"x\", y = \"Density\", \n      title = paste(\"Chi-square with 3 degrees of freedom\"))\n\nchi_sq5 <- tib |> \n# Plot the Chi-square distribution: df = 5\n    ggplot2::ggplot(ggplot2::aes(x = x, y = y5)) +\n    ggplot2::geom_line(color = \"blue\") +\n    ggplot2::labs(x = \"x\", y = \"Density\", \n      title = paste(\"Chi-square with 5 degrees of freedom\"))\n\nchi_sq7 <- tib |> \n# Plot the Chi-square distribution: df = 7\n    ggplot2::ggplot(ggplot2::aes(x = x, y = y7)) +\n    ggplot2::geom_line(color = \"blue\") +\n    ggplot2::labs(x = \"x\", y = \"Density\", \n      title = paste(\"Chi-square with 7 degrees of freedom\"))\n\ngridExtra::grid.arrange(chi_sq1, chi_sq3, chi_sq5, chi_sq7, ncol = 2)\n```\n\n::: {.cell-output-display}\n![Chi-square probability distributions with different degrees of freedom](05-chi-squared_files/figure-html/fig-chi-squared-dist-1.png){#fig-chi-squared-dist width=672}\n:::\n:::\n\n***\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! The graphs have different y scales!\n:::\n::::{.my-watch-out-container}\nThis is the replication of Figure 5.7 from the book.\n\nNote: The first impression --- that all probability distributions have same height --- is wrong! All four graphs have very different density scales! \n\nWe will see that all four distributions overlaid into one graphic will give a different impression.\n::::\n:::::\n\n\n\n\n::::\n:::::\n\n\n###### 4 $\\chi^2$ dist together\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-chi-squared-dist-together}\n: Four chi-square probability distributions with different degrees of freedom in one graph\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define sequence of x-values\ntib_chisq <- tibble::tibble(x = seq(0, 30, length.out = 600))\n\ntib_chisq |> \n# Compute density values\n    dplyr::mutate(\n        y1 = stats::dchisq(x, df = 1),\n        y3 = stats::dchisq(x, df = 3),\n        y5 = stats::dchisq(x, df = 5),\n        y7 = stats::dchisq(x, df = 7)\n    ) |> \n    tidyr::pivot_longer(-1) |>  \n    \n    ggplot2::ggplot(\n        ggplot2::aes(x, value, color = name)) + \n    ggplot2::geom_line(linewidth = 1) +\n    ggplot2::ylim(0, .3) +\n    ggplot2::labs(y = \"Density\") +\n    ggplot2::scale_color_viridis_d(\n        name = \"Degrees\\nof Freedom\",\n        labels = c(\"1\", \"3\", \"5\", \"7\"),\n        option = \"plasma\",\n        end = .8\n    )\n```\n\n::: {.cell-output-display}\n![Four chi-square probability distributions with different degrees of freedom](05-chi-squared_files/figure-html/fig-chi-squared-dist-together-1.png){#fig-chi-squared-dist-together width=672}\n:::\n:::\n\n\n::::\n:::::\n\n###### $\\chi^2$ as function\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-chi-squared-function}\n: Four chi-square probability distributions as ggplot function\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot() +\n    ggplot2::xlim(0, 30) +\n    ggplot2::stat_function(\n        fun = dchisq,\n        args = list(df = 3)\n    )\n```\n\n::: {.cell-output-display}\n![Chi-square probability distributions with 3 degrees of freedom](05-chi-squared_files/figure-html/fig-chi-squared-dist-function-1.png){#fig-chi-squared-dist-function width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-compute-df}\n: Compute degrees of freedom (df) and population standard deviation of a chi-squared distribution\n::::::\n:::\n::::{.my-procedure-container}\n\n1. Subtract 1 from the number of each categories used for the test.\n2. Multiply the resulting numbers together gives the degrees of freedom (df)\n3. The square root of twice times df is the population standard deviation $\\sqrt{(2 \\times df)}$.\n::::\n:::::     \n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-comp-df}\n: Compute degrees of freedom (`df`) and population standard deviation for the chi-squared distribution of `race` by `ease_vote`\n::::::\n:::\n::::{.my-example-container}\n\nI am following @prp-compute-df:\n\n1. Subtract 1 from the number of each categories used for the test.\n\n- We have four categories in `race`: White non-Hispanic, Black non-Hispanic, Hispanic, Other. $4 - 1 = 3$.\n- We have 2 categories in `ease_vote`: Register to vote and Make easy to vote. $2 - 1 = 1$.\n\n2. Multiply the resulting numbers together gives the degrees of freedom (df): $3 \\times 1 = 3$\n\n3. The population standard deviation is $\\sqrt{(2 \\times df)}$ = $\\sqrt{(2 \\times 3)}$ = 2.449.\n::::\n:::::\n\nThe <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a> distribution shown, which is the chi-squared <a class='glossary' title='A probability density function (PDF) tells us the probability that a random variable takes on a certain value. (Statology) The probability density function (PDF) for a given value of random variable X represents the density of probability (probability per unit random variable) within a particular range of that random variable X. Probability densities can take values larger than 1. (StackExchange Mathematics) We can use a continuous probability distribution to calculate the probability that a random variable lies within an interval of possible values. To do this, we use the continuous analogue of a sum, an integral. However, we recognise that calculating an integral is equivalent to calculating the area under a probability density curve. We use p(value) for probability densities and Pr for probabilities. (Bayesian Statistics, Chap.3)'>probability density function</a> (PDF), **shows the probability of a value of chi-squared occurring when there is no relationship** between the two variables contributing to the chi-squared.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap05-chi-squared-pdf}\n: Determine the probability using the chi-squared distribution \n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### test example1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-pdf-test-example1}\n: Chi-squared probability distribution (df = 5)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Define start of shade\nx_shade = 10 \ny_shade = stats::dchisq(10, 5)\n\n\n\n## Define sequence of x-values\ntib <- tibble::tibble(x = seq(0, 30, length.out = 600)) |> \n    # Compute density values\n    dplyr::mutate(\n        y = stats::dchisq(x, df = 5)\n    )\n\n## Subset data for shaded area\nshade_10 <- tib |> \n    dplyr::filter(x >= x_shade) |> \n    ## Necessary as starting point for y = 0!\n    tibble::add_row(x = 10, y = 0, .before = 1)\n\n\ntib |> \n    ## Plot the Chi-square distribution: df = 5\n    ggplot2::ggplot(ggplot2::aes(x = x, y = y)) +\n    ggplot2::geom_line() +\n    \n    ## Draw segment \n    ggplot2::geom_segment(\n        x = x_shade,\n        y = 0,\n        xend = x_shade,\n        yend = y_shade\n    ) +\n    \n    ## Shade curve\n    ggplot2::geom_polygon(\n        data = shade_10, \n        fill = \"lightblue\",\n        ggplot2::aes(x = x, y = y)\n        ) +\n    ggplot2::labs(x = \"x\", y = \"Density\", \n      title = paste(\"Chi-square with 5 degree of freedom and shaded area starting with x = 10.0\"))\n```\n\n::: {.cell-output-display}\n![Chi-squared probability distribution (df = 5)](05-chi-squared_files/figure-html/fig-pdf-test-example1-1.png){#fig-pdf-test-example1 width=672}\n:::\n:::\n\n***\n\nThe probability that the differences between observed and expected values would result in a chi-squared of exactly 10 is --- looking into the data --- around 2.8%, e.g., very small. \n\nIt is more useful to know what the probability is of getting a chi-squared of *10 or higher*. The probability of the chi-squared value being 10 or higher would be the area under the curve from 10 to the end of the distribution at the far right. \n\nThe probability of the chi-squared value being 10 or higher is about 15%. Even if this value is not very probable it is way above to be statistically significant (5%). The probability of the squared differences between observed and expected adding up to 10 or more is low when there is no relationship between the variables and result in a chi-squared value well inside the probability density function (PDF).\n\nFor instance: In our test case the $\\chi^2$-value of 10.0 lies well inside the probability curve. The probability that this value can occur when there is no statistically relevant relationship is relatively high (15%). We can't therefore reject the H0, because we do not have a statistically significant value of 5% or less. This can be seen clearly in the resulting graph of @lst-pdf-test-example-2, created with the {**sjPlot**} package (see @pak-sjPlot).\n\n\n\n\n::::\n:::::\n\n###### test example2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-pdf-test-example2}\n: Chi-squared probability distribution (df = 5)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#lst-pdf-test-example-2}\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::dist_chisq(chi2 = 10, deg.f = 5)\n```\n\n::: {.cell-output-display}\n![Chi-squared probability distribution (df = 5)](05-chi-squared_files/figure-html/fig-pdf-test-example2-1.png){#fig-pdf-test-example2 width=672}\n:::\n:::\n\n\nChi-squared probability distribution (df = 5) created with {**sjPlot**}\n:::\n***\n\nThis graph uses the {**sjPlot**) package and is very easy to produce. It shows that the p-value for x = 10 is 0.08 (8%), e.g., higher as the standard value of 0.05 (5%). To be statistically significant, the $\\chi^2$ value would need to be equal or higher than 11.07.\n\n\n::::\n:::::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\n{**sjPlot**}: Great package and easy to use in default mode, but you need time to learn the many configurations\n:::\n::::{.my-watch-out-container}\nEven if the standard version of the plot is easy to create, to adapt the graph is another issue. In the background {**sjPlot**} uses the {**ggplot2**} package, but you can’t specify changes by mixing (**sjPlot**) with {**ggplot2**} commands. I tried it and it produced two different plots. To customize plot appearance you have to learn the many arguments of of `sjPlot:set_theme()` and `sjPlot::plot_grpfrq()`. (See also @pak-sjPlot)\n\n(I managed to change the theme in {**sjPlot**} by setting the default theme in {**ggplot2**} with `ggplot2::theme_set(ggplot2::theme:bw())` as global option in the setup chunk.)\n::::\n:::::\n\n\n\n###### Race & voting\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-chisq-voting}\n: Determine probability of ease of voting by race\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvote_clean <- base::readRDS(\"data/chap05/vote_clean.rds\")\n\n(\n    chisq_ease_vote_stats <- stats::chisq.test(ease_vote_table)\n)\n\nbase::invisible(\n    chisq_sjplot <- sjPlot::dist_chisq(\n        chi2 = chisq_ease_vote_stats[[\"statistic\"]][[\"X-squared\"]],\n        deg.f = chisq_ease_vote_stats[[\"parameter\"]][[\"df\"]]\n        )\n)\n```\n\n::: {.cell-output-display}\n![Chi-squared probability distribution of `ease_vote` by `race`](05-chi-squared_files/figure-html/chap05-chisq-voting-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tPearson's Chi-squared test\n#> \n#> data:  ease_vote_table\n#> X-squared = 28.952, df = 3, p-value = 2.293e-06\n```\n\n\n:::\n:::\n\n***\n\n- The limit $\\chi^2$ where a statistically significant p-value < 0.05 would start is with 7.8147279 much lower. The label of $\\chi^2$ = 7.81 therefore is not the actual chi-squared value (which is 28.95), but it is the chi-squared value where the <a class='glossary' title='The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary)'>p-value</a> is .05. From here on we will get with bigger chi-squared values even smaller statistically significant p-values until we finally reach at $\\chi^2$ = 28.95 a p-value of 2.2925504\\times 10^{-6}.\n- **p-value**: The p-value 2.2925504\\times 10^{-6} is far below the statistically significant level of 0.05. \n- **$\\chi^2$**: The shaded area equal or greater than 28.95 is so small that you can’t see it.\n\n::: {.callout-tip}\nThere is a statistically significant association between views on voting ease and race-ethnicity [$\\chi^2(3) = 28.95; p < .05$].\n:::\n\n\n\n\n\n\n\n\n\n\n::::\n:::::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWhenever possible, use the actual p-value rather than p < .05\n:::\n::::{.my-watch-out-container}\nIn this case the <a class='glossary' title='The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary)'>p-value</a> is so small that it wouldn’t look nice to provide the exact figure.\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n\n## Achievement 5: Null Hypothesis Significance Testing {#sec-chap05-achievement5}\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-chap05-nhst}\n: Null Hypothesis Significance Testing\n::::::\n:::\n::::{.my-procedure-container}\n1. Write the null and alternate hypotheses. \n2. Compute the test statistic. \n3. Calculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true). \n4a. If the probability that the null is true is very small, usually less than 5%, reject the null hypothesis. \n4b. If the probability that the null is true is not small, usually 5% or greater, retain the null hypothesis.\n::::\n:::::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Last step has to alternate options\n:::\n::::{.my-watch-out-container}\nIn the book the above @prp-chap05-nhst has 5 options. But the last two steps (4 and 5) are contradictory alternatives. If one is true, the other does not apply. My @prp-chap05-nhst has therefore only 4 steps.\n::::\n:::::\n\n\n\n### NHST Step 1\n\nThe null (<a class='glossary' title='The null hypothesis (H0, or simply the Null) is a statement of no difference or no association that is used to guide statistical inference testing (SwR, Glossary)'>H0</a>) and alternate (<a class='glossary' title='An alternate hypothesis (HA or sometimes written as H1) is a claim that there is a difference or relationship among things; the alternate hypothesis is paired with the null hypothesis that typcially states there is no relationship or no difference between things. (SwR, Glossary)'>HA</a>) are written about the population and are tested using a sample from the population.\n\n::: {.callout-note}\n- **H0**: People’s opinions on voter registration are the same across raceethnicity groups. \n- **HA**: People’s opinions on voter registration are not the same across race-ethnicity groups.\n:::\n\n### NHST Step 2\n\nThe second step is to use the test statistic. When examining a relationship between two categorical variables the appropriate test statistic is the <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a> statistic, $\\chi^2$. You can see in the last line of @lst-chap05-chi-squared-ease-voting that $\\chi^2 = 28.952$.\n\n### NHST Step 3\n\nThe probability of seeing a chi-squared as big as 28.952 in our sample if there were no relationship in the population between opinion on voting ease and race-ethnicity group would be 0.000002293 or p < .05.\n\n### NHST Step 4\n\nThe probability that the null hypothesis, “People’s opinions on voter registration are the same across race-ethnicity groups,” is true in the population based on what we see in the sample is 0.000002293 or p < .05. This is a very small probability of being true and indicates that the null hypothesis is not likely to be true and should therefore be *rejected.*\n\n::: {.callout-tip}\nWe used the chi-squared test to test the null hypothesis that there was no relationship between opinions on voter registration and race/ethnicity group. We rejected the null hypothesis and concluded that there was a statistically significant association between views on voter registration and race-ethnicity [$\\chi^2(3) = 28.952; p < .05$].\n:::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Chi-squared test and chi-squared goodness-of-fit test are not the same!\n:::\n::::{.my-watch-out-container}\nThe chi-squared goodness-of-fit test is used for comparing the values of a single categorical variable to values from a hypothesized or <a class='glossary' title='A population consists statistically of all the observations that fit some criterion; for example, all of the people currently living in the country of Bhutan or all of the people in the world currently eating strawberry ice cream. (SwR, Glossary)'>population</a> variable. The goodness-of-fit test is often used when trying to determine if a <a class='glossary' title='Samples are subsets of observations from some population that is often analyzed to learn about the population sampled. (SwR, Glossary)'>samples</a> are a good representation of the population.\n\n\n::::\n:::::\n\n## Achievement 6: Standardized residuals {#sec-chap05-achievement6}\n\n### Introduction\n\nOne limitation of the chi-squared independence test is that it determines whether or not there is a statistically significant relationship between two categorical variables but does not identify what makes the relationship significant. The name for this type of test is <a class='glossary' title='An omnibus is a statistical test that identifies that there is some relationship going on between two categorical variables, but not what that relationship is. (SwR, Glossary)'>omnibus</a>.\n\n<a class='glossary' title='Standardized residuals are the standardized differences between observed and expected values in a chi-squared analysis; a large standardized residual indicates that the observed and expected values were very different. (SwR, Glossary)'>Standardized residuals</a> (like <a class='glossary' title='A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (StatisticsHowTo)'>z-scores</a> can aid analysts in determining which of the observed frequencies are significantly larger or smaller than expected. The standardized residual is computed by subtracting the expected value in a cell from the observed value in a cell and dividing by the square root of the expected value.\n\n$$\n\\text{Standardized residual} = \\frac{observed - expected}{\\sqrt{expected}}\n$$ {#eq-chap05-standardized-residuals}\n\nThe standardized residual is distributed like a z-score. Values of the standardized residuals that are higher than 1.96 or lower than –1.96 indicate that the observed value in that group is much higher or lower than the expected value. These are the groups that are contributing the most to a large chi-squared statistic and could be examined further and included in the interpretation.\n\n::: {#not-adjusted-standardized-residuals}\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Adjusted Standardized Residuals\n:::\n::::{.my-watch-out-container}\nThere are also *adjusted* standardized residuals. To increase the confusion Alan Agresti [-@agresti2018a] calls these residuals \"Standardized Pearson Residual\". To understand the difference between standardized and *adjusted* standardized residuals read see [Standardized Residuals in Statistics: What are They?](https://www.statisticshowto.com/what-is-a-standardized-residuals/) [@glenn.d]. Adjusted standardized residuals have higher values and are therefore not interpretable with the z-score values (e.g., looking for values greater or smaller than 2, res. 1.96 standard deviations). I will therefore stick with the (normal) standardized residuals.\n\n$$\n\\begin{align*}\n& \\text{Adjusted residual} =  \\\\\n& \\frac{observed - expected}{\\sqrt{expected \\times (1-\\text{row total proportion}) \\times (1-\\text{col total proportion}) )}}\n\\end{align*}\n$$ {#eq-chap05-adjusted-standardized-residuals}\n\nWhat are Adjusted Standardized Residuals?\n:::\n::::\n:::::\n\n\n\n\nThe book recommends to get the standardized residuals with `Descr::CrossTable()`. But I have checked out that there are other possibilities as well. \n\n\n:::::{.my-resource}\n:::{.my-resource-header}\nPackages with functions to get standardized residuals of chi-squared tests\n:::\n::::{.my-resource-container}\nThe following list collects these resources I have found together with the approximate average download data of the appropriate package. This figures will give you an idea about package use, but will not say anything about the quality of the package or the standardized residual function we are looking for.\n\n- {**stats**}: `stats::chisq.test()$residuals`\n- {**descr**}: `descr::CrossTable()`: It has arguments to show residuals, standardized residuals and adjusted standardized residuals \n- {**janitor**}: `janitor::chisq.test(<tabyl>)$residuals`\n- {**questionr**}`questionr::chisq.residuals()` \n- {**rstatix**}: `rstatix::pearson_residuals()`, `rstatix::std_residuals()` \n\nThere is also the possibility to use `graphics::mosaicplot()` with the option `shade = TRUE` to examine residuals visually for the source of differences (See [@greenwood2022]).\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-pkgs-chisq-residuals}\n: Number of daily downloads for packages with functions to display chi-squared residuals\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell tbl-cap='Daily donwloads of packages with functions to display chi-squared residuals'}\n\n```{.r .cell-code}\npkgs = c(\"janitor\", \"questionr\", \"rstatix\", \"descr\")\npkgs_dl(pkgs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 4 × 2\n#>   package       n\n#>   <chr>     <dbl>\n#> 1 rstatix    5560\n#> 2 janitor    3223\n#> 3 questionr   695\n#> 4 descr       416\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n::::\n:::::\n\n### Computation\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap05-compute-standardized-residuals}\n: Compute standardized residuals with functions of different packages\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### `desc::CrossTable()`\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-standardized-residuals-desc}\n: Compute standardized residuals with `descr::CrossTable()`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## load vote_clean ##########\nvote_clean <-  base::readRDS(\"data/chap05/vote_clean.rds\")\n\ndescr::CrossTable(\n    x = ease_vote_table,\n    expected = TRUE,\n    prop.r = FALSE,\n    prop.c = FALSE,\n    prop.t = FALSE,\n    prop.chisq = FALSE,\n    chisq = TRUE,\n    resid = TRUE,\n    sresid = TRUE,\n    asresid = TRUE \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    Cell Contents \n#> |-------------------------|\n#> |                       N | \n#> |              Expected N | \n#> |                Residual | \n#> |            Std Residual | \n#> |           Adj Std Resid | \n#> |-------------------------|\n#> \n#> ==================================================================\n#>                       Ease of voting\n#> Race                  Register to vote   Make easy to vote   Total\n#> ------------------------------------------------------------------\n#> White non-Hispanic                 292                 338     630\n#>                                  256.6               373.4        \n#>                                 35.357             -35.357        \n#>                                  2.207              -1.830        \n#>                                  4.811              -4.811        \n#> ------------------------------------------------------------------\n#> Black non-Hispanic                  28                  98     126\n#>                                   51.3                74.7        \n#>                                -23.329              23.329        \n#>                                 -3.256               2.700        \n#>                                 -4.532               4.532        \n#> ------------------------------------------------------------------\n#> Hispanic                            51                  97     148\n#>                                   60.3                87.7        \n#>                                 -9.291               9.291        \n#>                                 -1.197               0.992        \n#>                                 -1.687               1.687        \n#> ------------------------------------------------------------------\n#> Other                               27                  46      73\n#>                                   29.7                43.3        \n#>                                 -2.738               2.738        \n#>                                 -0.502               0.416        \n#>                                 -0.678               0.678        \n#> ------------------------------------------------------------------\n#> Total                              398                 579     977\n#> ==================================================================\n#> \n#> Statistics for All Table Factors\n#> \n#> Pearson's Chi-squared test \n#> ------------------------------------------------------------\n#> Chi^2 = 28.95154      d.f. = 3      p = 2.29e-06\n```\n\n\n:::\n:::\n\n\n***\n\nHere I have displayed the first and only time also the adjusted standardized residuals. As you can see they are much higher and do not obey the z-score distribution. I do not want how to interpret them. As far as I a understood they are only used for some software packages as e.g., [SDA](https://sda.berkeley.edu/) to highlight outstanding values. (See @not-adjusted-standardized-residuals)\n\n***\n\n::: {#bul-prepare-interpretation-standardized-residuals}\n\n- From the very small p-value (which is almost 0) we see that we have a statistically relevant association between opinions about opinions for ease of voting and race / ethnicity. \n- The biggest part for rejecting the null hypotheses, that there is not association has the group of black non-Hispanic. A much bigger proportion as we would have expected of black non-Hispanic support ease of voting and are against registration for voting.\n- Another trend that goes in the reverse direction concerns the white non-Hispanic group. This group endorse that people should register for voting with a higher proportion as expected.\n\nWhat does the standardized residuals tell us?\n:::\n\n::::\n:::::\n\n::: {.callout-tip}\nWe used the chi-squared test to test the null hypothesis that there was no relationship between opinions on voter registration by race/ethnicity group. We rejected the null hypothesis and concluded that there was a statistically significant association between views on voter registration and race-ethnicity [$\\chi^2(3) = 28.95; p < .05$]. Based on standardized residuals, the statistically significant chi-squared test result was driven by more White non-Hispanic participants and fewer Black non-Hispanic participants than expected believe that people should prove they want to vote by registering, and more Black non-Hispanic participants than expected believe that the voting process should be made easier.\n:::\n\n\n###### `stats::chisq.test()`\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-standardized-residuals-stats}\n: Compute standardized residuals with `stats::chisq.test()`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::chisq.test(ease_vote_table)$residuals\n\ngraphics::mosaicplot(\n    x = ease_vote_table,\n    shade = TRUE,\n    main = \"Ease of voting by race / ethnicity\"\n)\n```\n\n::: {.cell-output-display}\n![](05-chi-squared_files/figure-html/standardized-residuals-stats-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                     Ease of voting\n#> Race                 Register to vote Make easy to vote\n#>   White non-Hispanic        2.2070569        -1.8298512\n#>   Black non-Hispanic       -3.2561796         2.6996695\n#>   Hispanic                 -1.1965274         0.9920302\n#>   Other                    -0.5020807         0.4162707\n```\n\n\n:::\n:::\n\n\n***\n\nI think that this result using base R tools is easier to understand and interpret as the presentation provided by `descr::CrossTable()`.  Especially the graph highlights the important differences. Solid lines represent values higher whereas dashed lines point to proportion that are smaller than expected. And the color scale gives you immediate feedback about the size of difference.\n\n::::\n:::::\n\n###### `janitor::chis.test()`\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-standardized-residuals-janitor}\n: Compute standardized residuals with `janitor::chisq.test()`\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\njanitor::chisq.test(ease_vote_table)$residuals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                     Ease of voting\n#> Race                 Register to vote Make easy to vote\n#>   White non-Hispanic        2.2070569        -1.8298512\n#>   Black non-Hispanic       -3.2561796         2.6996695\n#>   Hispanic                 -1.1965274         0.9920302\n#>   Other                    -0.5020807         0.4162707\n```\n\n\n:::\n:::\n\n***\n\nExactly the same result as with `stats::chisq.test()`.\n::::\n:::::\n\n###### `questionr::chis.test()`\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-standardized-residuals-questionr}\n: Compute standardized residuals with `questionr::chisq.residuals()`\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nquestionr::chisq.residuals(ease_vote_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                     Ease of voting\n#> Race                 Register to vote Make easy to vote\n#>   White non-Hispanic             2.21             -1.83\n#>   Black non-Hispanic            -3.26              2.70\n#>   Hispanic                      -1.20              0.99\n#>   Other                         -0.50              0.42\n```\n\n\n:::\n:::\n\n***\n\nThe only difference of this result is that the values are rounded. This is nice because for the interpretation we do not need the detailed values.\n\n\n::::\n:::::\n\n###### `rstatix::chisq_test()`\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-standardized-residuals-rstatix}\n: Compute standardized residuals with `rstatix::chisq_.residuals_test()`\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n(chisq_ease_vote_rstatix <- rstatix::chisq_test(ease_vote_table))\n\nrstatix::chisq_descriptives(chisq_ease_vote_rstatix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 6\n#>       n statistic          p    df method          p.signif\n#> * <int>     <dbl>      <dbl> <int> <chr>           <chr>   \n#> 1   977      29.0 0.00000229     3 Chi-square test ****    \n#> # A tibble: 8 × 9\n#>   Race          Ease.of.voting observed   prop row.prop col.prop expected  resid\n#>   <fct>         <fct>             <int>  <dbl>    <dbl>    <dbl>    <dbl>  <dbl>\n#> 1 White non-Hi… Register to v…      292 0.299     0.463   0.734     257.   2.21 \n#> 2 Black non-Hi… Register to v…       28 0.0287    0.222   0.0704     51.3 -3.26 \n#> 3 Hispanic      Register to v…       51 0.0522    0.345   0.128      60.3 -1.20 \n#> 4 Other         Register to v…       27 0.0276    0.370   0.0678     29.7 -0.502\n#> 5 White non-Hi… Make easy to …      338 0.346     0.537   0.584     373.  -1.83 \n#> 6 Black non-Hi… Make easy to …       98 0.100     0.778   0.169      74.7  2.70 \n#> 7 Hispanic      Make easy to …       97 0.0993    0.655   0.168      87.7  0.992\n#> 8 Other         Make easy to …       46 0.0471    0.630   0.0794     43.3  0.416\n#> # ℹ 1 more variable: std.resid <dbl>\n```\n\n\n:::\n:::\n\n***\n\nThe result with {**rstatix**} is very detailed. Using {**rstatix**} has the additioonal advantage that it is {**tidyverse**} compatible and you can use the pipe. The package includes many different tests and has with 6877 downloads from the RStudio CRAN mirror in one day (2024-03-14) a pretty big user group.\n\n\n::::\n:::::\n\n\n\n:::\n\n::::\n:::::\n\n:::::{.my-important}\n:::{.my-important-header}\nWhich package should I use to show standardized residuals?\n:::\n::::{.my-important-container}\n3. `descr::CrossTable()` is used in the book, but I can't recommend it. The result cannot be transformed into a data.frame or tibble and it is therefore neither {**tidyverse**} compatible nor can you use the pipe.\n\n2. A good solution is the combination of `stats::chisq.test()` and `graphics::mosaicplot()`. Especially the mosaic plot helps to figure out quickly which cells are important.\n\n1. The best solution in my opinion is {**rstatix**}: Its results can be very detailed. {**rstatix**} is {**tidyverse**} compatible and you can use the pipe. The result with {**rstatix**} is very detailed. Using {**rstatix**} has the additional advantage that it is {**tidyverse**} compatible and you can use the pipe. The package includes [many different tests](https://rpkgs.datanovia.com/rstatix/) and has with 6877 downloads from the RStudio CRAN mirror in one day (2024-03-14) a pretty big user group.) and can therefore used or other tasks as well. With 6877 downloads from the RStudio CRAN mirror in one day (2024-03-14) it has a pretty big user group.\n\n**Because of the wide range of tests and the bi user basis I will apply {**rstatix**} as the predominant alternative  whenever the result is the same with other packages.**\n::::\n:::::\n\n\n## Achievement 7: Effect sizes {#sec-chap05-achievement7}\n\n### ICramér’s V \n\nConcerning out data about opinions about ease of voting we have two established two facts:\n\n1. There is an association between ease of voting opinions and race /ethnicity.\n2. This relation is driven mainly by black non-Hispanic preferring to a higher degree ease of voting and --- to a lesser degree -- white non-Hispanic supporting in a higher proportion than expected that people need to register for voting.\n\nBut we do not know the strength of this relationships. The strength of a relationship in statistics is referred to as <a class='glossary' title='Effect size is a measure of the strength of a relationship; effect sizes are important in inferential statistics in order to determine and communicate whether a statistically significant result has practical importance. (SwR, Glossary)'>effect size</a>. For <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a>, there are a few options, including the commonly used effect size statistic of <a class='glossary' title='Cramér’s V is an effect size to determine the strength of the relationship between two categorical variables; often reported with the results of a chi-squared. (SwR, Glossary)'>Cramér’s V</a>.\n\n***\n\n$$\nV = \\sqrt{\\frac{\\chi^2}{n(k-1)}}\n$$ {#eq-chap05-cramers-v-formula}\n\n- $\\chi^2$: The chi-squared is the test statistic for the analysis.\n- $n$: The sample size.\n- $k$: The number of categories in the variable with the *fewest* categories.\n\n***\n\n$$\nV = \\sqrt{\\frac{29.852}{977(2-1)}} = 0.17\n$$ {#eq-chap05-cramers-v-example-calculation}\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-chap05-cramers-v}\n: Interpretation of Cramér’s V\n::::::\n:::\n::::{.my-assessment-container}\n<a class='glossary' title='Cramér’s V is an effect size to determine the strength of the relationship between two categorical variables; often reported with the results of a chi-squared. (SwR, Glossary)'>Cramér’s V</a> is a measure of the strength of association between two nominal variables. It ranges from 0 to 1 where:\n\n- Small or weak effect size for V = .1 \n- Medium or moderate effect size for V = .3 \n- Large or strong effect size for V = .5\n\nMore detailed interpretation based on the degrees of freedom in [How to Interpret Cramér’s V (with Examples)](https://www.statology.org/interpret-cramers-v/) [@bobbittn.d].\n\n| Degrees of freedom | Small | Medium | Large |\n|--------------------|-------|--------|-------|\n| 1                  | 0.10  | 0.30   | 0.50  |\n| 2                  | 0.07  | 0.21   | 0.35  |\n| 3                  | 0.06  | 0.17   | 0.29  |\n| 4                  | 0.05  | 0.15   | 0.25  |\n| 5                  | 0.04  | 0.13   | 0.22  |\n\n: How to interpret Cramér’s V?  {#tbl-cramers-v} {.striped .hover}\n\n::::\n:::::\n\n:::::{.my-resource}\n:::{.my-resource-header}\nNumber of daily downloads for packages with functions to compute Cramér’s V\n:::\n::::{.my-resource-container}\n\n- {**lsr**}: `lsr::cramersV()`\n- {**rcompanion**}: `rcompanion::cramerV()`\n- {**DescTools**}: `DescTools::CramerV()`\n- {**sjstats**}: `sjstats::cramer()`\n- {**rstatix**}: `rstatix::cramer_v()`\n- {**collinear**}: `collinear::cramber_v()`\n- {**confintr**}: `confintr::cramersv()`\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-pkgs-cramers-v}\n: Number of daily downloads for packages with functions to compute Cramèr’s V\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell tbl-cap='Daily donwloads of packages with functions to to compute Cramèr’s V'}\n\n```{.r .cell-code}\npkgs = c(\"lsr\", \"rcompanion\", \"rstatix\", \"DescTools\",\n         \"confintr\", \"sjstats\", \"collinear\")\npkgs_dl(pkgs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 7 × 2\n#>   package        n\n#>   <chr>      <dbl>\n#> 1 rstatix     5560\n#> 2 DescTools   2282\n#> 3 sjstats      891\n#> 4 rcompanion   872\n#> 5 lsr          555\n#> 6 confintr     341\n#> 7 collinear      9\n```\n\n\n:::\n:::\n\nI have checked only {**lsr**} and {**rstatix**} as I was happy with the result of the {**rstatix**} package.\n::::\n:::::\n\n\n::::\n:::::\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap05-computing-cramers-v}\n: Computing Cramér’s V\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### `lsr::cramersV()`\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-computing-cramers-v-lsr}\n: Computing Cramér’s V with {**lsr**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlsr::cramersV(ease_vote_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.1721427\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n###### `rstatix::cramer_v()`\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-code-name-b}\n: Computing Cramér’s V with {**rstatix**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrstatix::cramer_v(ease_vote_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.1721427\n```\n\n\n:::\n:::\n\n\n***\n\nThe more conservative interpretation from the book sees the effect size between small and medium, corresponding to a relationship between weak to moderate  Including the degrees of freedom we get the starting point for a moderate relationship. I will use the more conservative interpretation.\n\n::: {.callout-tip}\nThere is a statistically significant relationship between opinions on voter registration and race-ethnicity, and the relationship is weak to moderate. This is consistent with the frequencies, which are different from expected, but not by an enormous amount in most of the groups.\n:::\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n\n### Yates continuity correction\n\nWhen both variables have just two categories then you should apply the <a class='glossary' title='Yates continuity correction is a correction for chi-squared that subtracts .5 from the difference between observed and expected in each cell, making the chi-squared value smaller and statistical significance harder to reach; it is often used when there are few observations in one or more of the cells. This correction is also used when both variables have just two categories because the chi-squared distribution is not a perfect representation of the distribution of differences between observed and expected of a chi-squared in the situation where both variables are binary. (SwR, Glossary and Chap. 5)'>Yates continuity correction</a>. It subtracts an additional .5 from the difference between observed and expected in each group, or cell of the table, making the chi-squared test statistic value smaller, making it therefore harder to reach statistical significance.\n\nThe correction is necessary because the chi-squared distribution is not a perfect representation of the distribution of differences between observed and expected of a chi-squared test in the situation where both variables are binary. Normally functions apply the correction as default whenever two binary variables are tested but you can decide via an argument whether you want to apply the correction or not.\n\nAn exception is `descr::CrossTable()` which provides automatically both versions whenever you compute the test statistic for a 2 by 2 table. This is somewhat illogical because you would always need only the version with the correction for a 2 by 2 table (and not both) and sometimes you would also want to apply it when there are few observations in one or more of the cells. \n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap05-yates}\n: Computing a chi-squared test statistic with the Yates continuity correction\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### descr::CrossTable() only test\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-yates-descr-only-test}\n: Chi-squared test for ease of voting and home ownership\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## load vote_clean ##########\nvote_clean <-  base::readRDS(\"data/chap05/vote_clean.rds\")\n\ndescr::CrossTable(\n    x = vote_clean$ease_vote,\n    y = vote_clean$ownhome,\n    expected = FALSE,\n    prop.r = FALSE,\n    prop.c = FALSE,\n    prop.t = FALSE,\n    prop.chisq = FALSE,\n    chisq = TRUE,\n    resid = FALSE,\n    sresid = FALSE,\n    asresid = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    Cell Contents \n#> |-------------------------|\n#> |                       N | \n#> |-------------------------|\n#> \n#> ==============================================\n#>                         vote_clean$ownhome\n#> vote_clean$ease_vote    Owned   Rented   Total\n#> ----------------------------------------------\n#> Register to vote          287      112     399\n#> ----------------------------------------------\n#> Make easy to vote         375      208     583\n#> ----------------------------------------------\n#> Total                     662      320     982\n#> ==============================================\n#> \n#> Statistics for All Table Factors\n#> \n#> Pearson's Chi-squared test \n#> ------------------------------------------------------------\n#> Chi^2 = 6.240398      d.f. = 1      p = 0.0125 \n#> \n#> Pearson's Chi-squared test with Yates' continuity correction \n#> ------------------------------------------------------------\n#> Chi^2 = 5.898905      d.f. = 1      p = 0.0152\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n###### rstatix::chisq_test() only test\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-yates-rstatix-only-test}\n: Chi-squared test for ease of voting and home ownership with and without Yates continuity correction\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nvote_ownhome_chisq1 <- rstatix::chisq_test(\n    vote_clean$ease_vote,\n    vote_clean$ownhome,\n    correct = FALSE\n)\n\nvote_ownhome_chisq2 <- rstatix::chisq_test(\n    vote_clean$ease_vote,\n    vote_clean$ownhome,\n    correct = TRUE\n)\n\nvote_ownhome_chisq <- \n    dplyr::bind_rows(\n        vote_ownhome_chisq1,\n        vote_ownhome_chisq2\n        ) |> \n    tibble::add_column(\n        \"Yates\" = c(\"No\", \"Yes\"),\n        .before = \"p.signif\"\n        )\n\nvote_ownhome_chisq\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 7\n#>       n statistic      p    df method          Yates p.signif\n#>   <int>     <dbl>  <dbl> <int> <chr>           <chr> <chr>   \n#> 1  1028      6.24 0.0125     1 Chi-square test No    *       \n#> 2  1028      5.90 0.0152     1 Chi-square test Yes   *\n```\n\n\n:::\n:::\n\n\n***\n\nTo compare the differences I have computed the chi-squared test twice with and without Yates correction. Then I have combined the results and added a column with the label yes/no.\n\n::::\n:::::\n\n\n###### descr::CrossTable() full data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-yates-descr}\n: Chi-squared test for ease of voting and home ownership\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescr::CrossTable(\n    x = vote_clean$ease_vote,\n    y = vote_clean$ownhome,\n    expected = TRUE,\n    prop.r = FALSE,\n    prop.c = FALSE,\n    prop.t = FALSE,\n    prop.chisq = FALSE,\n    chisq = TRUE,\n    resid = TRUE,\n    sresid = TRUE,\n    asresid = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    Cell Contents \n#> |-------------------------|\n#> |                       N | \n#> |              Expected N | \n#> |                Residual | \n#> |            Std Residual | \n#> |-------------------------|\n#> \n#> ===============================================\n#>                         vote_clean$ownhome\n#> vote_clean$ease_vote     Owned   Rented   Total\n#> -----------------------------------------------\n#> Register to vote           287      112     399\n#>                            269      130        \n#>                          18.02   -18.02        \n#>                          1.099   -1.580        \n#> -----------------------------------------------\n#> Make easy to vote          375      208     583\n#>                            393      190        \n#>                         -18.02    18.02        \n#>                         -0.909    1.307        \n#> -----------------------------------------------\n#> Total                      662      320     982\n#> ===============================================\n#> \n#> Statistics for All Table Factors\n#> \n#> Pearson's Chi-squared test \n#> ------------------------------------------------------------\n#> Chi^2 = 6.240398      d.f. = 1      p = 0.0125 \n#> \n#> Pearson's Chi-squared test with Yates' continuity correction \n#> ------------------------------------------------------------\n#> Chi^2 = 5.898905      d.f. = 1      p = 0.0152\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n###### rstatix::chisq_test() full data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-yates-rstatix}\n: Chi-squared test for ease of voting and home ownership with and without Yates continuity correction\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nvote_ownhome_chisq\n\nglue::glue(\" \")\nglue::glue(\"#####################################################################\")\nglue::glue(\" \")\n\nrstatix::chisq_descriptives(vote_ownhome_chisq)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 7\n#>       n statistic      p    df method          Yates p.signif\n#>   <int>     <dbl>  <dbl> <int> <chr>           <chr> <chr>   \n#> 1  1028      6.24 0.0125     1 Chi-square test No    *       \n#> 2  1028      5.90 0.0152     1 Chi-square test Yes   *       \n#>  \n#> #####################################################################\n#>  \n#> # A tibble: 4 × 9\n#>   x             y     observed  prop row.prop col.prop expected  resid std.resid\n#>   <fct>         <fct>    <int> <dbl>    <dbl>    <dbl>    <dbl>  <dbl>     <dbl>\n#> 1 Register to … Owned      287 0.292    0.719    0.434     269.  1.10       2.50\n#> 2 Make easy to… Owned      375 0.382    0.643    0.566     393. -0.909     -2.50\n#> 3 Register to … Rent…      112 0.114    0.281    0.35      130. -1.58      -2.50\n#> 4 Make easy to… Rent…      208 0.212    0.357    0.65      190.  1.31       2.50\n```\n\n\n:::\n:::\n\n\n***\n\nTo compare the differences I have computed the chi-squared test twice with and without Yates correction. Then I have combined the results and added a column with the label yes/no.\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\nIn all tabs of @exm-chap05-yates you can see that with the Yates continuity correction the $\\chi^2$ value is smaller and results in a somewhat higher p-value. But that does not matter in this case: Both versions are statistically significant $p < .05$.\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-statistically-signifant}\n: What do the stars under the heading `p-signif` in the results of the chi-squared tests with {**rstatix**} mean?\n::::::\n:::\n::::{.my-assessment-container}\n\n| significance <br>code          | p-value       |\n|:------------------------------:|---------------|\n|               ***              | [0, 0.001]    |\n|               **               | (0.001, 0.01] |\n|                *               | (0.01, 0.05]  |\n|                .               | (0.05, 0.1]   |\n|                                | (0.1, 1]      |\n\n: How to interpret stars as significance levels?  {#tbl-stars-significance} {.striped .hover}\n\n::::\n:::::\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-cramers-v-voting-homeowner}\n: Comuting the effect size with Cramér’s V\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nrstatix::cramer_v(\n    vote_clean$ease_vote,\n    vote_clean$ownhome,\n    correct = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.07750504\n```\n\n\n:::\n:::\n\n\nThe Yates continuity corrections also applies for the Cramér’s V effect size calculation. In this case the value of V falls into the weak or small effect size range.\n\n::::\n:::::\n\n:::::{.my-note}\n:::{.my-note-header}\nSummary abbreviated\n:::\n::::{.my-note-container}\nI have not followed the <a class='glossary' title='Null Hypothesis Significance Testing (NHST) is a process for organizing inferential statistical tests. (SwR, Glossary)'>NHST</a> procedure and the analysis of the relationship for ease of voting and home ownership. I understand and feel save about most of the content, therefore I focus only on material where I have difficulties or where I need more practice (as with the <a class='glossary' title='Yates continuity correction is a correction for chi-squared that subtracts .5 from the difference between observed and expected in each cell, making the chi-squared value smaller and statistical significance harder to reach; it is often used when there are few observations in one or more of the cells. This correction is also used when both variables have just two categories because the chi-squared distribution is not a perfect representation of the distribution of differences between observed and expected of a chi-squared in the situation where both variables are binary. (SwR, Glossary and Chap. 5)'>Yates continuity correction</a> and <a class='glossary' title='Cramér’s V is an effect size to determine the strength of the relationship between two categorical variables; often reported with the results of a chi-squared. (SwR, Glossary)'>Cramér’s V</a>.\n::::\n:::::\n\n\n### Phi coefficient\n\nFor 2 × 2 tables, the $k – 1$ term in the denominator of the Cramér’s V formula is always 1, so this term is not needed in the calculation. The formula without this term is called the <a class='glossary' title='The phi coefficient is a meassure of effect size to determine the strength of the relationship between two binary variables; often reported with the results of a chi-squared test (SwR, Glossary)'>phi coefficient</a>.\n\n***\n$$\n\\phi = \\sqrt{\\frac{\\chi^2}{n}}\n$$ {#eq-chap05-phi-coefficient}\n\nn = sample size\n\n***\n\n### Odds ratio\n\n:::::{.my-resource}\n:::{.my-resource-header}\nExplaining the odds ratio\n:::\n::::{.my-resource-container}\nThe explication in <a class='glossary' title='SwR is my abbreviation of: Harris, J. K. (2020). Statistics With R: Solving Problems Using Real-World Data (Illustrated Edition). SAGE Publications, Inc.'>SwR</a> is not easy to understand. So I have used other material a well:\n\n- Frost, J. (2022, January 11). Odds Ratio: Formula, Calculating & Interpreting. Statistics By Jim. https://statisticsbyjim.com/probability/odds-ratio/\n- Glen, S. (n.d). Odds Ratio Calculation and Interpretation. Statistics How To. https://www.statisticshowto.com/probability-and-statistics/probability-main-index/odds-ratio/\n- Poldrack, R. A. (2020, January 13). 10.12: Odds and Odds Ratios. Statistics LibreTexts. https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Statistical_Thinking_for_the_21st_Century_(Poldrack)/10%3A_Probability/10.12%3A_Odds_and_Odds_Ratios\n- Szumilas, M. (2010). Explaining Odds Ratios. Journal of the Canadian Academy of Child and Adolescent Psychiatry, 19(3), 227–229. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2938757/\n- Tenny, S., & Hoffman, M. R. (2024). Odds Ratio. In StatPearls. StatPearls Publishing. http://www.ncbi.nlm.nih.gov/books/NBK431098/\n\n::::\n:::::\n\nOdds is usually defined in statistics as the probability an event will occur divided by the probability that it will not occur. In other words, it’s a ratio of successes (or wins) to losses (or failures). As an example, if a racehorse runs 100 races and wins 20 times, the odds of the horse winning a race is 20/80 = 1/4 = 0.25.  \n\nThe odds definition is different to the somewhat similar definition of probability, which is the fraction of times an event occurs in a certain number of trials. In the horse example, the probability of a win is 20/100 = 0.2. (see [@glenn.da])\n\n$$\nOdds = \\frac{\\text{Probability Event Occurs (p)}}{{\\text{Probability Event Does Not Occur (1-p)}}}\n$$ {#eq-chap05-odds}\n\nOdds ratios with groups quantify the strength of the relationship between two conditions. They indicate how likely an outcome is to occur in one context relative to another. \n\n$$\n\\text{Odds Ratio} = \\frac{\\text{Odss of an Event (Condition A)}}{{\\text{Odds of an Event (Condition B)}}}\n$$ {#eq-chap05-odds-ratio}\n\nThe denominator (condition B) in the odds ratio formula is the baseline or control group. Consequently, the OR tells you how much more or less likely the numerator events (condition A) are likely to occur relative to the denominator events. If you have a treatment and control group, the treatment will be in the numerator while the control group is in the denominator of the formula [@frost2022].\n\nTaken the definition of odds and odds ratio together we get the formula:\n\n$$\n\\begin{align*}\n\\text{Odds Ratio} = \\frac{\\text{Odds of an Event (Condition A)}}{{\\text{Odds of an Event (Condition B)}}} = \\\\\n\\frac{\\text{Odds of an Event (A)} / \\text{Odds of an Non Event (A)}}{\\text{Odds of an Event (B)} / \\text{Odds of an Non Event (B)}} = \\\\\n\\frac{\\text{Odds of an Event (A)} \\times \\text{Odds of an Non Event (B)}}{\\text{Odds of Non Event (A)} \\times \\text{Odds of a Event (B)}}\n\\end{align*}\n$$ {#eq-chap05-odds-ratio2}\n\n\nThe book explanation of the <a class='glossary' title='Odds is usually defined in statistics as the probability an event will occur divided by the probability that it will not occur. An odds ratio (OR) is a measure of association between a certain property A and a second property B in a population. Specifically, it tells you how the presence or absence of property A has an effect on the presence or absence of property B. (Statistics How To). An odds ratio is a ratio of two ratios. They quantify the strength of the relationship between two conditions. They indicate how likely an outcome is to occur in one context relative to another. (Statistics by Jim)'>odds ratio</a> uses with <a class='glossary' title='Exposure is a characteristic, behavior, or other factor that may be associated with an outcome. (SwR, Glossary)'>exposure</a> and <a class='glossary' title='Outcome is the variable being explained or predicted by a model; in linear and logistic regression, the outcome variable is on the left-hand side of the equal sign. (SwR, Glossary)'>outcome</a> two new concepts and is therefore more difficult to understand. Under this terminology is the odds ratio a measure of the likelihood of a particular outcome. The odds ratio is calculated as the ratio of the number of events that produce or are exposed to that outcome to the number of events that do not produce, resp. are not exposed to the outcome. The odds ratio measures the odds of some event or outcome occurring given a particular exposure compared to the odds of it happening without that exposure. Or more generally: The odds ratio tells us the ratio of the odds of an event occurring in a treatment group compared to the odds of an event occurring in a control group. (Still pretty difficult…)\n\nIn our case of voting opinion and housing status the odds ratio would measure the odds of people that think one should register to vote given owning a home, compared to the odds of people that think one should register to vote given not owning a home. \n\n$$\nOR = \\frac{\\text{exposed with outcome} / \\text{unexposed with outcome}}{\\text{exposed no outcome} / \\text{unexposed no outcome}}\n$$ {#eq-chap05-odds-ratio3}\n\nTo fill in the correct values one has to conceptualize a 2x2 table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble::tribble(\n  ~Exposure,      ~Cases,  ~Control,\n  \"Exposed\",     \"a\",     \"b\",\n  \"Not Exposed\", \"c\",     \"d\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 3\n#>   Exposure    Cases Control\n#>   <chr>       <chr> <chr>  \n#> 1 Exposed     a     b      \n#> 2 Not Exposed c     d\n```\n\n\n:::\n:::\n\nThe columns \"Cases\" and \"Control\" are the Outcomes:\n\n- a = Number of exposed cases\n- b = Number of exposed non-cases\n- c = Number of unexposed cases\n- d = Number of unexposed non-cases\n\n$$\nOR = \\frac{a / c}{b / d} = \\frac{a \\times d}{b \\times c}\n$$ {#eq-chap05-odds-ratio-4}\n\nNow let's think what this general structure mean in our case with voting opinions (easy versus register) and housing status (owner or renter). \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvote_clean <- base::readRDS(\"data/chap05/vote_clean.rds\")\n(\n    vote_housing_table <- base::table(\n        vote_clean$ownhome,\n        vote_clean$ease_vote,\n        dnn = c(\"Housing status\", \"Voting opinion\")\n    )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>               Voting opinion\n#> Housing status Register to vote Make easy to vote\n#>         Owned               287               375\n#>         Rented              112               208\n```\n\n\n:::\n:::\n\n\n***\n\n::: {#bul-chap05-or-example}\n\n**Exposure and Outcome**\n\n- Exposed: Landlords \n- Not Exposed: Renter (tenants)\n- Cases: People that favor register for voting\n- Control: People that want easy voting\n\n**Cells and their values**\n\n- Number of exposed cases [1,1] = (a) = House owner that want people to register for voting = 287.\n- Number of exposed non-cases [1,2] = (c) = House owner that want easy voting = 375.\n- Number of unexposed cases [2,1] = (b) = Renter that want people to register for voting = 112.\n- Number of unexposed non-cases [2,2] = (d) = Renter that want easy voting = 208.\n\nCalculation of the odds ratio (OR) using the two-by-two frequency table of voting opinion by housing status\n\n:::\n\n***\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-chap05-odds-ratio}\n: Interpretation of odds ratios using our example of voting opinion by housing status\n::::::\n:::\n::::{.my-assessment-container}\n\n**General rule**\n\n- **OR = 1** indicates that the likelihood of the outcome for exposed is the same as for unexposed\n- **OR > 1** indicates higher odds of the outcome for exposed compared to unexposed, e.g. the event/outcome is more likely to occur.\n- **OR < 1** indicates lower odds of the outcome for exposed compared to unexposed, e.g. the event/outcome is less likely to occur.\n\n**Our example**\n\n- Home owners have 1.42 times the odds of thinking people should register to vote compared to people who do not own homes. \n- Or alternatively: Home owners have 42% higher odds of thinking people should register to vote compared to people who do not own homes.\n\n::::\n:::::\n\n\n$$\nOR = \\frac{a / c}{b / d} = \\frac{287 / 112}{375 / 208} = \\frac{2.5625}{1.802885} = 1.42\n$$ {#eq-chap05-odds-ratio-example}\n\nThe p-value for odds ratios has the same broad meaning as p-values for the chi-squared. But instead of being based on the area under the curve for the chi-squared distribution, it is based on the area under the curve for the log of the odds ratio, which is approximately normally distributed. The odds ratio can only be a positive number, and it results in a right-skewed distribution, which the log function can often transform to something close to normal.\n\n:::::{.my-resource}\n:::{.my-resource-header}\nPackages with odds ratio function\n:::\n::::{.my-resource-container}\nThe book explains the manual calculation and recommends the {**fmsb**} package. I found via internet research some other packages with an odds ratio function: The following list is alphaetically sorted:\n\n- {**DescTools**}: `DescTools::OddsRatio()`\n- {**epitools**}: `epitools::oddsratio()`\n- {**fmsb**}: `fmsb::oddsratio()`\n\nThe packages {**tern**} and {**BioProbability**} feature also a odds ratio function. But I haven't looked into these packages because they have less than 100 downloads daily form the RStudio CRAN Mirror server.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-pkgs-dl-odds-ratio}\n: Number of daily downloads for packages with an odds ratio function\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#tbl-odds-ratio-pkgs .cell tbl-cap='Daily donwloads of packages with odds ratio function'}\n\n```{.r .cell-code}\npkgs = c(\"DescTools\", \"epitools\", \"fmsb\", \"tern\", \"BioProbability\")\npkgs_dl(pkgs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 5 × 2\n#>   package            n\n#>   <chr>          <dbl>\n#> 1 DescTools       2282\n#> 2 fmsb             411\n#> 3 epitools         335\n#> 4 tern              26\n#> 5 BioProbability    22\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n::::\n:::::\n\n\n\n\n\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap05-odds-ratio}\n: Computing the odds ratio\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Manually\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-odds-ratio-by-hand}\n: Odds ratio of ease of voting by home ownership computed manually\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglue::glue(\"############### Table format used ################## \")\n(\n    vote_housing_table <- base::table(\n        vote_clean$ownhome,\n        vote_clean$ease_vote,\n        dnn = c(\"Voting opinion\", \"Housing status\")\n    )\n)\nodds_ratio <-  round((287 / 112) / (375 / 208), 2)\n\nglue::glue(\" \")\nglue::glue(\"###################################################\")\nglue::glue(\"Oddsratio: {odds_ratio}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ############### Table format used ################## \n#>               Housing status\n#> Voting opinion Register to vote Make easy to vote\n#>         Owned               287               375\n#>         Rented              112               208\n#>  \n#> ###################################################\n#> Oddsratio: 1.42\n```\n\n\n:::\n:::\n\n***\n\nThe calculation uses the frequencies in the 2 × 2 table where the rows are the exposure and the columns are the outcome.\n::::\n:::::\n\n\n###### {**fmsb**}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-odds-ratio-fmsb}\n: Odds ratio of ease of voting by home ownership using `fmsb::oddsratio()`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglue::glue(\"*****************   Input counts manually   ***********\")\nfmsb::oddsratio(a = 287, b = 112, c = 375, d = 208)\n\nglue::glue(\" \")\nglue::glue(\"*******************************************************\")\n\nfmsb::oddsratio(vote_housing_table)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning in N1 * N0 * M1 * M0: NAs produced by integer overflow\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> *****************   Input counts manually   ***********\n#>            Disease Nondisease Total\n#> Exposed        287        375   662\n#> Nonexposed     112        208   320\n#> Total          399        583   982\n#> \n#> \tOdds ratio estimate and its significance probability\n#> \n#> data:  287 112 375 208\n#> p-value = 0.01253\n#> 95 percent confidence interval:\n#>  1.078097 1.873847\n#> sample estimates:\n#> [1] 1.421333\n#> \n#>  \n#> *******************************************************\n#>            Disease Nondisease Total\n#> Exposed        287        375   662\n#> Nonexposed     112        208   320\n#> Total          399        583   982\n#> \n#> \tOdds ratio estimate and its significance probability\n#> \n#> data:  vote_housing_table\n#> p-value = NA\n#> 95 percent confidence interval:\n#>  1.078097 1.873847\n#> sample estimates:\n#> [1] 1.421333\n```\n\n\n:::\n:::\n\n\n***\n\nHere I have replicated the code from the book. {**fmsb**} has a disadvantage: You have to specify the values manually, you can't use a table object. It is said that the function will also work with a matrix but then I got a warning message:\n\n> Warning in N1 * N0 * M1 * M0: NAs produced by integer overflow\n\nAs a result of the produced NA's the p-value is not computed. (But the calculated odds ratio is correct.)\n\nSo the best option is to stick with manually input. Besides of this inconvenience there is also a somewhat improper medical summary of the table (\"Disease\" / \"Nondisease\"). \n\n\n\n::::\n:::::\n\n###### {**DescTools**}\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-odds-ratio-desctools}\n: Odds ratio of ease of voting by home ownership using `DescTools::OddsRatio()`\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nDescTools::OddsRatio(\n    x = vote_housing_table, \n    conf.level = .95,\n    method = \"midp\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> odds ratio     lwr.ci     upr.ci \n#>   1.420209   1.078575   1.876316\n```\n\n\n:::\n:::\n\n***\n\nThis is a very sparse output. In contrast to the two other packages it misses the table summary and the p-value.\n\n\n::::\n:::::\n\n###### {**epitools**}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-odds-ratio-epitools}\n: Odds ratio of ease of voting by home ownership using `epitools::oddsratio()`\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nepitools::oddsratio.midp(vote_housing_table, \n                    correction = TRUE,\n                    verbose = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $x\n#>               Housing status\n#> Voting opinion Register to vote Make easy to vote\n#>         Owned               287               375\n#>         Rented              112               208\n#> \n#> $data\n#>               Housing status\n#> Voting opinion Register to vote Make easy to vote Total\n#>         Owned               287               375   662\n#>         Rented              112               208   320\n#>         Total               399               583   982\n#> \n#> $p.exposed\n#>               Housing status\n#> Voting opinion Register to vote Make easy to vote     Total\n#>         Owned         0.7192982         0.6432247 0.6741344\n#>         Rented        0.2807018         0.3567753 0.3258656\n#>         Total         1.0000000         1.0000000 1.0000000\n#> \n#> $p.outcome\n#>               Housing status\n#> Voting opinion Register to vote Make easy to vote Total\n#>         Owned         0.4335347         0.5664653     1\n#>         Rented        0.3500000         0.6500000     1\n#>         Total         0.4063136         0.5936864     1\n#> \n#> $measure\n#>               odds ratio with 95% C.I.\n#> Voting opinion estimate    lower    upper\n#>         Owned  1.000000       NA       NA\n#>         Rented 1.420209 1.078575 1.876316\n#> \n#> $conf.level\n#> [1] 0.95\n#> \n#> $p.value\n#>               two-sided\n#> Voting opinion midp.exact fisher.exact chi.square\n#>         Owned          NA           NA         NA\n#>         Rented 0.01235162   0.01268694  0.0151503\n#> \n#> $correction\n#> [1] TRUE\n#> \n#> attr(,\"method\")\n#> [1] \"median-unbiased estimate & mid-p exact CI\"\n```\n\n\n:::\n:::\n\n\n***\n\nThis is the most detailed output. There exist also a less verbose version without \n\n- replicating the raw data $x$\n- calculation of exposed proportions\n- calculation of outcome proportions\n- repeating the confidence level\n\nThis more stringent version has the most important information and is in my opinion the best option for calculating the odds ratio.\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n:::::{.my-important}\n:::{.my-important-header}\n{**epitools**} is my preferred method for the odds ratio calculation\n:::\n::::{.my-important-container}\n\nBecause of the somewhat inconvenient data input for the `oddsratio()` function of the {**fmsb**} package and the sparse output `OddsRatio()` function of the {**DescTool**} package I prefer the computation with {**epitools**} in its more stringent option (`verbose = FALSE`).\n\n::::\n:::::\n\n\n## Achievement 8: When chi-squared assumptions fail {#sec-chap05-achievement8}\n\nWhat is to do when one of the chi-squared assumption fails?\n\n### Variables not nominal or ordinal\n\nUse a different statistical test. Chi-squared is only appropriate for categorical variables.\n\n### Sample too small\n\nThe assumption of expected values of 5 or higher in at least 80% of groups is necessary because the sampling distribution for the chi-squared statistic only approximates the actual chi-squared distribution but does not capture it completely accurately. When a sample is large, the approximation is better and using the chi-squared distribution to determine statistical significance works well.\n\nHowever, for very small samples, the approximation is not great, so a different method of computing the p-value is better. The method most commonly used is the <a class='glossary' title='Fisher’s exact test is an alternative to the chi-squared test for use with small samples. (SwR, Glossary)'>Fisher’s exact test</a> (`stats::fisher.test()`, `rstatix::fisher_test()`, `janitor::fisher.test()`, `fmsb::pairwise.fisher.test()`).\n\n### Observation not independent\n\n- If both variables are binary (have only two categories) use <a class='glossary' title='McNemar’s test is an alternative to the chi-squared test of independence for when observations are not independent and both variables are binary; for example, McNemar’s test could be used to compare proportions in two groups before and after an intervention (SwR, Glossary)'>McNemar’s test</a> (`stats::mcnemar.test()`)\n- If there are three or more groups for one variable and a binary second variable use <a class='glossary' title='Cochran’s Q-test is an alternative to the chi-squared test of independence for when observations are not independent; for example, comparing groups before and after an intervention would fail the independent observations assumption (SwR, Glossary)'>Cochran’s Q-test</a>. Besides the book recommendation `nonpar::cochrans.q()` the test is also availabe in other packages, that I used already for this book: `DescTools::CochranQTest()`, `rstatix::cochran_qtest()`.\n\n\n## Exercises (empty)\n\n## Packages introduced in this chapter \n\n### cranlogs\n\n:::::{.my-resource}\n:::{.my-resource-header}\ncranlogs: Download Logs from the 'RStudio' 'CRAN' Mirror \n:::\n::::{.my-resource-container}\n\n***\n\n::: {#pak-cranlogs}\n\n***\n\n{**cranlogs**}: [Download Logs from the RStudio CRAN Mirror](https://r-hub.github.io/cranlogs/) [@cranlogs]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap05/logoi/logo-cranlogs-min.png){width=\"176\"}\n\n\n<a class='glossary' title='An API, or application programming interface, is a set of defined rules that enable different applications to communicate with each other. It acts as an intermediary layer that processes data transfers between systems, letting companies open their application data and functionality to external third-party developers, business partners, and internal departments within their companies. (IBM)'>API</a> to the database of <a class='glossary' title='Comprehensive R Archive Network'>CRAN</a> package downloads from the RStudio CRAN mirror. The database itself is at <http://cranlogs.r-pkg.org>,\n    see <https://github.com/r-hub/cranlogs.app> for the raw API.\n\n:::\n\nRStudio publishes the download logs from their CRAN package mirror daily at http://cran-logs.rstudio.com.\n\nThis R package queries a web API maintained by R-hub that contains the daily download numbers for each package.\n\nThe RStudio CRAN mirror is not the only CRAN mirror, but it’s a popular one: it’s the default choice for RStudio users. The actual number of downloads over all CRAN mirrors is unknown.\n\n\n{**cranlogs**}: Download Logs from the 'RStudio' 'CRAN' Mirror\n:::\n\n***\n::::\n:::::\n\n\n### crosstable\n\n:::::{.my-resource}\n:::{.my-resource-header}\ncrosstable: Crosstables for Descriptive Analyses \n:::\n::::{.my-resource-container}\n\n***\n\n::: {#pak-crosstable}\n\n***\n\n{**crosstable**}: [Crosstables for Descriptive Analyses](https://danchaltiel.github.io/crosstable/) [@crosstable]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap05/logoi/logo-crosstable-min.png){width=\"176\"}\n\n\nCrosstable is a package centered on a single function, crosstable, which easily computes descriptive statistics on datasets. It can use the {**tidyverse**} syntax and is interfaced with the package {**officer**} to create automatized reports.\n\n:::\n\nCreate descriptive tables for continuous and categorical variables. Apply summary statistics and counting function, with or without a grouping variable, and create beautiful reports using {**rmarkdown**} or {**officer**}. You can also compute effect sizes and statistical tests if needed.\n\n{**crosstable**}: Crosstables for Descriptive Analyses\n:::\n\nI believe that the main usage for this package is to prepare ready-to-print tables. Similar like {**gtsummary**} (see @pak-gtsummary) it provides some descriptive statistics with many display options. But I got the impression that analysis of data is not the main usage of these packages. \n\nFor instance you could use `crosstable::display_test(chisq.test(x, y))` to get as result a string, for instance: p value: <0.0001 \\n(Pearson's Chi-squared test)\". This is nice to include for a table, but for the analysis one would also need the values of the different cells.\n\n***\n::::\n:::::\n\n### DescTools\n\n:::::{.my-resource}\n:::{.my-resource-header}\nDescTools: Tools for Descriptive Statistics \n:::\n::::{.my-resource-container}\n\n***\n\n::: {#pak-DescTools}\n\n***\n\n{**DescTools**}: [Tools for Descriptive Statistics](https://andrisignorell.github.io/DescTools/) [@DescTools]\n\n\n(*There is no hexagon logo for {**DescTools**} available*)\n\n:::\n\nA collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data. The author's intention was to create a toolbox, which facilitates the (notoriously time consuming) first descriptive tasks in data analysis, consisting of calculating descriptive statistics, drawing graphical summaries and reporting the results. \n\nThe package contains furthermore functions to produce documents using MS Word (or PowerPoint) and functions to import data from Excel. Many of the included functions can be found scattered in other packages and other sources written partly by Titans of R. The reason for collecting them here, was primarily to have them consolidated in ONE instead of dozens of packages (which themselves might depend on other packages which are not needed at all), and to provide a common and consistent interface as far as function and arguments naming, NA handling, recycling rules etc. are concerned. Google style guides were used as naming rules (in absence of convincing alternatives). The 'BigCamelCase' style was consequently applied to functions borrowed from contributed R packages as well.\n\n{**DescTools**}: Tools for Descriptive Statistics\n:::\n\n***\n::::\n:::::\n\n\n\n### fmsb\n\n:::::{.my-resource}\n:::{.my-resource-header}\nfmsb: Functions for Medical Statistics Book with some Demographic Data \n:::\n::::{.my-resource-container}\n\n***\n\n::: {#pak-fmsb}\n\n***\n\n{**fmsb**}: [Functions for Medical Statistics Book with some Demographic Data](https://cran.r-project.org/package=fmsb) [@fmsb]\n\n(*There is no hexagon logo for {**fmsb**} available*)\n\n\nSeveral utility functions for the book entitled \"Practices of Medical and Health Data Analysis using R\" (Pearson Education Japan, 2007) with Japanese demographic data and some demographic analysis related functions.\n\n{**fmsb**}: Functions for Medical Statistics Book with some Demographic Data\n:::\n\n***\n::::\n:::::\n\n### lsr\n\n:::::{.my-resource}\n:::{.my-resource-header}\nlsr: Companion to \"Learning Statistics with R\" \n:::\n::::{.my-resource-container}\n\n***\n\n::: {#pak-lsr}\n\n***\n\n{**lsr**}: [Companion to \"Learning Statistics with R\"](https://learningstatisticswithr.com/) [@lsr]\n\n(*There is no hexagon logo for {**lsr**} available*)\n\nA collection of tools intended to make introductory statistics easier to teach, including wrappers for common hypothesis tests and basic data manipulation. It accompanies Navarro, D. J. (2015). Learning Statistics with R: A Tutorial for Psychology Students and Other Beginners, Version 0.6. \n\n\n{**lsr**}: Companion to \"Learning Statistics with R\"\n:::\n\n***\n::::\n:::::\n\n### naniar\n\n:::::{.my-resource}\n:::{.my-resource-header}\nnaniar: Data Structures, Summaries, and Visualisations for Missing Data \n:::\n::::{.my-resource-container}\n\n***\n\n::: {#pak-naniar}\n\n***\n\n{**naniar**}: [https://github.com/njtierney/naniar](https://naniar.njtierney.com/) [@naniar]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap05/logoi/logo-naniar-min.png){width=\"176\"}\n\n\n{**naniar**} provides principled, tidy ways to summarise, visualise, and manipulate missing data with minimal deviations from the workflows in ggplot2 and tidy data.\n\n:::\n\nMissing values are ubiquitous in data and need to be explored and handled in the initial stages of analysis. {**naniar**} provides data structures and functions that facilitate the plotting of missing values and examination of imputations. This allows missing data dependencies to be  explored with minimal deviation from the common work patterns of 'ggplot2' and tidy data. The work is fully discussed in Tierney & Cook [-@tierney2023].\n\n{**naniar**}: https://github.com/njtierney/naniar\n:::\n\n\n***\n::::\n:::::\n\n### rstatix\n\n:::::{.my-resource}\n:::{.my-resource-header}\nrstatix: Pipe-Friendly Framework for Basic Statistical Tests \n:::\n::::{.my-resource-container}\n\n***\n\n::: {#pak-rstatix}\n\n***\n\n{**rstatix**}: [Pipe-Friendly Framework for Basic Statistical Tests](https://rpkgs.datanovia.com/rstatix/) [@rstatix]\n\n\n(*There is no hexagon logo for {**rstatix**} available*)\n\nProvides a simple and intuitive pipe-friendly framework, coherent with the {**tidyverse**} design philosophy, for performing basic statistical tests, including t-test, Wilcoxon test, ANOVA, Kruskal-Wallis and correlation analyses.\n\nThe output of each test is automatically transformed into a tidy data frame to facilitate visualization.\n\nAdditional functions are available for reshaping, reordering, manipulating and visualizing correlation matrix. Functions are also included to facilitate the analysis of factorial experiments, including purely ‘within-Ss’ designs (repeated measures), purely ‘between-Ss’ designs, and mixed ‘within-and-between-Ss’ designs.\n\nIt’s also possible to compute several effect size metrics, including “eta squared” for ANOVA, “Cohen’s d” for t-test and “Cramer’s V” for the association between categorical variables. The package contains helper functions for identifying univariate and multivariate outliers, assessing normality and homogeneity of variances.\n\n{**rstatix**}: Pipe-Friendly Framework for Basic Statistical Tests\n:::\n\n***\n::::\n:::::\n\n\n### sjPlot\n\n:::::{.my-resource}\n:::{.my-resource-header}\nsjPlot: Data Visualization for Statistics in Social Science \n:::\n::::{.my-resource-container}\n\n***\n\n::: {#pak-sjPlot}\n\n***\n\n{**sjPlot**}: [Data Visualization for Statistics in Social Science](https://strengejacke.github.io/sjPlot/) [@sjPlot]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap05/logoi/logo-sjPlot-min.png){width=\"176\"}\n\nCollection of plotting and table output functions for data visualization. Results of various statistical analyses (that are commonly used in social sciences) can be visualized using this package, including simple and cross tabulated frequencies, histograms, box plots, (generalized) linear models, mixed effects models, PCA and correlation matrices, cluster analyses, scatter plots, Likert scales, effects plots of interaction terms in regression models, constructing index or score variables and much more.\n\n:::\n\n{**sjPlot**}: Data Visualization for Statistics in Social Science\n:::\n\nThe standard plot versions are easy to create, but to adapt the resulted graph is another issue. Although {**sjPlot**} uses in the background the {**ggplot2**} package, you can’t specify changes with ggplot2 commands. I tried it and it produced two different plots. \n\nTo customize plot appearance you have to learn the many arguments of of `sjPlot:set_theme()` and `sjPlot::plot_grpfrq()`. See the documentation of the [many specialized functions](https://strengejacke.github.io/sjPlot/reference/index.html#plot-customization) to tweak the default values.\n\n\n***\n::::\n:::::\n\n\n\n## Glossary\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Alternate Hypothesis </td>\n   <td style=\"text-align:left;\"> An alternate hypothesis (HA or sometimes written as H1) is a claim that there is a difference or relationship among things; the alternate hypothesis is paired with the null hypothesis that typcially states there is no relationship or no difference between things. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> API </td>\n   <td style=\"text-align:left;\"> An API, or application programming interface, is a set of defined rules that enable different applications to communicate with each other. It acts as an intermediary layer that processes data transfers between systems, letting companies open their application data and functionality to external third-party developers, business partners, and internal departments within their companies. (&lt;a href=\"https://www.ibm.com/topics/api\"&gt;IBM&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Chi-squared </td>\n   <td style=\"text-align:left;\"> Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Cochran’s Q-test </td>\n   <td style=\"text-align:left;\"> Cochran’s Q-test is an alternative to the chi-squared test of independence for when observations are not independent; for example, comparing groups before and after an intervention would fail the independent observations assumption (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Cramér’s V </td>\n   <td style=\"text-align:left;\"> Cramér’s V is an effect size to determine the strength of the relationship between two categorical variables; often reported with the results of a chi-squared. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> CRAN </td>\n   <td style=\"text-align:left;\"> Comprehensive R Archive Network </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Degrees of Freedom </td>\n   <td style=\"text-align:left;\"> Degree of Freedom (df) is the number of pieces of information that are allowed to vary in computing a statistic before the remaining pieces of information are known; degrees of freedom are often used as parameters for distributions (e.g., chi-squared, F). (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Effect Size </td>\n   <td style=\"text-align:left;\"> Effect size is a measure of the strength of a relationship; effect sizes are important in inferential statistics in order to determine and communicate whether a statistically significant result has practical importance. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Exposure </td>\n   <td style=\"text-align:left;\"> Exposure is a characteristic, behavior, or other factor that may be associated with an outcome. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Fisher’s exact test </td>\n   <td style=\"text-align:left;\"> Fisher’s exact test is an alternative to the chi-squared test for use with small samples. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> McNemar’s test </td>\n   <td style=\"text-align:left;\"> McNemar’s test is an alternative to the chi-squared test of independence for when observations are not independent and both variables are binary; for example, McNemar’s test could be used to compare proportions in two groups before and after an intervention (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> NHST </td>\n   <td style=\"text-align:left;\"> Null Hypothesis Significance Testing (NHST) is a process for organizing inferential statistical tests. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Null Hypothesis </td>\n   <td style=\"text-align:left;\"> The null hypothesis (H0, or simply the Null) is a statement of no difference or no association that is used to guide statistical inference testing (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Odds Ratio </td>\n   <td style=\"text-align:left;\"> Odds is usually defined in statistics as the probability an event will occur divided by the probability that it will not occur. An odds ratio (OR) is a measure of association between a certain property A and a second property B in a population. Specifically, it tells you how the presence or absence of property A has an effect on the presence or absence of property B. (&lt;a href=\"https://www.statisticshowto.com/probability-and-statistics/probability-main-index/odds-ratio/\"&gt;Statistics How To&lt;/a&gt;). An odds ratio is a ratio of two ratios. They quantify the strength of the relationship between two conditions. They indicate how likely an outcome is to occur in one context relative to another. (&lt;a href=\"https://statisticsbyjim.com/probability/odds-ratio/\"&gt;Statistics by Jim&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Omnibus </td>\n   <td style=\"text-align:left;\"> An omnibus is a statistical test that identifies that there is some relationship going on between two categorical variables, but not what that relationship is. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Outcome </td>\n   <td style=\"text-align:left;\"> Outcome is the variable being explained or predicted by a model; in linear and logistic regression, the outcome variable is on the left-hand side of the equal sign. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p-value </td>\n   <td style=\"text-align:left;\"> The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Parameter </td>\n   <td style=\"text-align:left;\"> Unobserved variables are usually called Parameters. (SR2, Chap.2) A parameter is an unknown numerical characteristics of a population that must be estimated. (CDS). They are also numbers that govern statistical models ([stats.stackexchange](https://stats.stackexchange.com/a/255994/207389)). A parameter is also a number that is a defining characteristic of some population or a feature of a population. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Pew Research Center </td>\n   <td style=\"text-align:left;\"> The Pew Research Center (also simply known as Pew) is a nonpartisan American think tank based in Washington, D.C. It provides information on social issues, public opinion, and demographic trends shaping the United States and the world. It also conducts public opinion polling, demographic research, random sample survey research, and panel based surveys, media content analysis, and other empirical social science research. (&lt;a href=\"https://en.wikipedia.org/wiki/Pew_Research_Center\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Phi coefficient </td>\n   <td style=\"text-align:left;\"> The phi coefficient is a meassure of effect size to determine the strength of the relationship between two binary variables; often reported with the results of a chi-squared test (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Population </td>\n   <td style=\"text-align:left;\"> A population consists statistically of all the observations that fit some criterion; for example, all of the people currently living in the country of Bhutan or all of the people in the world currently eating strawberry ice cream. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Probability Density Function </td>\n   <td style=\"text-align:left;\"> A probability density function (PDF) tells us the probability that a random variable takes on a certain value. (&lt;a href=\"https://www.statology.org/cdf-vs-pdf/\"&gt;Statology&lt;/a&gt;) The probability density function (PDF) for a given value of random variable X represents the density of probability (probability per unit random variable) within a particular range of that random variable X. Probability densities can take values larger than 1. ([StackExchange Mathematics](https://math.stackexchange.com/a/1464837/1215136)) We can use a continuous probability distribution to calculate the probability that a random variable lies within an interval of possible values. To do this, we use the continuous analogue of a sum, an integral. However, we recognise that calculating an integral is equivalent to calculating the area under a probability density curve. We use `p(value)` for probability densities and `P </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Samples </td>\n   <td style=\"text-align:left;\"> Samples are subsets of observations from some population that is often analyzed to learn about the population sampled. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standard Deviation </td>\n   <td style=\"text-align:left;\"> The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter $\\sigma$ (sigma), for the population standard deviation, or the Latin letter $s$ for the sample standard deviation. ([Wikipedia](https://en.wikipedia.org/wiki/Standard_deviation)) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standardized Residuals </td>\n   <td style=\"text-align:left;\"> Standardized residuals are the standardized differences between observed and expected values in a chi-squared analysis; a large standardized residual indicates that the observed and expected values were very different. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> SwR </td>\n   <td style=\"text-align:left;\"> SwR is my abbreviation of: Harris, J. K. (2020). Statistics With R: Solving Problems Using Real-World Data (Illustrated Edition). SAGE Publications, Inc. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Yates continuity correction </td>\n   <td style=\"text-align:left;\"> Yates continuity correction is a correction for chi-squared that subtracts .5 from the difference between observed and expected in each cell, making the chi-squared value smaller and statistical significance harder to reach; it is often used when there are few observations in one or more of the cells. This correction is also used when both variables have just two categories because the chi-squared distribution is not a perfect representation of the distribution of differences between observed and expected of a chi-squared in the situation where both variables are binary. (SwR, Glossary and Chap. 5) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Z-score </td>\n   <td style=\"text-align:left;\"> A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (&lt;a href=\"https://www.statisticshowto.com/probability-and-statistics/z-score/#Whatisazscore\"&gt;StatisticsHowTo&lt;/a&gt;) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Session Info {.unnumbered}\n\n::: my-r-code\n::: my-r-code-header\nSession Info\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.3.2 (2023-10-31)\n#>  os       macOS Sonoma 14.3.1\n#>  system   x86_64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2024-03-15\n#>  pandoc   3.1.12.2 @ /usr/local/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package      * version    date (UTC) lib source\n#>  abind          1.4-5      2016-07-21 [1] CRAN (R 4.3.0)\n#>  backports      1.4.1      2021-12-13 [1] CRAN (R 4.3.0)\n#>  base64enc      0.1-3      2015-07-28 [1] CRAN (R 4.3.0)\n#>  bayestestR     0.13.2     2024-02-12 [1] CRAN (R 4.3.2)\n#>  boot           1.3-30     2024-02-26 [2] CRAN (R 4.3.2)\n#>  broom          1.0.5      2023-06-09 [1] CRAN (R 4.3.0)\n#>  car            3.1-2      2023-03-30 [1] CRAN (R 4.3.0)\n#>  carData        3.0-5      2022-01-06 [1] CRAN (R 4.3.0)\n#>  cellranger     1.1.0      2016-07-27 [1] CRAN (R 4.3.0)\n#>  class          7.3-22     2023-05-03 [2] CRAN (R 4.3.2)\n#>  cli            3.6.2      2023-12-11 [1] CRAN (R 4.3.0)\n#>  coda           0.19-4.1   2024-01-31 [1] CRAN (R 4.3.2)\n#>  codetools      0.2-19     2023-02-01 [2] CRAN (R 4.3.2)\n#>  colorspace     2.1-1      2024-01-03 [1] R-Forge (R 4.3.2)\n#>  commonmark     1.9.1      2024-01-30 [1] CRAN (R 4.3.2)\n#>  cranlogs       2.1.1      2019-04-29 [1] CRAN (R 4.3.0)\n#>  crayon         1.5.2      2022-09-29 [1] CRAN (R 4.3.0)\n#>  curl           5.2.0      2023-12-08 [1] CRAN (R 4.3.2)\n#>  data.table     1.15.2     2024-02-29 [1] CRAN (R 4.3.2)\n#>  descr          1.1.8      2023-11-27 [1] CRAN (R 4.3.0)\n#>  DescTools      0.99.54    2024-02-03 [1] CRAN (R 4.3.2)\n#>  digest         0.6.34     2024-01-11 [1] CRAN (R 4.3.0)\n#>  dplyr          1.1.4      2023-11-17 [1] CRAN (R 4.3.0)\n#>  e1071          1.7-14     2023-12-06 [1] CRAN (R 4.3.0)\n#>  ellipsis       0.3.2      2021-04-29 [1] CRAN (R 4.3.0)\n#>  emmeans        1.10.0     2024-01-23 [1] CRAN (R 4.3.2)\n#>  epitools       0.5-10.1   2020-03-22 [1] CRAN (R 4.3.0)\n#>  estimability   1.5        2024-02-20 [1] CRAN (R 4.3.2)\n#>  evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.0)\n#>  Exact          3.2        2022-09-25 [1] CRAN (R 4.3.0)\n#>  expm           0.999-9    2024-01-11 [1] CRAN (R 4.3.0)\n#>  fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.0)\n#>  farver         2.1.1      2022-07-06 [1] CRAN (R 4.3.0)\n#>  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#>  fmsb           0.7.6      2024-01-19 [1] CRAN (R 4.3.0)\n#>  forcats        1.0.0      2023-01-29 [1] CRAN (R 4.3.0)\n#>  generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#>  ggeffects      1.5.0      2024-02-24 [1] CRAN (R 4.3.2)\n#>  ggplot2        3.5.0      2024-02-23 [1] CRAN (R 4.3.2)\n#>  gld            2.6.6      2022-10-23 [1] CRAN (R 4.3.0)\n#>  glossary     * 1.0.0.9000 2023-08-12 [1] Github (debruine/glossary@819e329)\n#>  glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.0)\n#>  gridExtra      2.3        2017-09-09 [1] CRAN (R 4.3.0)\n#>  gtable         0.3.4      2023-08-21 [1] CRAN (R 4.3.0)\n#>  haven          2.5.4      2023-11-30 [1] CRAN (R 4.3.2)\n#>  here           1.0.1      2020-12-13 [1] CRAN (R 4.3.0)\n#>  highr          0.10       2022-12-22 [1] CRAN (R 4.3.0)\n#>  hms            1.1.3      2023-03-21 [1] CRAN (R 4.3.0)\n#>  htmltools      0.5.7      2023-11-03 [1] CRAN (R 4.3.0)\n#>  htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.3.0)\n#>  httpuv         1.6.14     2024-01-26 [1] CRAN (R 4.3.2)\n#>  httr           1.4.7      2023-08-15 [1] CRAN (R 4.3.0)\n#>  insight        0.19.8     2024-01-31 [1] CRAN (R 4.3.2)\n#>  janitor        2.2.0      2023-02-02 [1] CRAN (R 4.3.0)\n#>  jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.0)\n#>  kableExtra     1.4.0      2024-01-24 [1] CRAN (R 4.3.2)\n#>  knitr          1.45       2023-10-30 [1] CRAN (R 4.3.0)\n#>  labeling       0.4.3      2023-08-29 [1] CRAN (R 4.3.0)\n#>  labelled       2.12.0     2023-06-21 [1] CRAN (R 4.3.0)\n#>  later          1.3.2      2023-12-06 [1] CRAN (R 4.3.0)\n#>  lattice        0.22-5     2023-10-24 [2] CRAN (R 4.3.0)\n#>  lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.3.0)\n#>  lme4           1.1-35.1   2023-11-05 [1] CRAN (R 4.3.0)\n#>  lmom           3.0        2023-08-29 [1] CRAN (R 4.3.0)\n#>  lsr            0.5.2      2021-12-01 [1] CRAN (R 4.3.0)\n#>  lubridate      1.9.3      2023-09-27 [1] CRAN (R 4.3.0)\n#>  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#>  markdown       1.12       2023-12-06 [1] CRAN (R 4.3.0)\n#>  MASS           7.3-60.0.1 2024-01-13 [2] CRAN (R 4.3.0)\n#>  Matrix         1.6-5      2024-01-11 [1] CRAN (R 4.3.0)\n#>  mime           0.12       2021-09-28 [1] CRAN (R 4.3.0)\n#>  miniUI         0.1.1.1    2018-05-18 [1] CRAN (R 4.3.0)\n#>  minqa          1.2.6      2023-09-11 [1] CRAN (R 4.3.0)\n#>  modelr         0.1.11     2023-03-22 [1] CRAN (R 4.3.0)\n#>  multcomp       1.4-25     2023-06-20 [1] CRAN (R 4.3.0)\n#>  munsell        0.5.0      2018-06-12 [1] CRAN (R 4.3.0)\n#>  mvtnorm        1.2-4      2023-11-27 [1] CRAN (R 4.3.2)\n#>  naniar         1.0.0      2023-02-02 [1] CRAN (R 4.3.0)\n#>  nlme           3.1-164    2023-11-27 [1] CRAN (R 4.3.2)\n#>  nloptr         2.0.3      2022-05-26 [1] CRAN (R 4.3.0)\n#>  performance    0.10.9     2024-02-17 [1] CRAN (R 4.3.2)\n#>  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#>  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#>  promises       1.2.1      2023-08-10 [1] CRAN (R 4.3.0)\n#>  proxy          0.4-27     2022-06-09 [1] CRAN (R 4.3.0)\n#>  purrr          1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n#>  questionr      0.7.8      2023-01-31 [1] CRAN (R 4.3.0)\n#>  R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#>  Rcpp           1.0.12     2024-01-09 [1] CRAN (R 4.3.0)\n#>  readxl         1.4.3      2023-07-06 [1] CRAN (R 4.3.0)\n#>  repr           1.1.6      2023-01-26 [1] CRAN (R 4.3.0)\n#>  rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.0)\n#>  rmarkdown      2.25       2023-09-18 [1] CRAN (R 4.3.0)\n#>  rootSolve      1.8.2.4    2023-09-21 [1] CRAN (R 4.3.1)\n#>  rprojroot      2.0.4      2023-11-05 [1] CRAN (R 4.3.0)\n#>  rstatix        0.7.2      2023-02-01 [1] CRAN (R 4.3.0)\n#>  rstudioapi     0.15.0     2023-07-07 [1] CRAN (R 4.3.0)\n#>  rversions      2.1.2      2022-08-31 [1] CRAN (R 4.3.0)\n#>  sandwich       3.1-0      2023-12-11 [1] CRAN (R 4.3.0)\n#>  scales         1.3.0      2023-11-28 [1] CRAN (R 4.3.2)\n#>  sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n#>  shiny          1.8.0      2023-11-17 [1] CRAN (R 4.3.0)\n#>  sjlabelled     1.2.0      2022-04-10 [1] CRAN (R 4.3.0)\n#>  sjmisc         2.8.9      2021-12-03 [1] CRAN (R 4.3.0)\n#>  sjPlot         2.8.15     2023-08-17 [1] CRAN (R 4.3.0)\n#>  sjstats        0.18.2     2022-11-19 [1] CRAN (R 4.3.0)\n#>  skimr          2.1.5      2022-12-23 [1] CRAN (R 4.3.0)\n#>  snakecase      0.11.1     2023-08-27 [1] CRAN (R 4.3.0)\n#>  stringi        1.8.3      2023-12-11 [1] CRAN (R 4.3.0)\n#>  stringr        1.5.1      2023-11-14 [1] CRAN (R 4.3.0)\n#>  survival       3.5-8      2024-02-14 [2] CRAN (R 4.3.2)\n#>  svglite        2.1.3      2023-12-08 [1] CRAN (R 4.3.0)\n#>  systemfonts    1.0.5      2023-10-09 [1] CRAN (R 4.3.0)\n#>  TH.data        1.1-2      2023-04-17 [1] CRAN (R 4.3.0)\n#>  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#>  tidyr          1.3.1      2024-01-24 [1] CRAN (R 4.3.2)\n#>  tidyselect     1.2.0      2022-10-10 [1] CRAN (R 4.3.0)\n#>  timechange     0.3.0      2024-01-18 [1] CRAN (R 4.3.0)\n#>  utf8           1.2.4      2023-10-22 [1] CRAN (R 4.3.0)\n#>  vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n#>  viridisLite    0.4.2      2023-05-02 [1] CRAN (R 4.3.0)\n#>  visdat         0.6.0      2023-02-02 [1] CRAN (R 4.3.0)\n#>  withr          3.0.0      2024-01-16 [1] CRAN (R 4.3.0)\n#>  xfun           0.42       2024-02-08 [1] CRAN (R 4.3.2)\n#>  xml2           1.3.6      2023-12-04 [1] CRAN (R 4.3.0)\n#>  xtable         1.8-4      2019-04-21 [1] CRAN (R 4.3.0)\n#>  yaml           2.3.8      2023-12-11 [1] CRAN (R 4.3.0)\n#>  zoo            1.8-12     2023-04-13 [1] CRAN (R 4.3.0)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.3-x86_64/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}