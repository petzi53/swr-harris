{
  "hash": "2d4645ec3d2437411a6fb2b78dfdfb29",
  "result": {
    "engine": "knitr",
    "markdown": "# Correlation coefficient {#sec-chap08}\n\n\n\n\n\n## Achievements to unlock\n\n::: my-objectives\n::: my-objectives-header\nObjectives\n:::\n\n::: my-objectives-container\n**SwR Achievements**\n\n-   **Achievement 1**: Exploring the data using graphics and descriptive\n    statistics (@sec-chap08-achievement1).\n-   **Achievement 2**: Computing and interpreting Pearson’s *r*\n    correlation coefficient (@sec-chap08-achievement2).\n-   **Achievement 3**: Conducting an inferential statistical test for\n    Pearson’s *r* correlation coefficient (@sec-chap08-achievement3).\n-   **Achievement 4**: Examining effect size for Pearson’s *r* with the\n    coefficient of determination (@sec-chap08-achievement4).\n-   **Achievement 5**: Checking assumptions for Pearson’s *r*\n    correlation analyses (@sec-chap08-achievement5).\n-   **Achievement 6**: Transforming the variables as an alternative when\n    Pearson’s *r* correlation assumptions are not met\n    (@sec-chap08-achievement6).\n-   **Achievement 7**: Using Spearman’s rho as an alternative when\n    Pearson’s *r* correlation assumptions are not met\n    (@sec-chap08-achievement7).\n-   **Achievement 8**: Introducing partial correlations\n    (@sec-chap08-achievement8).\n:::\n:::\n\n## The clean water conundrum\n\n-   Women and girls tend to be responsible for collecting water for\n    their families, often walking long distances in unsafe areas and\n    carrying heavy loads.\n-   In some cultures, lack of access to sanitation facilities also means\n    that women can only defecate after dark, which can be physically\n    uncomfortable and/or put them at greater risk for harassment and\n    assault.\n-   The lack of sanitation facilities can keep girls out of school when\n    they are menstruating.\n\n**Goals**\n\n1.  With data from a few different sources examining the relationship\n    between the percentage of people in a country with water access and\n    the percentage of school-aged girls who are in school.\n2.  With data exploring the relationship between the percentage of\n    females in school and the percentage of people living on less than\n    \\$1 per day.\n\n## Resources & Chapter Outline\n\n### Data, codebook, and R packages {#sec-chap08-data-codebook-packages}\n\n::: my-resource\n::: my-resource-header\nData, codebook, and R packages for learning about descriptive statistics\n:::\n\n::: my-resource-container\n**Data**\n\nTwo options:\n\n1.  Download the `water_educ_2015_who_unesco_ch8.csv` and\n    `2015-outOfSchoolRate-primarySecondary-ch8.xlsx` data sets from\n    <https://edge.sagepub.com/harris1e>.\n2.  Follow the instructions in Box 8.1 to import and clean the data\n    directly from the original Internet sources. Please note that the\n    WHO makes small corrections to past data occasionally, so use of\n    data imported based on Box 8.1 instructions may result in minor\n    differences in results throughout the chapter. To match chapter\n    results exactly, use the data provided.\n\n::: my-note\n::: my-note-header\nUsing data provided from the book\n:::\n\n::: my-note-container\nI have learned a lot about data cleaning procedures in the last\nchapters. I feel secure and decided from now on that I will take data\nprovided by the book. This help me to focus my attention on the\nstatistical subjects of the book.\n:::\n:::\n\n**Codebook**\n\nTwo options:\n\n1.  Download the codebook file `opioid_county_codebook.xlsx` from\n    <https://edge.sagepub.com/harris1e>.\n2.  Use the online version of the codebook from the amfAR Opioid &\n    Health Indicators Database website (https://opioid.amfar.org)\n\n**Packages**\n\n1.  Packages used with the book (sorted alphabetically) (Install the\n    following R packages if not already installed.)\n\n-   {**tidyverse**}: @pak-tidyverse (Hadley Wickham)\n-   {**readxl**}: @pak-readxl (Jennifer Bryan)\n-   {**lmtest**}: @pak-lmtest (Achim Zeileis)\n-   {**rcompanion**}: @pak-rcompanion (Salvatore Mangiafico)\n-   {**ppcor**}: @pak-ppcor (Seongho Kim)\n\n2.  My additional packages (sorted alphabetically)\n:::\n:::\n\n### Get data\n\n::: my-example\n::: my-example-header\n::: {#exm-chap08-get-data}\n: Get data for chapter 8\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### water-educ\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-get-water-educ-data}\n: Get Water-Education data\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n## run only once (manually)\nwater_educ <- readr::read_csv(\n    file = \"data/chap08/water_educ_2015_who_unesco_ch8.csv\",\n    show_col_types = FALSE\n    )\n\nsave_data_file(\"chap08\", water_educ, \"water_educ.rds\")\n```\n:::\n\n:::\n:::\n\n###### header2\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-code-name-b}\n: Numbered R Code Title (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n1 + 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 2\n```\n\n\n:::\n:::\n\n:::\n:::\n:::\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Show raw data\n\n::: my-example\n::: my-example-header\n::: {#exm-chap08-show-data}\n: Show data for chapter 8\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### water-educ\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-show-water-educ}\n: Show Water & Education data\n:::\n:::\n\n::: my-r-code-container\n\n::: {#tbl-show-water-educ .cell tbl-cap='Show descriptive data from the Water-Edcuation UNESCO file'}\n\n```{.r .cell-code}\nwater_educ <- base::readRDS(\"data/chap08/water_educ.rds\")\n\nwater_educ |> \n    skimr::skim()\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |water_educ |\n|Number of rows           |97         |\n|Number of columns        |10         |\n|_______________________  |           |\n|Column type frequency:   |           |\n|character                |1          |\n|numeric                  |9          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|country       |         0|             1|   4|  52|     0|       97|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable       | n_missing| complete_rate|  mean|    sd|    p0|   p25|   p50|    p75|   p100|hist  |\n|:-------------------|---------:|-------------:|-----:|-----:|-----:|-----:|-----:|------:|------:|:-----|\n|med.age             |         0|          1.00| 30.33|  8.69| 15.00| 22.50| 29.70|  39.00|  45.90|▆▇▇▆▇ |\n|perc.1dollar        |        33|          0.66| 13.63| 20.52|  1.00|  1.00|  1.65|  17.12|  83.80|▇▁▁▁▁ |\n|perc.basic2015sani  |         0|          1.00| 79.73| 27.18|  7.00| 73.00| 93.00|  99.00| 100.00|▁▁▁▁▇ |\n|perc.safe2015sani   |        47|          0.52| 71.50| 25.84|  9.00| 61.25| 76.50|  93.00| 100.00|▂▂▁▆▇ |\n|perc.basic2015water |         1|          0.99| 90.16| 15.82| 19.00| 88.75| 97.00| 100.00| 100.00|▁▁▁▁▇ |\n|perc.safe2015water  |        45|          0.54| 83.38| 22.34| 11.00| 73.75| 94.00|  98.00| 100.00|▁▁▁▂▇ |\n|perc.in.school      |         0|          1.00| 87.02| 13.94| 33.32| 83.24| 92.02|  95.81|  99.44|▁▁▁▂▇ |\n|female.in.school    |         0|          1.00| 87.06| 15.10| 27.86| 83.70| 92.72|  96.61|  99.65|▁▁▁▂▇ |\n|male.in.school      |         0|          1.00| 87.00| 12.95| 38.66| 82.68| 91.50|  95.57|  99.36|▁▁▁▂▇ |\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nInstead of `base::summary()` I used `skimr::skim()` which fives more\ndescriptive information.\n\n**Codebook**\n\n-   **country**: the name of the country\n-   **med.age**: the median age of the citizens in the country\n-   **perc.1dollar**: percentage of citizens living on \\$1 per day or\n    less\n-   **perc.basic2015sani**: percentage of citizens with basic sanitation\n    access\n-   **perc.safe2015san**i: percentage of citizens with safe sanitation\n    access\n-   **perc.basic2015water**: percentage of citizens with basic water\n    access\n-   **perc.safe2015water**: percentage of citizens with safe water\n    access\n-   **perc.in.school**: percentage of school-age people in primary and\n    secondary school\n-   **female.in.school**: percentage of female school-age people in\n    primary and secondary school\n-   **male.in.school**: percentage of male school-age people in primary\n    and secondary school\n:::\n:::\n\n###### header2\n\n::: my-r-code\n::: my-r-code-header\n<div>\n\n: Numbered R Code Title (Tidyverse)\n\n</div>\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n1 + 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 2\n```\n\n\n:::\n:::\n\n:::\n:::\n:::\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Recode data\n\n## Exploring data {#sec-chap08-achievement1}\n\nThe two variables of interests are:\n\n-   female.in.school and\n-   perc.basic2015water\n\n::: my-example\n::: my-example-header\n::: {#exm-chap08-exploring-data}\n: Exploring data for chapter 8\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### mean & sd\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-mean-sd-water-educ}\n: Mean and standard deviation for `female.in.school` and\n`perc.basic2015water`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n    skimr::skim(c(female.in.school, perc.basic2015water)\n    )\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |water_educ |\n|Number of rows           |97         |\n|Number of columns        |10         |\n|_______________________  |           |\n|Column type frequency:   |           |\n|numeric                  |2          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: numeric**\n\n|skim_variable       | n_missing| complete_rate|  mean|    sd|    p0|   p25|   p50|    p75|   p100|hist  |\n|:-------------------|---------:|-------------:|-----:|-----:|-----:|-----:|-----:|------:|------:|:-----|\n|female.in.school    |         0|          1.00| 87.06| 15.10| 27.86| 83.70| 92.72|  96.61|  99.65|▁▁▁▂▇ |\n|perc.basic2015water |         1|          0.99| 90.16| 15.82| 19.00| 88.75| 97.00| 100.00| 100.00|▁▁▁▁▇ |\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThe mean percent of school-aged females in school was 87.06 (sd = 15.1),\nand the mean percent of citizens who had basic access to water was 90.16\n(sd = 15.82).\n\nThis is a pretty high percentage. The very high median shows that there\nis a heavy left-skewed distribution. 93 & 97% are in the first half of\nthe distribution located!\n\n::: my-note\n::: my-note-header\nAdvantages of the `skimr::skim()` function\n:::\n\n::: my-note-container\nThis above summary show the advantage of the `skimr::skim()` function\nversus the `base::summary()` resp. the extra calculation of mean and sd.\n`skimr::skim()` is (a) easier to use (just one line!) and (b) displays\nmuch more information, e.g., different percentiles with a small\nhistogram. Important here is, for instance, that we can compare mean and\nmedian in one step.\n:::\n:::\n:::\n:::\n\n###### scatterplot1\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-scatterplot-female-water}\n: Scatterplot of `female.in.school` and `perc.basic2015water`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = female.in.school / 100,\n            y = perc.basic2015water / 100\n        )\n    ) +\n    ggplot2::geom_point(\n        na.rm = TRUE,\n        ggplot2::aes(\n            color = \"Country\"                \n        ), \n        size = 2.5,\n        alpha = 0.3\n    ) +\n    ggplot2::labs(\n        x = \"Percent with basic water access\",\n        y = \"Percent of school-aged females in school\" \n    ) +\n    ggplot2::scale_color_manual(\n        name = \"\",\n        values = \"purple3\"\n    ) +\n    ggplot2::scale_x_continuous(\n        labels = scales::label_percent()\n    ) +\n    ggplot2::scale_y_continuous(\n        labels = scales::percent\n    )\n```\n\n::: {.cell-output-display}\n![Relationship of percentage of females in school and percentage of citizens with basic water access in countries worldwide](08-correlation_files/figure-html/fig-scatterplot-female-water-1.png){#fig-scatterplot-female-water width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nI have used two different argument styles for the percent scale from the\n{**scales**} package (see: @pak-scales):\n\n-   `labels = scales::percent` as in the book\n-   `labels = scales::label_percent()` from the help file of the\n    {**scales**} package.\n:::\n:::\n\n###### scatterplot2\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-scatterplot-female-dollar}\n: Scatterplot of `female.in.school` and `perc.1dollar`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = perc.1dollar / 100,\n            y = female.in.school / 100\n        )\n    ) +\n    ggplot2::geom_jitter(\n        na.rm = TRUE,\n        ggplot2::aes(\n            color = \"Country\"                \n        ),\n        size = 2.5,\n        alpha = 0.3\n    ) +\n    ggplot2::labs(\n        x = \"Percent of people living on less than $1 per day\", \n        y = \"Percent with basic water access\"\n    ) +\n    ggplot2::scale_color_manual(\n        name = \"\",\n        values = \"purple3\"\n    ) +\n    ggplot2::scale_x_continuous(\n        labels = scales::label_percent()\n    ) +\n    ggplot2::scale_y_continuous(\n        labels = scales::percent\n    )\n```\n\n::: {.cell-output-display}\n![Relationship of percentage of females in school and percentage of people living on less than $1 per day in countries worldwide](08-correlation_files/figure-html/fig-scatterplot-female-dollar-1.png){#fig-scatterplot-female-dollar width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n:::\n:::\n:::\n:::\n:::\n\n## Pearson’s *r* correlation coefficient {#sec-chap08-achievement2}\n\n### Introduction\n\nOne method of measuring the relationship between two continuous variable\nis <a class='glossary' title='Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together. (Statistics How To(https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/covariance/))'>covariance</a>, which quantifies\nwhether two variables vary together (co-vary).\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-chap08-covariance}\n: Formula for covariance\n:::\n:::\n\n::: my-theorem-container\n$$\ncov_{xy} = \\sum_{i=1}^{n}\\frac{(x_{i}-m_{x})(y_{i}-m_{y})}{n-1}\n$$ {#eq-chap08-covariance}\n:::\n:::\n\nThe numerator essentially adds up how far each observation is away from\nthe mean values of the two variables being examined, so this ends up\nbeing a very large number quantifying how far away all the observations\nare from the mean values. The denominator divides this by\n<a class='glossary' title='Bessel’s correction is the use of n − 1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance. It also partially corrects the bias in the estimation of the population standard deviation. However, the correction often increases the mean squared error in these estimations. This technique is named after Friedrich Bessel. (Wikipedia)'>Bessel’s correction</a> (@sec-chap04-clt) of $n – 1$, which\nis close to the sample size and essentially finds the average deviation\nfrom the means for each observation.\n\nI skipped Figure 8.4 and 8.5 because they do not bring any news for me.\n(Note that there is a wrong label for x-axis in Figure 8.5: Instead of\n\"Percent living on less than \\$1 per day\" it says wrongly \"Percent with\nbasic water access\".)\n\n### Missing values\n\nThe covariance function `stats::cov()` is like the `base::mean()`\nfunction in that it cannot handle NA values. As we are going to\ncalculate `female.in.school` with `perc.basic2015water` and\n`female.in.school` with `perc.1dollar` we would have three different\nvariables with NA's.\n\nIt is important not to to remove all rows with missing data of all three\nvariables at the same time because that would delete more rows as for\neach pair of variable would be necessary. We know from\n@tbl-show-water-educ that\n\n-   `female.in.school` has no missing values\n-   `perc.basic2015water` has 1 missing value\n-   `perc.1dollar` has 33 missing values\n\nThere are two options:\n\na)  To use two different covariance calculations, each time with the\n    appropriate `tidyr::drop_na()` function as used finally in the book.\nb)  To apply the appropriate `use` argument of the `stats::cov()`\n    function for each calculations, which I will use and which was the\n    first try in the book.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-cov-female-water-pov}\n: Covariance of females in school and percentage with basic access to\ndrinking water\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n  dplyr::summarize(\n      cov_females_water = stats::cov(\n          x = perc.basic2015water,\n          y = female.in.school,\n          use = \"pairwise.complete.obs\",\n          method = \"pearson\"\n          ),\n      cov_females_pov = stats::cov(\n          x = perc.1dollar,\n          y = female.in.school,\n          use = \"pairwise.complete.obs\",\n          method = \"pearson\")\n      )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 2\n#>   cov_females_water cov_females_pov\n#>               <dbl>           <dbl>\n#> 1              194.           -203.\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThe book argument for NA's is `use = \"complete\"` which is an allowed\nabbreviation for `use = \"complete.obs\"`. I have employed\n`use = \"pairwise.complete.obs\"` which is a more precise argument but\nworks only for the (default) \"pearson\" method.\n:::\n:::\n\n### Interpretation\n\nThe covariance does not have an intuitive inherent meaning; it is not a\npercentage or a sum or a difference. In fact, the size of the covariance\ndepends largely on the size of what is measured. For example, something\nmeasured in millions might have a covariance in the millions or hundreds\nof thousands. The value of the covariance indicates whether there is a\nrelationship at all and the direction of the relationship --- that is,\nwhether the relationship is positive or negative.\n\nIn this case, a nonzero value indicates that there is some relationship.\nIn the first case (`cov_females_water`) it is a positive relationship;\nin the second case (`cov_females_pov`) it is a negative relationship.\nThe size of the numbers are irrelevant!\n\nTherefore <a class='glossary' title='In statistics, standardization (also called Normalizing) is the process of putting different variables on the same scale. This process allows you to compare scores between different types of variables. Typically, to standardize variables, you calculate the mean and standard deviation for a variable. Then, for each observed value of the variable, you subtract the mean and divide by the standard deviation. (Statistics by Jim) See scale() in R.  (Chap.4)'>standardization</a> by dividing by the\n<a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a> of the two involved variables is\nnecessary. The result is called the\n<a class='glossary' title='Correlation coefficients are a standardized measure of how two variables are related, or co-vary. They are used to measure how strong a relationship is between two variables. There are several types of correlation coefficient, but the most popular is Pearson’s. Pearson’s correlation (also called Pearson’s R) is a correlation coefficient commonly used in linear regression. (Statistics How To)'>correlation coefficient</a> and is referred\nto as *r*.\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-chap08-pearson-r}\n: Computing the Pearson *r* correlation between two variables\n:::\n:::\n\n::: my-theorem-container\n$$\n\\begin{align*}\nr_{xy} = \\frac{cov_{xy}}{s_{x}s_{y}} \\\\\nr_{xy} = \\sum_{i = 1}^{n}\\frac{z_{x}z_{y}}{n-1}\n\\end{align*}\n$$ {#eq-chap08-pearson-r}\n\n------------------------------------------------------------------------\n\nThe second line is also know as the product-moment correlation\ncoefficient. The formula for *r* can be organized in many different\nways, one of which is as the mean of the summed products of\n<a class='glossary' title='A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (StatisticsHowTo)'>z-scores</a>.\n\n::: my-assessment\n::: my-assessment-header\n::: {#cor-chap08-pearson-r}\n: Range of Pearson’s *r* and interpretation of strength\n:::\n:::\n\n::: my-assessment-container\n-   **-1: Negative correlations** occur when one variable goes up and\n    the other goes down.\n-   **0: No correlation** happens when there is no discernable pattern\n    in how two variables vary.\n-   **+1: Positive correlations** occur when one variable goes up, and\n    the other one also goes up (or when one goes down, the other one\n    does too).\n\n------------------------------------------------------------------------\n\n-   **r = –1.0** is perfectly negative\n-   **r = –.8** is strongly negative\n-   **r = –.5** is moderately negative\n-   **r = –.2** is weakly negative\n-   **r = 0** is no relationship\n-   **r = .2** is weakly positive\n-   **r = .5** is moderately positive\n-   **r = .8** is strongly positive\n-   **r = 1.0** is perfectly positive\n:::\n:::\n:::\n:::\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap08-correlation}\n: Compute and show correlation\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### compute cor()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-cor-water-pov-female}\n: Compute correlations for water access, poverty and female education\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ <-  base::readRDS(\"data/chap08/water_educ.rds\")\n\nwater_educ |> \n  dplyr::summarize(\n     cor_females_water = cor(\n         x = perc.basic2015water,\n         y = female.in.school,\n         use = \"complete.obs\"\n         ),\n     cor.females.pov = cor(\n         x = perc.1dollar,\n         y = female.in.school,\n         use = \"complete.obs\"\n         )\n     )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 2\n#>   cor_females_water cor.females.pov\n#>               <dbl>           <dbl>\n#> 1             0.809          -0.714\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n###### graph1 cor\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-graph1-cor}\n: Display correlation water access and female education with `lm` and `loess` smoother with a special constructed legend\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = female.in.school/100, \n          x = perc.basic2015water/100\n          )\n      ) +\n  ggplot2::geom_smooth(\n      ggplot2::aes(color = \"Linear fit line\"),\n      formula = y ~ x,\n      method = \"lm\",\n      se = FALSE, \n      na.rm = TRUE\n      ) +\n    ggplot2::geom_smooth(\n      ggplot2::aes(color = \"Loess line\"),\n      formula = y ~ x,\n      method = \"loess\",\n      se = FALSE, \n      na.rm = TRUE\n      ) +\n  ggplot2::geom_point(\n      ggplot2::aes(size = \"Country\"), \n      color = \"#7463AC\", \n      alpha = .6,\n      na.rm = TRUE\n      ) +\n  ggplot2::labs(\n      y = \"Percent of school-aged females in school\",\n      x = \"Percent with basic water access\"\n      ) +\n  ggplot2::scale_x_continuous(labels = scales::percent) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::scale_color_manual(\n      values = c(\"gray60\", \"darkred\"), \n      name = \"\"\n      ) +      \n  ggplot2::scale_size_manual(values = 2, name = \"\")\n```\n\n::: {.cell-output-display}\n![Display correlation water access and female education with `lm` and `loess` smoother with a special constructed legend](08-correlation_files/figure-html/fig-graph1-cor-1.png){#fig-graph1-cor width=672}\n:::\n:::\n\n***\n\n**`ggplot2::geom_smooth()` layer**\n\n- The formula argument would not be necessary, because the program assumes y ~ x for fewer than 1000 observations.\n- If I haven't specified the method with `lm` than the default value would have been chosen, e.g. (depending on fewer than 1000) which is a local polynomial regression fitting.\n- To show the difference I had used both `method = lm` and in another layer `method = loess`. The <a class='glossary' title='Loess curve is a graph curve that shows the relationship between two variables without constraining the line to be straight; it can be compared to a linear fit line to determine whether the relationship is close to linear or not (= checking the [linearity] assumption). The procedure originated as LOWESS (LOcally WEighted Scatter-plot Smoother). is a nonparametric method because the linearity assumptions of conventional regression methods have been relaxed. It is called local regression because the fitting at say point x is weighted toward the data nearest to x. (SwR, Glossary and LOESS Curve Fitting (Local Polynomial Regression))'>Loess curve</a> results in the slightly curved line (the red curve). Instead of fitting the whole data at once (= \"lm\"), method \"loess\" creates a local regression because the fitting at say point x is weighted toward the data nearest to x and not to the general mean.\n\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Legends are generated from attributes inside the `ggplot2::aes()` statement \n:::\n::::{.my-watch-out-container}\nIt is important to know: \n\n- If all aesthetics are determined outside the `ggplot2::aes()` functions then there is not legend generated. \n- The name of the aesthetics are arbitrary and result as labels inside the legend. \n\nIn this case I have used twice the \"color\" aesthetic, but as value I gave as argument was the type of line and not an actual color. The actual color for the lines you will fin in the `ggplot2::scale_color_manual()` layer at the very bottom of the code. \n\nSee also the next two graphs (@fig-graph2-cor and @fig-graph3-cor) about water access and female education where I have explored different types of points and lines inside the aesthetic function.\n::::\n:::::\n\n\n\n\n\n::::\n:::::\n\n###### graph2 cor\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-graph3-cor}\n: Display correlation water access and female education with two legends explaining what the different symbols represent\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = female.in.school/100, \n          x = perc.basic2015water/100\n          )\n      ) +\n  ggplot2::geom_smooth(\n      ggplot2::aes(color = \"Linear fit line\"),\n      formula = y ~ x,\n      method = \"lm\",\n      se = FALSE, \n      na.rm = TRUE\n      ) +\n  ggplot2::geom_point(\n      ggplot2::aes(size = \"Country\"), \n      color = \"#7463AC\", \n      alpha = .6,\n      na.rm = TRUE\n      ) +\n  ggplot2::labs(\n      y = \"Percent of school-aged females in school\",\n      x = \"Percent with basic water access\"\n      ) +\n  ggplot2::scale_x_continuous(labels = scales::percent) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::scale_color_manual(values = \"gray60\", name = \"Legend 2\") +\n  ggplot2::scale_size_manual(values = 2, name = \"Legend 1\")\n```\n\n::: {.cell-output-display}\n![Display correlation water access and female education with two legends explaining what the different symbols represent](08-correlation_files/figure-html/fig-graph2-cor-1.png){#fig-graph2-cor width=672}\n:::\n:::\n\n***\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Legends are generated from attributes inside the `ggplot2::aes()` statement \n:::\n::::{.my-watch-out-container}\nThe two `ggplot2::aes()` functions used for this graph are `ggplot2::aes(size = \"Country\")` and `ggplot2::aes(linetype = \"Linear fit line\")`. To get two different legends (point and lines), two different attributes were used within the `aes()`. \n::::\n:::::\n\n\n\n\n\n::::\n:::::\n\n###### graph3 cor\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-graph3-cor}\n: Display correlation water access and female education with a legend explaining what the different symbols represent\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = female.in.school/100, \n          x = perc.basic2015water/100\n          )\n      ) +\n  ggplot2::geom_smooth(\n      ggplot2::aes(color = \"Linear fit line\"),\n      formula = y ~ x,\n      method = \"lm\",\n      se = FALSE, \n      na.rm = TRUE\n      ) +\n  ggplot2::geom_point(\n      ggplot2::aes(color = \"Country\"), \n      size = 2, \n      alpha = .6,\n      na.rm = TRUE\n      ) +\n  ggplot2::labs(\n      y = \"Percent of school-aged females in school\",\n      x = \"Percent with basic water access\"\n      ) +\n  ggplot2::scale_x_continuous(labels = scales::percent) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::scale_color_manual(\n      name = \"Legend\",\n      values = c(\"#7463AC\", \"gray60\") \n      )\n```\n\n::: {.cell-output-display}\n![Display correlation water access and female education with a legend explaining what the different symbols represent](08-correlation_files/figure-html/fig-graph3-cor-1.png){#fig-graph3-cor width=672}\n:::\n:::\n\n***\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! The name of the attribute inside the `aes()` is arbitrary\n:::\n::::{.my-watch-out-container}\n@fig-graph3-cor has the color attribute for both the points and the line within `aes()` and so both colors are included in the only legend.\n\nThe name of the attribute inside the `aes()` is arbitrary and will result in the **label of the legend**. The type of this attribute has to be addressed and specified with the correct manual scale (`ggplot2::scale_xxx_manual()`) and will display the appropriate symbol for the attribute.\n\n**ATTENTION**: With new versions of {**ggplot2**} the symbols are not merged as in the book’s version. This would have been not correct, because the line does not go through all points. Points and lines are different aesthetics but they are merged under on legend with one common attribute, their color.\n::::\n:::::\n\n::: {.callout-tip}\nThe Pearson’s product-moment correlation coefficient demonstrated that the percentage of females in school is positively correlated with the percentage of citizens with basic access to drinking water (r = 0.81). Essentially, as access to water goes up, the percentage of females in school also increases in countries.\n:::\n\n::::\n:::::\n\n###### graph4 cor\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-graph4-cor}\n: Display relationship of percentage of citizens living on less than $1 per day and the percent of school-aged females in school in countries worldwide\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = female.in.school/100, \n          x = perc.1dollar/100\n          )\n      ) +\n  ggplot2::geom_smooth(\n      ggplot2::aes(color = \"Linear fit line\"),\n      formula = y ~ x,\n      method = \"lm\",\n      se = FALSE, \n      na.rm = TRUE\n      ) +\n  ggplot2::geom_point(\n      ggplot2::aes(color = \"Country\"), \n      size = 2, \n      alpha = .6,\n      na.rm = TRUE\n      ) +\n  ggplot2::labs(\n      y = \"Percent of school-aged females in school\",\n      x = \"Percent of citizens living on less than $1 per day\"\n      ) +\n  ggplot2::scale_x_continuous(labels = scales::percent) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::scale_color_manual(\n      name = \"\",\n      values = c(\"#7463AC\", \"gray60\") \n      )\n```\n\n::: {.cell-output-display}\n![Display correlation of percentage of citizens living on less than $1 per day and the percent of school-aged females in school in countries worldwide](08-correlation_files/figure-html/fig-graph4-cor-1.png){#fig-graph4-cor width=672}\n:::\n:::\n\n***\n\n\n\n::::\n:::::\n\n::: {.callout-tip}\nThe Pearson’s product-moment correlation coefficient demonstrated that the percentage of females in school is negatively correlated with the percentage of citizens living on less than $1 per day (r = -0.71). Essentially, as the percentage of citizens living on less than $1 per day goes up, the percentage of females in school decreases in countries.\n:::\n\n:::\n\n::::\n:::::\n\n## Achievement 3: Inferential statistical test for Pearson’s r {#sec-chap08-achievement3}\n\n\n### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: {.callout-note}\n- **H0**: There is no relationship between the two variables (r = 0).\n- **HA**: There is a relationship between the two variables (r ≠ 0).\n:::\n\n### NHST Step 2\n\nCompute the test statistic. \n\nThe null hypothesis is tested using a <a class='glossary' title='The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (Statistics How-To)'>t-statistic</a> comparing the <a class='glossary' title='Correlation coefficients are a standardized measure of how two variables are related, or co-vary. They are used to measure how strong a relationship is between two variables. There are several types of correlation coefficient, but the most popular is Pearson’s. Pearson’s correlation (also called Pearson’s R) is a correlation coefficient commonly used in linear regression. (Statistics How To)'>correlation coefficient of r</a> to a hypothesized value of zero.\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap08-cor-test}\n: t-statistic for the significance test of r\n::::::\n:::\n::::{.my-theorem-container}\n\n$$\n\\begin{align*}\nt = \\frac{r_{xy}}{se_{r_{xy}}} \\\\\nse = \\sqrt\\frac{1-r_{xy}^2}{n-2} \\\\\nt = \\frac{r_{xy}}{\\sqrt\\frac{1-r_{xy}^2}{n-2}} =\\\\\nt = \\frac{r_{xy}\\sqrt{n-2}}{\\sqrt{1-r_{xy}^2}}\n\\end{align*}\n$$ {#eq-chap08-cor-test}\n::::\n:::::\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-ID-text}\n: Compute t-statistic for the significance test of r\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### manual\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-cor-test-manual}\n: Compute t-statistic for the significance test of *r* manually\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_data <- water_educ |> \n  tidyr::drop_na(perc.basic2015water)  |> \n  tidyr::drop_na(female.in.school) |> \n  dplyr::summarize(\n      cor_females_water = cor(\n          x = perc.basic2015water,\n          y = female.in.school\n          ),\n      sample_n = dplyr::n()\n      )\n\n(test_data$cor_females_water * (sqrt(test_data$sample_n - 2))) /\n    (sqrt(1 - (test_data$cor_females_water^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 13.32774\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n###### cor.test()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-cor-test}\n: Compute t-statistic for the significance test of *r* with `stats::cor.test()`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cor.test(x = water_educ$perc.basic2015water,\n#          y = water_educ$female.in.school)\n\n# using instead the formula interface\ncor.test(\n    formula = ~ female.in.school + perc.basic2015water,\n    data = water_educ\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tPearson's product-moment correlation\n#> \n#> data:  female.in.school and perc.basic2015water\n#> t = 13.328, df = 94, p-value < 2.2e-16\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.7258599 0.8683663\n#> sample estimates:\n#>       cor \n#> 0.8086651\n```\n\n\n:::\n:::\n\n***\nI have used the formula interface because it has a different syntax as I thought. My first trials were with `female.in.school ~ perc.basic2015water` but this didn't work. The (last) example in the help page demonstrated to me the other syntax.\n\nNote that it is not necessary to remove NA’s before applying `cor.test()` in both cases.\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n***\n\n### NHST Step 3\n\nReview and interpret the test statistics: \nCalculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\nThe very tiny p-value is statistically significant. \n\n### NHST Step 4\n\nConclude and write report.\n\n::: {.callout-tip}\nThe percentage of people who have basic access to water is statistically significantly, positively, and very strongly correlated with the percentage of primary- and secondary-age females in school in a country [r = .81; t(94) = 13.33; p < .05]. As the percentage of people living with basic access to water goes up, the percentage of females in school also goes up. While the correlation is .81 in the sample, it is likely between .73 and .87 in the population (95% CI: .73–.87).\n:::\n\n## Achievement 4: Coefficient of determiniation as effect size {#sec-chap08-achievement4}\n\nPearson’r is already a kind of effect size because it measures the strength of a relationship. But with the <a class='glossary' title='Coefficient of determination is the percentage of variance in one variable that is accounted for by another variable or by a group of variables; often referred to as R-squared and used to determine model fit for linear models. (SwR, Glossary)'>coefficient of determination</a> $R^2$ (also $r^2$) there is another effect size measure with a more direct interpretation. The coefficient of determination is the percentage of the variance in one variable that is shared, or explained, by the other variable.\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap08-formula-r-squared}\n: Computing the coefficient of determination $R^2$\n::::::\n:::\n::::{.my-theorem-container}\n$$\nr_{xy}^2 = (\\frac{cov_{xy}}{s_{x}s_{y}})^2\n$$ {#eq-chap08-r-squared}\n::::\n:::::\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-compute-r-squared}\n: Compute r-squared ($R^2$)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n(stats::cor.test(\n    x = water_educ$perc.basic2015water, \n    y = water_educ$female.in.school)$estimate\n)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       cor \n#> 0.6539392\n```\n\n\n:::\n:::\n\n***\n\nThe `stats::cor.test()` function creates an object of type `htest` which is a list of 9 different object. One of these object is the numeric vector `estimate` that holds the correlation value. There are two option to calculate r-squared:\n\n1. Assign the result of `stats::cor.test()` function to a named object. Append `$estimate^2` to this object to get r-squared. I have this done in one step, and appended `$estimate^2` at the end of the function without providing an interim object.\n2. You could calculate the correlation with `stats:cor()` or `stats::cor.test()` and then take the result and square it to get r-squared. But this method is more error-prone.\n\n\n::::\n:::::\n\n## Achievement 5: Checking assumptions for Pearson’s r {#sec-chap08-achievement5}\n\n### Introduction\n\n***\n\n::: {#bul-assumptions-pearson-r}\n\n- Observations are independent (@sec-check-independence). \n- Both variables are continuous (@sec-chap08-check-continuous). \n- Both variables are normally distributed (@sec-chap08-check-normality).\n- The relationship between the two variables is linear (<a class='glossary' title='Linearity is the assumption of some statistical models that requires the outcome, or transformed outcome, to have a linear relationship with numeric predictors, where linear relationships are relationships that are evenly distributed around a line. (SwR, Glossary)'>linearity</a>) (@sec-chap08-check-linearity). \n- The variance is constant with the points distributed equally around the line (<a class='glossary' title='Homoscedasticity is [homogeneity of variances], contrast is [Heteroscedasticity]. Homoscedasticity is an assumption of correlation and linear regression that requires that the variance of y be constant across all the values of x; visually, this assumption would show points along a fit line between x and y being evenly spread on either side of the line for the full range of the relationship. (SwR, Glossary)'>homoscedasticity</a>) {@sec-chap08-homoscedasticity).\n\nAssumptions for Pearson’s r\n\n:::\n\n### Observations are independent {#sec-check-independence}\n\nSo far the book had mentioned siblings and other family members or testing the same individuals several time as examples for not independent observations. Now we got two more examples:\n\n- Countries that are geographically close to each other, or that are in the same geographic region, may be more likely to share characteristics and therefore fail this assumption.\n- Countries in the analysis were those reporting data on the variables of interest, rather than a random sample of countries. Countries reporting data may be different from countries missing data. For example, they may have better computing infrastructure and more human and financial resources to afford to collect, store, and report data.\n\n### Continuous variables {#sec-chap08-check-continuous}\n\nBoth variables need to be of type `numeric`. In our case we have the number of countries as integer variable: Counting something is integer, measuring something is continuous. But in our case it can be treated statistically like a continuous variable.\n\nThe same is true with percent values, but there are some worries how to model percentages statistically. \n\n> A couple of … papers suggested that percentage variables are problematic for statistical models that have the purpose of predicting values of the outcome because predictions can fall outside the range of 0 to 100.\n\n\n:::::{.my-resource}\n:::{.my-resource-header}\nDealing with percentage data\n:::\n::::{.my-resource-container}\n- Logistic regression [@zhao2001] \n- Beta regression [@schmid2013; @cribari-neto2010; @ferrari2004]\n- Transforming the percentage \n- Recoding the variable to categorical and using a nonparametric method like <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a>.\n::::\n:::::\n\n\n### Normality {#sec-chap08-check-normality}\n\nComparing <a class='glossary' title='Histograms are visual displays of data used to examine the distribution of a numeric variable. (SwR, Glossary)'>histograms</a> and <a class='glossary' title='A quantile-quantile plot is a visualization of data using probabilities to show how closely a variable follows a normal distribution. (SwR, Glossary) This plot is made up of points below which a certain percentage of the observations fall. On the x-axis are normally distributed values with a mean of 0 and a standard deviation of 1. On the y-axis are the observations from the data. If the data are normally distributed, the values will form a diagonal line through the graph. (SwR, chapter 6)'>Q-Q plots</a> is one of the most applied techniques to test the normality assumption. I am also using histograms with an overlaid normal distribution and have an extra function developed for this recurring task.\n\nI will provide all three different graphs here one again, although I have already understood and memorized these practices.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap08-normality-assumption}\n: Checking the normality assumption\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### hist female\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-normality-female-hist}\n: Check normality of `female.in.school` variable\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ  |> \n  ggplot2::ggplot(\n      ggplot2::aes(x = female.in.school / 100)) +\n  ggplot2::geom_histogram(\n      fill = \"#7463AC\", \n      col = \"white\",\n      bins = 30,\n      na.rm = TRUE\n  ) +\n  ggplot2::labs(\n      x = \"Percent of school-aged females in school\",\n      y = \"Number of countries\"\n  ) +\n  ggplot2::scale_x_continuous(\n      labels = scales::percent\n  )\n```\n\n::: {.cell-output-display}\n![Distribution of percentage of school-aged females in school](08-correlation_files/figure-html/fig-normality-female-hist-1.png){#fig-normality-female-hist width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\n###### dnorm female\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-normality-female-hist-dnorm}\n: Percentage of school-aged females in school with an overlaid normal distribution\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist_dnorm(\n    df = water_educ,\n    v = water_educ$female.in.school / 100,\n    n_bins = 30,\n    x_label = \"Percent of school-aged females in school\"\n    ) +\n  ggplot2::scale_x_continuous(labels = scales::percent)\n```\n\n::: {.cell-output-display}\n![Distribution of percentage of school-aged females in school](08-correlation_files/figure-html/fig-normality-female-hist-dnorm-1.png){#fig-normality-female-hist-dnorm width=672}\n:::\n:::\n\n\n::::\n:::::\n\n###### abline()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-normality-female-qqplot-abline}\n: Comparison of the distribution of school-aged females in school with the theoretical normal distribution\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ  |> \n  ggplot2::ggplot(\n      ggplot2::aes(sample = female.in.school)\n  ) +\n  ggplot2::stat_qq(\n      ggplot2::aes(color = \"Country\"),\n      alpha = .6\n  ) +\n  ggplot2::geom_abline(\n      ggplot2::aes(\n          intercept = base::mean(female.in.school),\n          slope = stats::sd(female.in.school),\n          linetype = \"Normally distributed\"\n      ),\n          color = \"gray60\",\n          linewidth = 1\n  ) +\n  ggplot2::labs(\n      x = \"Theoretical normal distribution\",\n      y = \"Observed values of percent of\\nschool-aged females in school\",\n      title = \"Q-Q plot of female.in.school with `geom_abline()` and `ylim()`\") +\n  ggplot2::ylim(0,100) +\n  ggplot2::scale_linetype_manual(values = \"solid\", name = \"\") +\n  ggplot2::scale_color_manual(values = \"purple4\", name = \"\")\n```\n\n::: {.cell-output-display}\n![Q-Q-Plot: Distribution of school-aged females in school compared with the theoretical normal distribution](08-correlation_files/figure-html/fig-normality-female-qqplot-abline-1.png){#fig-normality-female-qqplot-abline width=672}\n:::\n:::\n\n***\n\nThis graph is the replication of Figure 8.15. It uses `ggplot2::geom_abline()` by calculating the mean as intercept and the slop as standard deviation. This is more complex as the `ggplot2::geom_qq_line()` resp. `ggplot2::stat_qq_line()` but has the advantage that the legend displays the line symbol with the same slope.\n\nA more simple alternative is `ggplot2::geom_qq_line()` resp. `ggplot2::stat_qq_line()` because these commands compute automatically the slope and intercept of the line connecting the points at specified quartiles of the theoretical and sample distributions. I have this more simple approach already used when I checked the <a class='glossary' title='A t-test is a type of statistical analysis used to compare the averages of two groups and determine whether the differences between them are more likely to arise from random chance. (Wikipedia)'>t-test</a> assumptions in @sec-chap06-achievement6. \n\nBut here we are using percentages, e.g. we need to limit the y-axis to values between 0 and 100%. And this restrictions prevents to show the line of the theoretical normal distribution. \n\n\n\n\n\n\n::::\n:::::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Do not forget, that the required aesthetic for the q-q-plot is \"sample\" and not \"x\"!\n:::\n:::::\n\n###### stat_qq_line()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-normality-female-qqplot-stat_qq-line}\n: Comparison of the distribution of school-aged females in school with the theoretical normal distribution\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- water_educ  |> \n  ggplot2::ggplot(\n      ggplot2::aes(sample = female.in.school)\n  ) +\n  ggplot2::stat_qq(\n      ggplot2::aes(color = \"Country\"),\n      alpha = .6\n  ) +\n  ggplot2::stat_qq_line(\n      ggplot2::aes(linetype = \"Normally distributed\"),\n     linewidth = 1,\n     color = \"grey60\",\n     fullrange = TRUE\n  ) +\n  ggplot2::labs(\n      x = \"Theoretical normal distribution\",\n      y = \"Observed values of percent of\\nschool-aged females in school\",\n      title = \"Q-Q plot of female.in.school\\nwith `stat__qq_line()` witht `ylim()`\") +\n  ggplot2::ylim(0,100) +\n  ggplot2::scale_linetype_manual(values = \"solid\", name = \"\") +\n  ggplot2::scale_color_manual(values = \"purple4\", name = \"\") +\n  ggplot2::theme(legend.position = \"top\")\n\n\np2 <- water_educ  |> \n  ggplot2::ggplot(\n      ggplot2::aes(sample = female.in.school)\n  ) +\n  ggplot2::stat_qq(\n      ggplot2::aes(color = \"Country\"),\n      alpha = .6\n  ) +\n  ggplot2::stat_qq_line(\n      ggplot2::aes(linetype = \"Normally distributed\"),\n     linewidth = 1,\n     color = \"grey60\",\n     fullrange = TRUE\n  ) +\n  ggplot2::labs(\n      x = \"Theoretical normal distribution\",\n      y = \"Observed values of percent of\\nschool-aged females in school\",\n      title = \"Q-Q plot of female.in.school\\nwith `stat__qq_line()` without `ylim()`\") +\n  ggplot2::scale_linetype_manual(values = \"solid\", name = \"\") +\n  ggplot2::scale_color_manual(values = \"purple4\", name = \"\") +\n  ggplot2::theme(legend.position = \"top\")\n\ngridExtra::grid.arrange(\n    p2, p1, ncol = 2\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Removed 1 row containing missing values or values outside the scale range\n#> (`geom_path()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> `geom_path()`: Each group consists of only one observation.\n#> ℹ Do you need to adjust the group aesthetic?\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Q-Q-Plot: Distribution of school-aged females in school compared with the theoretical normal distribution](08-correlation_files/figure-html/fig-normality-female-qqplot-stat_qq-line-1.png){#fig-normality-female-qqplot-stat_qq-line width=672}\n:::\n:::\n\n***\n\n::: {.callout-warning}\n- Each group consists of only one observation. Do you need to adjust the aesthetic?\n- Removed 1 row containing missing values or values outside the scale range (`geom_path()`).\n:::\n\n- The left panel didn't use the `ggplot2::ylim(0, 100)` restriction. It display the line for the theoretical normal distribution far outside the upper limit.\n- The right panel used the ylim restriction but failed to show the line for the theoretical normal distribution and displays two warnings.\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\nThere is nothing new when checking the normality assumption for basic water access. So I will skip these two graphs.\n\n### Linearity {#sec-chap08-check-linearity}\n\nThe linearity assumption requires that the relationship between the two variables falls along a line. For instance this assumption is met in @fig-graph2-cor. If it is difficult to tell, a <a class='glossary' title='Loess curve is a graph curve that shows the relationship between two variables without constraining the line to be straight; it can be compared to a linear fit line to determine whether the relationship is close to linear or not (= checking the [linearity] assumption). The procedure originated as LOWESS (LOcally WEighted Scatter-plot Smoother). is a nonparametric method because the linearity assumptions of conventional regression methods have been relaxed. It is called local regression because the fitting at say point x is weighted toward the data nearest to x. (SwR, Glossary and LOESS Curve Fitting (Local Polynomial Regression))'>Loess curve</a> can be added to confirm linearity as I have done it in @fig-graph1-cor.\n\nIt is instructive to see relationships that are non-linear. The next graph shows some relationships but they fall along curves instead of along straight lines.\n\n![Examples for nonlinear relationships](img/chap08/Nonlinear-relationships-min.png){#fig-nonlinear-relations\nfig-alt=\"The two variables seen in these two graphs are labeled x and y and are on the x and y axes respectively. Both graphs have a linear fit line as well as a less curve. The graph on the left titled 1, has x axis values that range from -10 to 5, in intervals of 5. The values on the y axis range from -100, to 100, in intervals of 50. The linear fit line in this graph is a horizontal line at the y axis value of about 30. The loess curve joins the data points in this graph in a U-shape with the midpoint at about (0, 0). The graph on the right titled 2, has x axis values that range from -10 to 5, in intervals of 5. The values on the y axis range from -100, to 100, in intervals of 50. The linear fit line in this graph is an upward-sloping line that starts at about (-65, -10) and ends at about (65, 10). The loess curve joins the data points in this graph in a curve that starts at about (-100, -10), rises sharply until about (-2.5, 0), and is parallel to the x axis until about (0.5, 0) and rises sharply again until about (10, 100). The loess curve intersects the linear fit line at three points, including at (0,0).\"\nfig-align=\"center\"}\n\n### Homoscedasticity {#sec-chap08-homoscedasticity}\n\nAnother assumption is the equal distribution of points around the line, which is often called the assumption of <a class='glossary' title='Homoscedasticity is [homogeneity of variances], contrast is [Heteroscedasticity]. Homoscedasticity is an assumption of correlation and linear regression that requires that the variance of y be constant across all the values of x; visually, this assumption would show points along a fit line between x and y being evenly spread on either side of the line for the full range of the relationship. (SwR, Glossary)'>homoscedasticity</a>.\n\nBesides a visual graphical inspection the <a class='glossary' title='Breusch-Pagan is a statistical test for determining whether variance is constant, which is used to test the assumption of homoscedasticity; Breusch-Pagan relies on the [chi-squared] distribution and is used during assumption checking for [homoscedasticity] in [linear regression]. (SwR, Glossary)'>Breusch-Pagan test</a> could be used to test the null hypothesis that the variance is constant around the line. The Breusch-Pagan test relies on the <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a> distribution, and the `lmtest::bptest()` function can be found in the {**lmtest**} package (see @pak-lmtest).\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap08-test-homoscedasticity}\n: Check if the homoscedasticity assumption is met\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### graph\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-test-homoscedasticity-graph}\n: Examine graphically if the equal distribution of points around the line (homoscedasticity assumption) is met\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplt <- water_educ |> \n  ggplot2::remove_missing(\n        na.rm = TRUE,\n        vars = c(\"perc.basic2015water\", \"female.in.school\")\n  ) |> \n  ggplot2::ggplot(\n        ggplot2::aes(\n            y = female.in.school/100, \n            x = perc.basic2015water/100\n          )\n  ) +\n  ggplot2::geom_point(\n      ggplot2::aes(\n          size = \"Country\"\n          ), \n      color = \"purple4\", \n      alpha = .6\n  ) +\n  ggplot2::geom_smooth(\n      formula = y ~ x,\n      ggplot2::aes(\n          linetype = \"Linear fit line\"\n          ),\n      color = \"grey60\",\n      method = \"lm\",\n      se = FALSE\n      ) +\n  ggplot2::geom_segment(\n      ggplot2::aes(\n          linetype = \"homoscedasticity check\"\n      ),\n      y = 57 / 100, x = 17 / 100,\n      xend = 97 / 100, yend = 100 / 100,\n      linewidth = 0.5,\n      color = \"grey60\"\n  ) +\n    ggplot2::geom_segment(\n      ggplot2::aes(\n          linetype = \"homoscedasticity check\"\n      ),\n      x = 72 / 100, y = 25 / 100,\n      xend = 100 / 100, yend = 80 / 100,\n      linewidth = 0.5,\n      color = \"grey60\"\n  ) +\n  ggplot2::labs(\n      y = \"Percent of school-aged females in school\",\n      x = \"Percent with basic access to water\") +\n  ggplot2::scale_x_continuous(labels = scales::percent) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::scale_color_manual(\n      values = c(\"gray60\", \"darkred\"), name = \"\") +\n  ggplot2::scale_size_manual(values = 2, name = \"\") +\n  ggplot2::scale_linetype_manual(values = c(2, 1), name = \"\")\n\nbase::suppressWarnings(base::print(plt))\n```\n\n::: {.cell-output-display}\n![Check if the homoscedasticity assumption is met](08-correlation_files/figure-html/fig-homoscedasticity-graph-1.png){#fig-homoscedasticity-graph width=672}\n:::\n:::\n\n***\n\nThis is the replication of Figure 8.20 of the book, that had no accompanying R code. I have applied trial and error for the `geom_segment()` layer. Later I noticed that I could have used the figures of the last paragraph of the fig-alt description.\n\nThe funnel shape of the data indicated that the points were not evenly spread around the line from right to left. On the left of the graph they were more spread out than on the right, where they were very close to the line. This indicates the data do not meet the homoscedasticity assumption.\n\n\n\n\n::::\n:::::\n\n::: {.callout-warning}\nI suppressed two warning from {**ggplot2**}, one for each `ggplot2::geom_segment()` layer:\n\n> All aesthetics have length 1, but the data has 96 rows. Did you mean to use `annotate()`?\n:::\n\n###### Breusch-Pagan\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-test-homoscedasticity-breusch-pagan}\n: Check homoscedasticity with the Breusch-Pagan test\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmtest::bptest(\n    formula = water_educ$female.in.school ~ water_educ$perc.basic2015water)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tstudentized Breusch-Pagan test\n#> \n#> data:  water_educ$female.in.school ~ water_educ$perc.basic2015water\n#> BP = 12.368, df = 1, p-value = 0.0004368\n```\n\n\n:::\n:::\n\n\n***\n\nThe Breusch-Pagan test statistic has a low p-value (BP = 12.37; p = 0.0004), indicating that the null hypothesis that the variance is constant would be rejected, e.g., the homoscedasticity assumption is not met.\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n### Conclusion\n\n***\n\n::: {#bul-assumptions-pearson-r-summary}\n\n- Observations are independent (@sec-check-independence): **No**. \n- Both variables are continuous (@sec-chap08-check-continuous): **Yes**. \n- Both variables are normally distributed (@sec-chap08-check-normality): **No**.\n- The relationship between the two variables is linear (<a class='glossary' title='Linearity is the assumption of some statistical models that requires the outcome, or transformed outcome, to have a linear relationship with numeric predictors, where linear relationships are relationships that are evenly distributed around a line. (SwR, Glossary)'>linearity</a>) (@sec-chap08-check-linearity): **Yes**. \n- The variance is constant with the points distributed equally around the line (<a class='glossary' title='Homoscedasticity is [homogeneity of variances], contrast is [Heteroscedasticity]. Homoscedasticity is an assumption of correlation and linear regression that requires that the variance of y be constant across all the values of x; visually, this assumption would show points along a fit line between x and y being evenly spread on either side of the line for the full range of the relationship. (SwR, Glossary)'>homoscedasticity</a>) {@sec-chap08-homoscedasticity): **No**.\n\nSummary of testing the assumptions for Pearson’s r in the example data\n:::\n\n***\n\nThe big question is: What can be done that several of the assumptions are not met? The books gives some words of advice:\n\n- Report the results and explain that the analysis does not meet assumptions, so that it is unclear if what is happening in the sample is a good reflection of what is happening in the population. \n- Transform the two variables to try and meet the assumptions for Pearson’s r and conduct the analysis again.\n- Choose a different type of analysis with assumptions that can be met by these data. \n\n:::::{.my-note}\n:::{.my-note-header}\nMy opinion to the three tips\n:::\n::::{.my-note-container}\n- The first advice is no solution. This strategy declares that the inferential process has failed.\n- Yes, this is a promising strategy. More on this in the next section.\n- I do not know what the third advice means. What kind of analysis is meant?\n::::\n:::::\n\n## Achievement 6: Transforming data {#sec-chap08-achievement6}\n\n### Introduction\n\nOne of the ways to deal with data that do not meet assumptions for Pearson’s *r* is to use a data transformation and examine the relationship between the transformed variables.\n\n***\n\n::: {#bul-data-transformations}\n\n- <a class='glossary' title='Nonlinear transformations are transformations that increases (or decreases) the linear relationship between two variables by applying an exponent (i.e., [power transformation]) or other function to one or both of the variables. (SwR, Glossary)'>Linear transformations</a> keep existing linear relationships between variables, often by multiplying or dividing one or both of the variables by some amount. \n- <a class='glossary' title='Nonlinear transformations are transformations that increases (or decreases) the linear relationship between two variables by applying an exponent (i.e., [power transformation]) or other function to one or both of the variables. (SwR, Glossary)'>Nonlinear transformations</a> increase (or decrease) the linear relationship between two variables by applying an exponent (i.e., <a class='glossary' title='Power transformations are transformations of a measure using an exponent like squaring or cubing or taking the square root or cube root; power transformations are nonlinear transformations. (SwR, Glossary)'>power transformations</a>) or other function to one or both of the variables.\n- <a class='glossary' title='Logit transformations are transformations that takes the log value of p/(1-p); this transformation is often used to normalize percentage data and is used in the logistic model to transform the outcome. (SwR, Glossary)'>Logit transformations</a> and <a class='glossary' title='Arcsine transformation are data transformation techniques often recommended to normalize percent or proportion data; the arcsine transformation uses the inverse of the sine function and the square root of the variable to transform. (SwR, Glossary)'>arcsine transformations</a> are often used for variables that are percentages or proportions because they account for <a class='glossary' title='A floor effect happens when a variable has many observations that take the lowest value of the variable, which can indicate that the range of values was insufficient to capture the true variability of the data. (SwR, Glossary)'>floor</a> and <a class='glossary' title='A ceiling effect happens when many observations are at the highest possible value for a variable. (SwR, Glossary)'>ceiling</a> effects.\n\n\nTypes of data transformations\n\n:::\n\n***\n\n### Logit transformation\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap08-logit-transformation}\n: Formula for the logit transformation\n::::::\n:::\n::::{.my-theorem-container}\n$$\ny_{logit} = log(\\frac{y}{1-y})\n$$ {#eq-chap08-logit}\n\n***\n\ny is a percent ranging from 0 to 1.\n\nThe logit transformation uses @eq-chap08-logit to make percentage data more normally distributed.\n::::\n:::::\n\n\n### Arcsine transformation\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap08-arcsine}\n: Formula for the arcsine transformation\n::::::\n:::\n::::{.my-theorem-container}\n$$\ny_{arcsine} = arcsine(\\sqrt{y})\n$$ {#eq-chap08-arcsine}\n\nThe arcsine transformation e.g., the inverse of the sine function, is also used to normalize percentage or proportion data by using @eq-chap08-arcsine to transform the variable $y$.\n\n***\n::::\n:::::\n\n### Folded power transformation\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap08-folded-power}\n: Formula for the folded power transformation\n::::::\n:::\n::::{.my-theorem-container}\n$$\ny_{folded.power} = y^\\frac{1}{p} - (1-y)^\\frac{1}{p}\n$$ {#eq-chap08-folded-power}\n\n***\n\n$p$ is the power to raise that could be calculated with `rcompanion::transformTukey()` (see @pak-rcompanion).\n::::\n:::::\n\n### Data transforming and checking normality assumption {#sec-chap08-transformed-normality}\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap08-transform-data}\n: Transforming data and checking normality assumptions\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### logit & arcsine\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-transform-logit-arcsine}\n: Logit and arcsine transformation of variables `female.in.school` & `perc.basic2015water`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ2 <- water_educ  |> \n  dplyr::mutate(\n      logit.female.school = base::log(\n          x = (female.in.school / 100) / (1 - female.in.school / 100)\n          )\n    ) |> \n  dplyr::mutate(\n      logit.perc.basic.water = base::log(\n          x = (perc.basic2015water / 100) / (1 - perc.basic2015water / 100)\n          )\n      )  |> \n  \n  dplyr::mutate(\n      arcsin.female.school = asin(\n          x = base::sqrt(female.in.school / 100)\n          )\n      )  |> \n  dplyr::mutate(\n      arcsin.perc.basic.water = asin(\n          x = base::sqrt(perc.basic2015water/100)\n          )\n      )\n\nsave_data_file(\"chap08\", water_educ2, \"water_educ2.rds\")\n\n# check the data\nskimr::skim(water_educ2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: There was 1 warning in `dplyr::summarize()`.\n#> ℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n#>   mangled_skimmers$funs)`.\n#> ℹ In group 0: .\n#> Caused by warning:\n#> ! There was 1 warning in `dplyr::summarize()`.\n#> ℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n#>   mangled_skimmers$funs)`.\n#> Caused by warning in `inline_hist()`:\n#> ! Variable contains Inf or -Inf value(s) that were converted to NA.\n```\n\n\n:::\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |            |\n|:------------------------|:-----------|\n|Name                     |water_educ2 |\n|Number of rows           |97          |\n|Number of columns        |14          |\n|_______________________  |            |\n|Column type frequency:   |            |\n|character                |1           |\n|numeric                  |13          |\n|________________________ |            |\n|Group variables          |None        |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|country       |         0|             1|   4|  52|     0|       97|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable           | n_missing| complete_rate|  mean|    sd|    p0|   p25|   p50|    p75|   p100|hist  |\n|:-----------------------|---------:|-------------:|-----:|-----:|-----:|-----:|-----:|------:|------:|:-----|\n|med.age                 |         0|          1.00| 30.33|  8.69| 15.00| 22.50| 29.70|  39.00|  45.90|▆▇▇▆▇ |\n|perc.1dollar            |        33|          0.66| 13.63| 20.52|  1.00|  1.00|  1.65|  17.12|  83.80|▇▁▁▁▁ |\n|perc.basic2015sani      |         0|          1.00| 79.73| 27.18|  7.00| 73.00| 93.00|  99.00| 100.00|▁▁▁▁▇ |\n|perc.safe2015sani       |        47|          0.52| 71.50| 25.84|  9.00| 61.25| 76.50|  93.00| 100.00|▂▂▁▆▇ |\n|perc.basic2015water     |         1|          0.99| 90.16| 15.82| 19.00| 88.75| 97.00| 100.00| 100.00|▁▁▁▁▇ |\n|perc.safe2015water      |        45|          0.54| 83.38| 22.34| 11.00| 73.75| 94.00|  98.00| 100.00|▁▁▁▂▇ |\n|perc.in.school          |         0|          1.00| 87.02| 13.94| 33.32| 83.24| 92.02|  95.81|  99.44|▁▁▁▂▇ |\n|female.in.school        |         0|          1.00| 87.06| 15.10| 27.86| 83.70| 92.72|  96.61|  99.65|▁▁▁▂▇ |\n|male.in.school          |         0|          1.00| 87.00| 12.95| 38.66| 82.68| 91.50|  95.57|  99.36|▁▁▁▂▇ |\n|logit.female.school     |         0|          1.00|  2.46|  1.31| -0.95|  1.64|  2.54|   3.35|   5.66|▂▅▇▆▂ |\n|logit.perc.basic.water  |         1|          0.99|   Inf|   NaN| -1.45|  2.07|  3.48|    Inf|    Inf|▁▅▅▇▇ |\n|arcsin.female.school    |         0|          1.00|  1.24|  0.20|  0.56|  1.16|  1.30|   1.39|   1.51|▁▁▂▇▇ |\n|arcsin.perc.basic.water |         1|          0.99|  1.34|  0.25|  0.45|  1.23|  1.40|   1.57|   1.57|▁▁▂▃▇ |\n\n\n:::\n:::\n\n\n***\n\nWe got several warnings. We can easily see that our new variable `logit.perc.basic.water` has problems because it contains infinite values. The reason is the formula for the logit function, that has as denominator $1-y$. When $y = 1$ for 100%, the denominator is zero and it is impossible to divide by zero. \n\nThe intuitive idea to change the original data slightly, e.g., to subtract a very tiny amount so that the results is not zero anymore, is a bad idea. It destroys the reproducibility and adds error into the dataset. A better solution is to try instead another transformation.\n\nThe suggestion is to use the folded power transformation (@thm-chap08-folded-power). But before we can apply the formula we need to compute the power for the transforming of the variables.\n\n\n::::\n:::::\n\n\n###### transformTukey1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-transform-tukey}\n: Tukey transformation to get power for transforming the variables `female.in.school` & `perc.basic2015water`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## use Tukey transformation to get power for transforming\n## female in school variable to more normal distribution\np_female <- rcompanion::transformTukey(\n    x = water_educ$female.in.school,\n    plotit = FALSE,\n    quiet = TRUE,\n    returnLambda = TRUE\n    )\n\n\n# use Tukey transformation to get power for transforming\n# basic 2015 water variable to more normal distribution\np_water <- rcompanion::transformTukey(\n    x = water_educ$perc.basic2015water,\n    plotit = FALSE,\n    quiet = TRUE,\n    returnLambda = TRUE\n    )\n```\n:::\n\n***\n\nUsing the Tukey transformation we get as power for transforming for the \n\n- female.in.school variable: 8.85 and for the\n- perc.basic2015water variabe: 9.975.\n::::\n:::::\n\n###### transformTukey2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-transform-tukey}\n: Tukey transformation to get power for transforming the variables `female.in.school` & `perc.basic2015water` with accompanying plots\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_female <- rcompanion::transformTukey(\n    x = water_educ$female.in.school,\n    plotit = TRUE,\n    quiet = TRUE,\n    returnLambda = TRUE\n    )\n\np_water <- rcompanion::transformTukey(\n    x = water_educ$perc.basic2015water,\n    plotit = TRUE,\n    quiet = TRUE,\n    returnLambda = TRUE,\n    statistic = 2\n    )\n```\n\n::: {.cell-output-display}\n![Tukey transformation to get plots of Shapiro-Wilks W or Anderson-Darling A vs. lambda, a histogram of transformed values, and a quantile-quantile plot of transformed values.](08-correlation_files/figure-html/fig-transform-folded-power-1.png){#fig-transform-folded-power-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Tukey transformation to get plots of Shapiro-Wilks W or Anderson-Darling A vs. lambda, a histogram of transformed values, and a quantile-quantile plot of transformed values.](08-correlation_files/figure-html/fig-transform-folded-power-2.png){#fig-transform-folded-power-2 width=672}\n:::\n\n::: {.cell-output-display}\n![Tukey transformation to get plots of Shapiro-Wilks W or Anderson-Darling A vs. lambda, a histogram of transformed values, and a quantile-quantile plot of transformed values.](08-correlation_files/figure-html/fig-transform-folded-power-3.png){#fig-transform-folded-power-3 width=672}\n:::\n\n::: {.cell-output-display}\n![Tukey transformation to get plots of Shapiro-Wilks W or Anderson-Darling A vs. lambda, a histogram of transformed values, and a quantile-quantile plot of transformed values.](08-correlation_files/figure-html/fig-transform-folded-power-4.png){#fig-transform-folded-power-4 width=672}\n:::\n\n::: {.cell-output-display}\n![Tukey transformation to get plots of Shapiro-Wilks W or Anderson-Darling A vs. lambda, a histogram of transformed values, and a quantile-quantile plot of transformed values.](08-correlation_files/figure-html/fig-transform-folded-power-5.png){#fig-transform-folded-power-5 width=672}\n:::\n\n::: {.cell-output-display}\n![Tukey transformation to get plots of Shapiro-Wilks W or Anderson-Darling A vs. lambda, a histogram of transformed values, and a quantile-quantile plot of transformed values.](08-correlation_files/figure-html/fig-transform-folded-power-6.png){#fig-transform-folded-power-6 width=672}\n:::\n:::\n\n***\n\nAfter changing the argument from `plotit = FALSE` to the default value of `plotit = TRUE` we get different plots for the normality assumption. \n\n- The first three graphs are plots for <a class='glossary' title='The Shapiro-Wilk test is a statistical test to determine or confirm whether a variable has a normal distribution; it is sensitive to small deviations from normality and not useful for sample sizes above 5,000 because it will nearly always find non-normality. (SwR, Glossary)'>Shapiro-Wilks W</a> vs. lambda with <a class='glossary' title='Histograms are visual displays of data used to examine the distribution of a numeric variable. (SwR, Glossary)'>histogram</a> and <a class='glossary' title='A quantile-quantile plot is a visualization of data using probabilities to show how closely a variable follows a normal distribution. (SwR, Glossary) This plot is made up of points below which a certain percentage of the observations fall. On the x-axis are normally distributed values with a mean of 0 and a standard deviation of 1. On the y-axis are the observations from the data. If the data are normally distributed, the values will form a diagonal line through the graph. (SwR, chapter 6)'>quantile-quantile plot</a> for the `female.in.school` variable (argument: `statistic = 1`, the default value).\n- The last three graphs are plots for <a class='glossary' title='The Anderson-Darling Goodness of Fit Test (AD-Test) is a measure of how well your data fits a specified distribution. It’s commonly used as a test for normality. (Statistics How-To)'>Anderson-Darling A</a> vs. lambda with histogram and Q-Q-plot for the `perc.basic2015water` variable (argument: `statistic = 2`).\n\nFor more information about Shapiro-Wilk and Anderson-Darling tests see @sec-chap06-omnibus-tests.\n\n::::\n:::::\n\n###### power\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-folded-power-transformation}\n: Compute variable with folded power transformation\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## create new transformation variables\nwater_educ3 <- water_educ  |> \n  dplyr::mutate(\n      arcsin.female.school = \n          base::asin(x = base::sqrt(female.in.school/100))\n      )  |> \n  dplyr::mutate(\n      arcsin.perc.basic.water = \n          base::asin(x = base::sqrt(perc.basic2015water/100))\n      ) |> \n  dplyr::mutate(\n      folded.p.female.school = \n          (female.in.school/100)^(1/p_female) - \n          (1-female.in.school/100)^(1/p_female)\n      )  |> \n  dplyr::mutate(\n      folded.p.basic.water = \n          (perc.basic2015water/100)^(1/p_water) - \n          (1-perc.basic2015water/100)^(1/p_water)\n      )\n\nsave_data_file(\"chap08\", water_educ3, \"water_educ3.rds\")\n\n# check the data\nskimr::skim(water_educ3)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |            |\n|:------------------------|:-----------|\n|Name                     |water_educ3 |\n|Number of rows           |97          |\n|Number of columns        |14          |\n|_______________________  |            |\n|Column type frequency:   |            |\n|character                |1           |\n|numeric                  |13          |\n|________________________ |            |\n|Group variables          |None        |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|country       |         0|             1|   4|  52|     0|       97|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable           | n_missing| complete_rate|  mean|    sd|    p0|   p25|   p50|    p75|   p100|hist  |\n|:-----------------------|---------:|-------------:|-----:|-----:|-----:|-----:|-----:|------:|------:|:-----|\n|med.age                 |         0|          1.00| 30.33|  8.69| 15.00| 22.50| 29.70|  39.00|  45.90|▆▇▇▆▇ |\n|perc.1dollar            |        33|          0.66| 13.63| 20.52|  1.00|  1.00|  1.65|  17.12|  83.80|▇▁▁▁▁ |\n|perc.basic2015sani      |         0|          1.00| 79.73| 27.18|  7.00| 73.00| 93.00|  99.00| 100.00|▁▁▁▁▇ |\n|perc.safe2015sani       |        47|          0.52| 71.50| 25.84|  9.00| 61.25| 76.50|  93.00| 100.00|▂▂▁▆▇ |\n|perc.basic2015water     |         1|          0.99| 90.16| 15.82| 19.00| 88.75| 97.00| 100.00| 100.00|▁▁▁▁▇ |\n|perc.safe2015water      |        45|          0.54| 83.38| 22.34| 11.00| 73.75| 94.00|  98.00| 100.00|▁▁▁▂▇ |\n|perc.in.school          |         0|          1.00| 87.02| 13.94| 33.32| 83.24| 92.02|  95.81|  99.44|▁▁▁▂▇ |\n|female.in.school        |         0|          1.00| 87.06| 15.10| 27.86| 83.70| 92.72|  96.61|  99.65|▁▁▁▂▇ |\n|male.in.school          |         0|          1.00| 87.00| 12.95| 38.66| 82.68| 91.50|  95.57|  99.36|▁▁▁▂▇ |\n|arcsin.female.school    |         0|          1.00|  1.24|  0.20|  0.56|  1.16|  1.30|   1.39|   1.51|▁▁▂▇▇ |\n|arcsin.perc.basic.water |         1|          0.99|  1.34|  0.25|  0.45|  1.23|  1.40|   1.57|   1.57|▁▁▂▃▇ |\n|folded.p.female.school  |         0|          1.00|  0.23|  0.12| -0.10|  0.17|  0.25|   0.31|   0.47|▂▂▆▇▂ |\n|folded.p.basic.water    |         1|          0.99|  0.48|  0.39| -0.13|  0.18|  0.29|   1.00|   1.00|▃▇▂▁▇ |\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n\n###### graphs\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-check-normality-transformed-data-graphs}\n: Check the normality assumptions of the transformed data with histograms\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ3 <- base::readRDS(\"data/chap08/water_educ3.rds\")\n\n# histogram of arcsin females in school (Figure 8.21)\nplt1 <- hist_dnorm(\n    df = water_educ3,\n    v = water_educ3$arcsin.female.school,\n    n_bins = 30,\n    x_label = \"Arcsine transformation of females in school\"\n    )\n\n# histogram of folded power transf females in school (Figure 8.22)\nplt2 <- hist_dnorm(\n    df = water_educ3,\n    v = water_educ3$folded.p.female.school,\n    n_bins = 30,\n    x_label = \"Folded power transformation of females in school\"\n    )\n\n# histogram of arcsine of water variable (Figure 8.23)\nplt3 <- hist_dnorm(\n    df = water_educ3,\n    v = water_educ3$arcsin.perc.basic.water,\n    n_bins = 30,\n    x_label = \"Arcsine transformed basic water access\"\n    )\n\n# histogram of folded power transformed water variable (Figure 8.24)\nplt4 <- hist_dnorm(\n    df = water_educ3,\n    v = water_educ3$folded.p.basic.water,\n    n_bins = 30,\n    x_label = \"Folded power transformed basic water access\"\n    )\n\ngridExtra::grid.arrange(\n    plt1, plt2, plt3, plt4, nrow = 2\n)\n```\n\n::: {.cell-output-display}\n![Histograms for checking normality assumptions of the transformed data](08-correlation_files/figure-html/fig-check-normality-transformed-data-graphs-1.png){#fig-check-normality-transformed-data-graphs width=672}\n:::\n:::\n\n***\n\n- **Top-Left**: The arcsine transformation of females in school looks better than the not transformed data in @fig-normality-female-hist-dnorm. But still it is not a normal but a left-skewed distribution.\n- **Top-Right**: The folded power transformation looks much better. It is still somewhat left-skewed but approaches quite well a normal distribution.\n- **Bottom-Left**: The arcsine transformation of basic water access looks terrible. \n- **Bottom-Right**: The folded power transformation of basic water access looks not better, maybe even worse. \n\nThe book suggests that the `perc.basic2015water` variable should better be recoded into categories. Since so many countries have 100% access, the variable could be binary, with 100% access in one category and less than 100% access in another category.\n\nAlthough `perc.basic2015water` did not meet the normality assumption the book applies the <a class='glossary' title='Null Hypothesis Significance Testing (NHST) is a process for organizing inferential statistical tests. (SwR, Glossary)'>NHST</a> procedure. I think the reason was just to practice the procedure because in my opinion it would not make sense to apply a significance test for correlation if several of the assumptions are not met.\n\n\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n\n### Testing assumptions for Pearson’s *r* with transformed data\n\n#### Normality\n\nThis assumption is not met, see the different graphs in @sec-chap08-transformed-normality.\n\n#### Linearity {#sec-chap08-linearity-transformed}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-check-linearity-transformed}\n: Check linearity assumption with transformed data with linear fit line and Loess curve\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n# explore plot of transformed females in school and basic water\n# with linear fit line and Loess curve (Figure 8.25)\nwater_educ3 |> \n  tidyr::drop_na(c(\n      folded.p.female.school,\n      folded.p.basic.water\n  )) |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = folded.p.female.school, \n          x = folded.p.basic.water)\n      ) +\n  ggplot2::geom_smooth(\n      formula = y ~x,\n      ggplot2::aes(\n          color = \"linear fit line\"\n          ), \n      method = \"lm\", \n      se = FALSE\n      ) +\n  ggplot2::geom_smooth(\n      formula = y ~x,\n      ggplot2::aes(\n          color = \"Loess curve\"\n          ), \n      method = \"loess\",\n      se = FALSE\n      ) +\n  ggplot2::geom_point(\n      ggplot2::aes(\n          size = \"Country\"\n          ), \n      color = \"#7463AC\", \n      alpha = .6\n      ) +\n  ggplot2::labs(\n      y = \"Power transformed percent of females in school\",\n      x = \"Power transformed percent with basic water access\"\n      ) +\n  ggplot2::scale_color_manual(\n      name = \"Type of fit line\", \n      values = c(\"gray60\",\"darkred\")) +\n  ggplot2::scale_size_manual(values = 2)\n```\n\n::: {.cell-output-display}\n![Check linearity assumption with transformed data](08-correlation_files/figure-html/fig-check-linearity-transformed-1.png){#fig-check-linearity-transformed width=672}\n:::\n:::\n\n***\nThe plot shows a pretty terrible deviation from linearity, which looks like it is mostly due to all the countries with 100% basic water access. A indicator for this guess is that both lines are bent by the right vertical line of countries with 100% basic water access. Without the lines would end around 0.45x / 0.5y.\n\n\n::::\n:::::\n\n\n#### Homoscedasticity {#sec-chap08-homoscedasticity-transformed}\n\n### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: {.callout-note}\n- **H0**: The data is spread equal around the regression line.\n- **HA**: The data is not spread equal around the regression line.\n:::\n\n\n### NHST Step 2\n\nCompute the test statistic. \n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap08-check-homoscedasticity-transformed}\n: Check homoscedasticity assumption with transformed data\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n# testing for homoscedasticity\nlmtest::bptest(\n    formula = \n        water_educ3$folded.p.female.school ~ \n        water_educ3$folded.p.basic.water\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tstudentized Breusch-Pagan test\n#> \n#> data:  water_educ3$folded.p.female.school ~ water_educ3$folded.p.basic.water\n#> BP = 6.3816, df = 1, p-value = 0.01153\n```\n\n\n:::\n:::\n\n\n\n\n::::\n:::::\n\n### NHST Step 3\n\nReview and interpret the test statistics: \nCalculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\nThe p-value of .01 is below .05 therefore statistically significant. \n\n### NHST Step 4\n\nConclude and write report.\n\nWith a p-value of .01, the null hypothesis is rejected and the assumption fails. The data transformation worked to mostly address the problem of normality for the females in school variable, but the transformed data were not better for linearity or homoscedasticity.\n\n::: {.callout-tip}\nThere was a statistically significant, positive, and strong (r = .67; t = 8.82; p < .05; 95% CI: .55–.77) relationship between the transformed variables for percentage of females in school and percentage of citizens with basic water access in a sample of countries. As the percentage of citizens with basic water access increases, so does the percentage of school-age females in school. The data failed several of the assumptions for **r* and so these results should not be generalized outside the sample.\n:::\n\n:::::{.my-note}\n:::{.my-note-header}\nInferential statistics without generalizing conclusion outside the sample?\n:::\n::::{.my-note-container}\nI find it very disappointing that most of the time the assumptions for the different tests are not met. As far as I understand all the many tests and hypotheses failed, so that one can't say anything outside the data of the sample. \n\nIn the above summary are included a <a class='glossary' title='The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary)'>p-value</a> and a <a class='glossary' title='A range of values, calculated from the sample observations, that is believed, with a particular probability, to contain the true parameter value. (Cambridge Dictionary of Statistics, 4th ed., p.98)'>confidence interval</a>. As both values are for generalizing from a sample to a population and some of the assumptions are not met, it is --- in my opinion --- not allowed to mention these values. They \"could\" not omitted as the book claims but I think the should mandatory not included in the summary. These results are not reliable when the assumptions are failed and should not be mentioned at all because that creates more informative results as effectively is the case.\n\nAnother thing that annoys me after eight chapter is the high redundancy with all the tests and NHST procedures. I got the impression that the honest account of the book shows that there is something wrong with the frequentist approach of statistics. Most of the frequentist textbooks I already have read do not so thoroughly check their assumptions. I am eager to learn more about the Bayesian alternative!\n::::\n:::::\n\n\n### Summary\n\n::: {#bul-assumptions-pearson-r-transformed-summary}\n\n- Observations are independent (@sec-check-independence): **No**. \n- Both variables are continuous (@sec-chap08-check-continuous): **Yes**. \n- Both variables are normally distributed (@sec-chap08-transformed-normality): **No**.\n- The relationship between the two variables is linear (<a class='glossary' title='Linearity is the assumption of some statistical models that requires the outcome, or transformed outcome, to have a linear relationship with numeric predictors, where linear relationships are relationships that are evenly distributed around a line. (SwR, Glossary)'>linearity</a>) (@sec-chap08-linearity-transformed): **No**. \n- The variance is constant with the points distributed equally around the line (<a class='glossary' title='Homoscedasticity is [homogeneity of variances], contrast is [Heteroscedasticity]. Homoscedasticity is an assumption of correlation and linear regression that requires that the variance of y be constant across all the values of x; visually, this assumption would show points along a fit line between x and y being evenly spread on either side of the line for the full range of the relationship. (SwR, Glossary)'>homoscedasticity</a>) {@sec-chap08-homoscedasticity-transformed): **No**.\n\nSummary of testing the assumptions for Pearson’s r with transformed data\n:::\n\n## Achievement 7: Spearman’s rho {#sec-chap08-achievement7}\n\n\n## Exercises (empty)\n\n## Packages introduced in this chapter\n\n### lmtest\n\n:::::{.my-resource}\n:::{.my-resource-header}\nlmtest: Testing Linear Regression Models \n:::\n::::{.my-resource-container}\n\n***\n\n::: {#pak-lmtest}\n\n***\n\n{**lmtest**}: [Testing Linear Regression Models](https://cran.r-project.org/package=lmtest) [@lmtest]\n\nA collection of tests, data sets, and examples for diagnostic checking in linear regression models. Furthermore, some generic tools for inference in parametric models are provided.\n\nVignette [Diagnostic Checking in Regression Relationships](https://cran.r-project.org/web/packages/lmtest/vignettes/lmtest-intro.pdf)\n\n{**lmtest**}: Testing Linear Regression Models\n:::\n\n***\n::::\n:::::\n\n\n## Glossary\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Anderson-Darling </td>\n   <td style=\"text-align:left;\"> The Anderson-Darling Goodness of Fit Test (AD-Test) is a measure of how well your data fits a specified distribution. It’s commonly used as a test for normality. (&lt;a href=\"https://www.statisticshowto.com/anderson-darling-test/\"&gt;Statistics How-To&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Arcsine Transformations </td>\n   <td style=\"text-align:left;\"> Arcsine transformation are data transformation techniques often recommended to normalize percent or proportion data; the arcsine transformation uses the inverse of the sine function and the square root of the variable to transform. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Bessel’s Correction </td>\n   <td style=\"text-align:left;\"> Bessel's correction is the use of n − 1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance. It also partially corrects the bias in the estimation of the population standard deviation. However, the correction often increases the mean squared error in these estimations. This technique is named after Friedrich Bessel. (&lt;a href=\"https://en.wikipedia.org/wiki/Bessel%27s_correction\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Breusch-Pagan </td>\n   <td style=\"text-align:left;\"> Breusch-Pagan is a statistical test for determining whether variance is constant, which is used to test the assumption of homoscedasticity; Breusch-Pagan relies on the [chi-squared] distribution and is used during assumption checking for [homoscedasticity] in [linear regression]. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Ceiling </td>\n   <td style=\"text-align:left;\"> A ceiling effect happens when many observations are at the highest possible value for a variable. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Chi-squared </td>\n   <td style=\"text-align:left;\"> Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Confidence Interval </td>\n   <td style=\"text-align:left;\"> A range of values, calculated from the sample observations, that is believed, with a particular probability, to contain the true parameter value. (Cambridge Dictionary of Statistics, 4th ed., p.98) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Correlation </td>\n   <td style=\"text-align:left;\"> Correlation coefficients are a standardized measure of how two variables are related, or co-vary. They are used to measure how strong a relationship is between two variables. There are several types of correlation coefficient, but the most popular is Pearson’s. Pearson’s correlation (also called Pearson’s R) is a correlation coefficient commonly used in linear regression. ([Statistics How To](https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/)) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Covariance cov </td>\n   <td style=\"text-align:left;\"> Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together. (Statistics How To(https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/covariance/)) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Determination </td>\n   <td style=\"text-align:left;\"> Coefficient of determination is the percentage of variance in one variable that is accounted for by another variable or by a group of variables; often referred to as R-squared and used to determine model fit for linear models. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Floor </td>\n   <td style=\"text-align:left;\"> A floor effect happens when a variable has many observations that take the lowest value of the variable, which can indicate that the range of values was insufficient to capture the true variability of the data. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Histograms </td>\n   <td style=\"text-align:left;\"> Histograms are visual displays of data used to examine the distribution of a numeric variable. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Homoscedasticity </td>\n   <td style=\"text-align:left;\"> Homoscedasticity is [homogeneity of variances], contrast is [Heteroscedasticity]. Homoscedasticity is an assumption of correlation and linear regression that requires that the variance of y be constant across all the values of x; visually, this assumption would show points along a fit line between x and y being evenly spread on either side of the line for the full range of the relationship. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Linearity </td>\n   <td style=\"text-align:left;\"> Linearity is the assumption of some statistical models that requires the outcome, or transformed outcome, to have a linear relationship with numeric predictors, where linear relationships are relationships that are evenly distributed around a line. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Loess </td>\n   <td style=\"text-align:left;\"> Loess curve is a graph curve that shows the relationship between two variables without constraining the line to be straight; it can be compared to a linear fit line to determine whether the relationship is close to linear or not (= checking the [linearity] assumption). The procedure originated as LOWESS (LOcally WEighted Scatter-plot Smoother). is a nonparametric method because the linearity assumptions of conventional regression methods have been relaxed. It is called local regression because the fitting at say point x is weighted toward the data nearest to x. (SwR, Glossary and &lt;a href=\"https://www.statsdirect.com/help/Default.htm#nonparametric_methods/loess.htm\"&gt;LOESS Curve Fitting (Local Polynomial Regression&lt;/a&gt;)) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Logit Transformations </td>\n   <td style=\"text-align:left;\"> Logit transformations are transformations that takes the log value of p/(1-p); this transformation is often used to normalize percentage data and is used in the logistic model to transform the outcome. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> NHST </td>\n   <td style=\"text-align:left;\"> Null Hypothesis Significance Testing (NHST) is a process for organizing inferential statistical tests. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Nonlinear Transformations </td>\n   <td style=\"text-align:left;\"> Nonlinear transformations are transformations that increases (or decreases) the linear relationship between two variables by applying an exponent (i.e., [power transformation]) or other function to one or both of the variables. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p-value </td>\n   <td style=\"text-align:left;\"> The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Power Transformations </td>\n   <td style=\"text-align:left;\"> Power transformations are transformations of a measure using an exponent like squaring or cubing or taking the square root or cube root; power transformations are nonlinear transformations. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Q-Q-Plot </td>\n   <td style=\"text-align:left;\"> A quantile-quantile plot is a visualization of data using probabilities to show how closely a variable follows a normal distribution. (SwR, Glossary) This plot is made up of points below which a certain percentage of the observations fall. On the x-axis are normally distributed values with a mean of 0 and a standard deviation of 1. On the y-axis are the observations from the data. If the data are normally distributed, the values will form a diagonal line through the graph. (SwR, chapter 6) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Shapiro-Wilk </td>\n   <td style=\"text-align:left;\"> The Shapiro-Wilk test is a statistical test to determine or confirm whether a variable has a normal distribution; it is sensitive to small deviations from normality and not useful for sample sizes above 5,000 because it will nearly always find non-normality. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standard Deviation </td>\n   <td style=\"text-align:left;\"> The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter $\\sigma$ (sigma), for the population standard deviation, or the Latin letter $s$ for the sample standard deviation. ([Wikipedia] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standardization </td>\n   <td style=\"text-align:left;\"> In statistics, standardization (also called Normalizing) is the process of putting different variables on the same scale. This process allows you to compare scores between different types of variables. Typically, to standardize variables, you calculate the mean and standard deviation for a variable. Then, for each observed value of the variable, you subtract the mean and divide by the standard deviation. ([Statistics by Jim](https://statisticsbyjim.com/glossary/standardization/)) See `scale()` in R.  (Chap.4) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> T-Statistic </td>\n   <td style=\"text-align:left;\"> The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (&lt;a href=\"https://www.statisticshowto.com/t-statistic/\"&gt;Statistics How-To&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> T-Test </td>\n   <td style=\"text-align:left;\"> A t-test is a type of statistical analysis used to compare the averages of two groups and determine whether the differences between them are more likely to arise from random chance. (&lt;a href=\"https://en.wikipedia.org/wiki/Student%27s_t-test\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Z-score </td>\n   <td style=\"text-align:left;\"> A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (&lt;a href=\"https://www.statisticshowto.com/probability-and-statistics/z-score/#Whatisazscore\"&gt;StatisticsHowTo&lt;/a&gt;) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Session Info {.unnumbered}\n\n::: my-r-code\n::: my-r-code-header\nSession Info\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.3.3 (2024-02-29)\n#>  os       macOS Sonoma 14.4.1\n#>  system   x86_64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2024-04-04\n#>  pandoc   3.1.12.3 @ /usr/local/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package      * version    date (UTC) lib source\n#>  base64enc      0.1-3      2015-07-28 [1] CRAN (R 4.3.0)\n#>  boot           1.3-30     2024-02-26 [2] CRAN (R 4.3.2)\n#>  cellranger     1.1.0      2016-07-27 [1] CRAN (R 4.3.0)\n#>  class          7.3-22     2023-05-03 [2] CRAN (R 4.3.3)\n#>  cli            3.6.2      2023-12-11 [1] CRAN (R 4.3.0)\n#>  codetools      0.2-20     2024-03-31 [1] CRAN (R 4.3.3)\n#>  coin           1.4-3      2023-09-27 [1] CRAN (R 4.3.0)\n#>  colorspace     2.1-1      2024-01-03 [1] R-Forge (R 4.3.2)\n#>  commonmark     1.9.1      2024-01-30 [1] CRAN (R 4.3.2)\n#>  curl           5.2.1      2024-03-01 [1] CRAN (R 4.3.2)\n#>  data.table     1.15.4     2024-03-30 [1] CRAN (R 4.3.2)\n#>  DescTools      0.99.54    2024-02-03 [1] CRAN (R 4.3.2)\n#>  digest         0.6.35     2024-03-11 [1] CRAN (R 4.3.2)\n#>  dplyr          1.1.4      2023-11-17 [1] CRAN (R 4.3.0)\n#>  e1071          1.7-14     2023-12-06 [1] CRAN (R 4.3.0)\n#>  evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.0)\n#>  Exact          3.2        2022-09-25 [1] CRAN (R 4.3.0)\n#>  expm           0.999-9    2024-01-11 [1] CRAN (R 4.3.0)\n#>  fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.0)\n#>  farver         2.1.1      2022-07-06 [1] CRAN (R 4.3.0)\n#>  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#>  generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#>  ggplot2        3.5.0      2024-02-23 [1] CRAN (R 4.3.2)\n#>  gld            2.6.6      2022-10-23 [1] CRAN (R 4.3.0)\n#>  glossary     * 1.0.0.9000 2023-08-12 [1] Github (debruine/glossary@819e329)\n#>  glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.0)\n#>  gridExtra      2.3        2017-09-09 [1] CRAN (R 4.3.0)\n#>  gtable         0.3.4      2023-08-21 [1] CRAN (R 4.3.0)\n#>  here           1.0.1      2020-12-13 [1] CRAN (R 4.3.0)\n#>  highr          0.10       2022-12-22 [1] CRAN (R 4.3.0)\n#>  htmltools      0.5.8      2024-03-25 [1] CRAN (R 4.3.2)\n#>  htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.3.0)\n#>  httr           1.4.7      2023-08-15 [1] CRAN (R 4.3.0)\n#>  jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.0)\n#>  kableExtra     1.4.0      2024-01-24 [1] CRAN (R 4.3.2)\n#>  knitr          1.45       2023-10-30 [1] CRAN (R 4.3.0)\n#>  labeling       0.4.3      2023-08-29 [1] CRAN (R 4.3.0)\n#>  lattice        0.22-6     2024-03-20 [2] CRAN (R 4.3.2)\n#>  libcoin        1.0-10     2023-09-27 [1] CRAN (R 4.3.0)\n#>  lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.3.0)\n#>  lmom           3.0        2023-08-29 [1] CRAN (R 4.3.0)\n#>  lmtest         0.9-40     2022-03-21 [1] CRAN (R 4.3.0)\n#>  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#>  markdown       1.12       2023-12-06 [1] CRAN (R 4.3.0)\n#>  MASS           7.3-60.0.1 2024-01-13 [2] CRAN (R 4.3.3)\n#>  Matrix         1.6-5      2024-01-11 [1] CRAN (R 4.3.0)\n#>  matrixStats    1.2.0      2023-12-11 [1] CRAN (R 4.3.0)\n#>  mgcv           1.9-1      2023-12-21 [1] CRAN (R 4.3.0)\n#>  modeltools     0.2-23     2020-03-05 [1] CRAN (R 4.3.0)\n#>  multcomp       1.4-25     2023-06-20 [1] CRAN (R 4.3.0)\n#>  multcompView   0.1-10     2024-03-08 [1] CRAN (R 4.3.2)\n#>  munsell        0.5.0      2018-06-12 [1] CRAN (R 4.3.0)\n#>  mvtnorm        1.2-4      2023-11-27 [1] CRAN (R 4.3.2)\n#>  nlme           3.1-164    2023-11-27 [1] CRAN (R 4.3.2)\n#>  nortest        1.0-4      2015-07-30 [1] CRAN (R 4.3.0)\n#>  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#>  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#>  plyr           1.8.9      2023-10-02 [1] CRAN (R 4.3.0)\n#>  proxy          0.4-27     2022-06-09 [1] CRAN (R 4.3.0)\n#>  purrr          1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n#>  R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#>  rcompanion     2.4.35     2024-02-17 [1] CRAN (R 4.3.2)\n#>  Rcpp           1.0.12     2024-01-09 [1] CRAN (R 4.3.0)\n#>  readxl         1.4.3      2023-07-06 [1] CRAN (R 4.3.0)\n#>  repr           1.1.7      2024-03-22 [1] CRAN (R 4.3.3)\n#>  rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.0)\n#>  rmarkdown      2.26       2024-03-05 [1] CRAN (R 4.3.2)\n#>  rootSolve      1.8.2.4    2023-09-21 [1] CRAN (R 4.3.1)\n#>  rprojroot      2.0.4      2023-11-05 [1] CRAN (R 4.3.0)\n#>  rstudioapi     0.16.0     2024-03-24 [1] CRAN (R 4.3.2)\n#>  rversions      2.1.2      2022-08-31 [1] CRAN (R 4.3.0)\n#>  sandwich       3.1-0      2023-12-11 [1] CRAN (R 4.3.0)\n#>  scales         1.3.0      2023-11-28 [1] CRAN (R 4.3.2)\n#>  sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n#>  skimr          2.1.5      2022-12-23 [1] CRAN (R 4.3.0)\n#>  stringi        1.8.3      2023-12-11 [1] CRAN (R 4.3.0)\n#>  stringr        1.5.1      2023-11-14 [1] CRAN (R 4.3.0)\n#>  survival       3.5-8      2024-02-14 [2] CRAN (R 4.3.3)\n#>  svglite        2.1.3      2023-12-08 [1] CRAN (R 4.3.0)\n#>  systemfonts    1.0.6      2024-03-07 [1] CRAN (R 4.3.2)\n#>  TH.data        1.1-2      2023-04-17 [1] CRAN (R 4.3.0)\n#>  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#>  tidyr          1.3.1      2024-01-24 [1] CRAN (R 4.3.2)\n#>  tidyselect     1.2.1      2024-03-11 [1] CRAN (R 4.3.2)\n#>  utf8           1.2.4      2023-10-22 [1] CRAN (R 4.3.0)\n#>  vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n#>  viridisLite    0.4.2      2023-05-02 [1] CRAN (R 4.3.0)\n#>  withr          3.0.0      2024-01-16 [1] CRAN (R 4.3.0)\n#>  xfun           0.43       2024-03-25 [1] CRAN (R 4.3.2)\n#>  xml2           1.3.6      2023-12-04 [1] CRAN (R 4.3.0)\n#>  yaml           2.3.8      2023-12-11 [1] CRAN (R 4.3.0)\n#>  zoo            1.8-12     2023-04-13 [1] CRAN (R 4.3.0)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.3-x86_64/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n:::\n:::\n",
    "supporting": [
      "08-correlation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}