{
  "hash": "de09ba565e2508bdee6cb0a5970a6b5a",
  "result": {
    "engine": "knitr",
    "markdown": "# Correlation coefficient {#sec-chap08}\n\n\n\n\n\n## Achievements to unlock\n\n::: {#obj-chap08}\n::: my-objectives\n::: my-objectives-header\nObjectives for chapter 08\n:::\n\n::: my-objectives-container\n**SwR Achievements**\n\n-   **Achievement 1**: Exploring the data using graphics and descriptive\n    statistics (@sec-chap08-achievement1).\n-   **Achievement 2**: Computing and interpreting Pearson’s *r*\n    correlation coefficient (@sec-chap08-achievement2).\n-   **Achievement 3**: Conducting an inferential statistical test for\n    Pearson’s *r* correlation coefficient (@sec-chap08-achievement3).\n-   **Achievement 4**: Examining effect size for Pearson’s *r* with the\n    coefficient of determination (@sec-chap08-achievement4).\n-   **Achievement 5**: Checking assumptions for Pearson’s *r*\n    correlation analyses (@sec-chap08-achievement5).\n-   **Achievement 6**: Transforming the variables as an alternative when\n    Pearson’s *r* correlation assumptions are not met (@sec-chap08-achievement6).\n-   **Achievement 7**: Using Spearman’s rho as an alternative when\n    Pearson’s *r* correlation assumptions are not met\n    (@sec-chap08-achievement7).\n-   **Achievement 8**: Introducing partial correlations\n    (@sec-chap08-achievement8).\n:::\n:::\n\nAchievements for chapter 08\n:::\n\n## The clean water conundrum\n\n-   Women and girls tend to be responsible for collecting water for\n    their families, often walking long distances in unsafe areas and\n    carrying heavy loads.\n-   In some cultures, lack of access to sanitation facilities also means\n    that women can only defecate after dark, which can be physically\n    uncomfortable and/or put them at greater risk for harassment and\n    assault.\n-   The lack of sanitation facilities can keep girls out of school when\n    they are menstruating.\n\n**Goals**\n\n1.  With data from a few different sources examining the relationship\n    between the percentage of people in a country with water access and\n    the percentage of school-aged girls who are in school.\n2.  With data exploring the relationship between the percentage of\n    females in school and the percentage of people living on less than\n    \\$1 per day.\n\n## Resources & Chapter Outline\n\n### Data, codebook, and R packages {#sec-chap08-data-codebook-packages}\n\n::: my-resource\n::: my-resource-header\n::: {#lem-chap08-resources}\n: Data, codebook, and R packages for learning about descriptive\nstatistics\n:::\n:::\n\n::: my-resource-container\n**Data**\n\nTwo options:\n\n1.  Download the `water_educ_2015_who_unesco_ch8.csv` and\n    `2015-outOfSchoolRate-primarySecondary-ch8.xlsx` data sets from\n    <https://edge.sagepub.com/harris1e>.\n2.  Follow the instructions in Box 8.1 to import and clean the data\n    directly from the original Internet sources. Please note that the\n    WHO makes small corrections to past data occasionally, so use of\n    data imported based on Box 8.1 instructions may result in minor\n    differences in results throughout the chapter. To match chapter\n    results exactly, use the data provided.\n\n::: my-note\n::: my-note-header\nUsing data provided from the book\n:::\n\n::: my-note-container\nI have learned a lot about data cleaning procedures in the last\nchapters. I feel secure and decided from now on that I will take data\nprovided by the book. This help me to focus my attention on the\nstatistical subjects of the book.\n:::\n:::\n\n**Codebook**\n\nTwo options:\n\n1.  Download the codebook file `opioid_county_codebook.xlsx` from\n    <https://edge.sagepub.com/harris1e>.\n2.  Use the online version of the codebook from the amfAR Opioid &\n    Health Indicators Database website (https://opioid.amfar.org)\n\n**Packages**\n\n1.  Packages used with the book (sorted alphabetically) (Install the\n    following R packages if not already installed.)\n\n-   {**tidyverse**}: @pak-tidyverse (Hadley Wickham)\n-   {**readxl**}: @pak-readxl (Jennifer Bryan)\n-   {**lmtest**}: @pak-lmtest (Achim Zeileis)\n-   {**rcompanion**}: @pak-rcompanion (Salvatore Mangiafico)\n-   {**ppcor**}: @pak-ppcor (Seongho Kim)\n\n2.  My additional packages (sorted alphabetically)\n:::\n:::\n\n### Get data & show raw data\n\n::: my-example\n::: my-example-header\n::: {#exm-chap08-get-data}\n: Get data and show raw for chapter 8\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Get water-educ\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-get-water-educ-data}\n: Get Water-Education data\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n## run only once (manually)\nwater_educ <- readr::read_csv(\n    file = \"data/chap08/water_educ_2015_who_unesco_ch8.csv\",\n    show_col_types = FALSE\n    )\n\nsave_data_file(\"chap08\", water_educ, \"water_educ.rds\")\n```\n:::\n\n\n(*For this R code chunk is no output available*)\n:::\n:::\n\n###### Show water_educ\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-show-water-educ}\n: Show Water & Education data\n:::\n:::\n\n::: my-r-code-container\n\n::: {#tbl-show-water-educ .cell tbl-cap='Show descriptive data from the Water-Edcuation UNESCO file'}\n\n```{.r .cell-code}\nwater_educ <- base::readRDS(\"data/chap08/water_educ.rds\")\n\nwater_educ |> \n    skimr::skim()\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |water_educ |\n|Number of rows           |97         |\n|Number of columns        |10         |\n|_______________________  |           |\n|Column type frequency:   |           |\n|character                |1          |\n|numeric                  |9          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|country       |         0|             1|   4|  52|     0|       97|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable       | n_missing| complete_rate|  mean|    sd|    p0|   p25|   p50|    p75|   p100|hist  |\n|:-------------------|---------:|-------------:|-----:|-----:|-----:|-----:|-----:|------:|------:|:-----|\n|med.age             |         0|          1.00| 30.33|  8.69| 15.00| 22.50| 29.70|  39.00|  45.90|▆▇▇▆▇ |\n|perc.1dollar        |        33|          0.66| 13.63| 20.52|  1.00|  1.00|  1.65|  17.12|  83.80|▇▁▁▁▁ |\n|perc.basic2015sani  |         0|          1.00| 79.73| 27.18|  7.00| 73.00| 93.00|  99.00| 100.00|▁▁▁▁▇ |\n|perc.safe2015sani   |        47|          0.52| 71.50| 25.84|  9.00| 61.25| 76.50|  93.00| 100.00|▂▂▁▆▇ |\n|perc.basic2015water |         1|          0.99| 90.16| 15.82| 19.00| 88.75| 97.00| 100.00| 100.00|▁▁▁▁▇ |\n|perc.safe2015water  |        45|          0.54| 83.38| 22.34| 11.00| 73.75| 94.00|  98.00| 100.00|▁▁▁▂▇ |\n|perc.in.school      |         0|          1.00| 87.02| 13.94| 33.32| 83.24| 92.02|  95.81|  99.44|▁▁▁▂▇ |\n|female.in.school    |         0|          1.00| 87.06| 15.10| 27.86| 83.70| 92.72|  96.61|  99.65|▁▁▁▂▇ |\n|male.in.school      |         0|          1.00| 87.00| 12.95| 38.66| 82.68| 91.50|  95.57|  99.36|▁▁▁▂▇ |\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nInstead of `base::summary()` I used `skimr::skim()` which fives more\ndescriptive information.\n\n**Codebook**\n\n-   **country**: the name of the country\n-   **med.age**: the median age of the citizens in the country\n-   **perc.1dollar**: percentage of citizens living on \\$1 per day or\n    less\n-   **perc.basic2015sani**: percentage of citizens with basic sanitation\n    access\n-   **perc.safe2015san**i: percentage of citizens with safe sanitation\n    access\n-   **perc.basic2015water**: percentage of citizens with basic water\n    access\n-   **perc.safe2015water**: percentage of citizens with safe water\n    access\n-   **perc.in.school**: percentage of school-age people in primary and\n    secondary school\n-   **female.in.school**: percentage of female school-age people in\n    primary and secondary school\n-   **male.in.school**: percentage of male school-age people in primary\n    and secondary school\n:::\n:::\n:::\n:::\n:::\n\n## Exploring data {#sec-chap08-achievement1}\n\nThe two variables of interests are:\n\n-   female.in.school and\n-   perc.basic2015water\n\n::: my-example\n::: my-example-header\n::: {#exm-chap08-exploring-data}\n: Exploring data for chapter 8\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### mean & sd\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-mean-sd-water-educ}\n: Mean and standard deviation for `female.in.school` and\n`perc.basic2015water`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n    skimr::skim(c(female.in.school, perc.basic2015water)\n    )\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |water_educ |\n|Number of rows           |97         |\n|Number of columns        |10         |\n|_______________________  |           |\n|Column type frequency:   |           |\n|numeric                  |2          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: numeric**\n\n|skim_variable       | n_missing| complete_rate|  mean|    sd|    p0|   p25|   p50|    p75|   p100|hist  |\n|:-------------------|---------:|-------------:|-----:|-----:|-----:|-----:|-----:|------:|------:|:-----|\n|female.in.school    |         0|          1.00| 87.06| 15.10| 27.86| 83.70| 92.72|  96.61|  99.65|▁▁▁▂▇ |\n|perc.basic2015water |         1|          0.99| 90.16| 15.82| 19.00| 88.75| 97.00| 100.00| 100.00|▁▁▁▁▇ |\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThe mean percent of school-aged females in school was 87.06 (sd = 15.1),\nand the mean percent of citizens who had basic access to water was 90.16\n(sd = 15.82).\n\nThis is a pretty high percentage. The very high median shows that there\nis a heavy left-skewed distribution. 93 & 97% are in the first half of\nthe distribution located!\n\n::: my-note\n::: my-note-header\nAdvantages of the `skimr::skim()` function\n:::\n\n::: my-note-container\nThis above summary show the advantage of the `skimr::skim()` function\nversus the `base::summary()` resp. the extra calculation of mean and sd.\n`skimr::skim()` is (a) easier to use (just one line!) and (b) displays\nmuch more information, e.g., different percentiles with a small\nhistogram. Important here is, for instance, that we can compare mean and\nmedian in one step.\n:::\n:::\n:::\n:::\n\n###### scatterplot1\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-scatterplot-female-water}\n: Scatterplot of `female.in.school` and `perc.basic2015water`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = female.in.school / 100,\n            y = perc.basic2015water / 100\n        )\n    ) +\n    ggplot2::geom_point(\n        na.rm = TRUE,\n        ggplot2::aes(\n            color = \"Country\"                \n        ), \n        size = 2.5,\n        alpha = 0.3\n    ) +\n    ggplot2::labs(\n        x = \"Percent with basic water access\",\n        y = \"Percent of school-aged females in school\" \n    ) +\n    ggplot2::scale_color_manual(\n        name = \"\",\n        values = \"purple3\"\n    ) +\n    ggplot2::scale_x_continuous(\n        labels = scales::label_percent()\n    ) +\n    ggplot2::scale_y_continuous(\n        labels = scales::percent\n    )\n```\n\n::: {.cell-output-display}\n![Relationship of percentage of females in school and percentage of citizens with basic water access in countries worldwide](08-correlation_files/figure-html/fig-scatterplot-female-water-1.png){#fig-scatterplot-female-water width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nI have used two different argument styles for the percent scale from the\n{**scales**} package (see: @pak-scales):\n\n-   `labels = scales::percent` as in the book\n-   `labels = scales::label_percent()` from the help file of the\n    {**scales**} package.\n:::\n:::\n\n###### scatterplot2\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-scatterplot-female-dollar}\n: Scatterplot of `female.in.school` and `perc.1dollar`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = perc.1dollar / 100,\n            y = female.in.school / 100\n        )\n    ) +\n    ggplot2::geom_jitter(\n        na.rm = TRUE,\n        ggplot2::aes(\n            color = \"Country\"                \n        ),\n        size = 2.5,\n        alpha = 0.3\n    ) +\n    ggplot2::labs(\n        x = \"Percent of people living on less than $1 per day\", \n        y = \"Percent with basic water access\"\n    ) +\n    ggplot2::scale_color_manual(\n        name = \"\",\n        values = \"purple3\"\n    ) +\n    ggplot2::scale_x_continuous(\n        labels = scales::label_percent()\n    ) +\n    ggplot2::scale_y_continuous(\n        labels = scales::percent\n    )\n```\n\n::: {.cell-output-display}\n![Relationship of percentage of females in school and percentage of people living on less than $1 per day in countries worldwide](08-correlation_files/figure-html/fig-scatterplot-female-dollar-1.png){#fig-scatterplot-female-dollar width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n:::\n:::\n:::\n:::\n:::\n\n## Pearson’s *r* correlation coefficient {#sec-chap08-achievement2}\n\n### Introduction\n\nOne method of measuring the relationship between two continuous variable\nis <a class='glossary' title='Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together. (Statistics How To)'>covariance</a>, which quantifies\nwhether two variables vary together (co-vary).\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-chap08-covariance}\n: Formula for covariance\n:::\n:::\n\n::: my-theorem-container\n$$\ncov_{xy} = \\sum_{i=1}^{n}\\frac{(x_{i}-m_{x})(y_{i}-m_{y})}{n-1}\n$$ {#eq-chap08-covariance}\n:::\n:::\n\nThe numerator essentially adds up how far each observation is away from\nthe mean values of the two variables being examined, so this ends up\nbeing a very large number quantifying how far away all the observations\nare from the mean values. The denominator divides this by\n<a class='glossary' title='Bessel’s correction is the use of n − 1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance. It also partially corrects the bias in the estimation of the population standard deviation. However, the correction often increases the mean squared error in these estimations. This technique is named after Friedrich Bessel. (Wikipedia)'>Bessel’s correction</a> (@sec-chap04-clt) of $n – 1$, which\nis close to the sample size and essentially finds the average deviation\nfrom the means for each observation.\n\nI skipped Figure 8.4 and 8.5 because they do not bring any news for me.\n(Note that there is a wrong label for x-axis in Figure 8.5: Instead of\n\"Percent living on less than \\$1 per day\" it says wrongly \"Percent with\nbasic water access\".)\n\n### Missing values\n\nThe covariance function `stats::cov()` is like the `base::mean()`\nfunction in that it cannot handle NA values. As we are going to\ncalculate `female.in.school` with `perc.basic2015water` and\n`female.in.school` with `perc.1dollar` we would have three different\nvariables with NA's.\n\nIt is important not to to remove all rows with missing data of all three\nvariables at the same time because that would delete more rows as for\neach pair of variable would be necessary. We know from\n@tbl-show-water-educ that\n\n-   `female.in.school` has no missing values\n-   `perc.basic2015water` has 1 missing value\n-   `perc.1dollar` has 33 missing values\n\nThere are two options:\n\na)  To use two different covariance calculations, each time with the\n    appropriate `tidyr::drop_na()` function as used finally in the book.\nb)  To apply the appropriate `use` argument of the `stats::cov()`\n    function for each calculations, which I will use and which was the\n    first try in the book.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-cov-female-water-pov}\n: Covariance of females in school and percentage with basic access to\ndrinking water\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n  dplyr::summarize(\n      cov_females_water = stats::cov(\n          x = perc.basic2015water,\n          y = female.in.school,\n          use = \"pairwise.complete.obs\",\n          method = \"pearson\"\n          ),\n      cov_females_pov = stats::cov(\n          x = perc.1dollar,\n          y = female.in.school,\n          use = \"pairwise.complete.obs\",\n          method = \"pearson\")\n      )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 2\n#>   cov_females_water cov_females_pov\n#>               <dbl>           <dbl>\n#> 1              194.           -203.\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThe book argument for NA's is `use = \"complete\"` which is an allowed\nabbreviation for `use = \"complete.obs\"`. I have employed\n`use = \"pairwise.complete.obs\"` which is a more precise argument but\nworks only for the (default) \"pearson\" method.\n:::\n:::\n\n### Interpretation\n\nThe covariance does not have an intuitive inherent meaning; it is not a\npercentage or a sum or a difference. In fact, the size of the covariance\ndepends largely on the size of what is measured. For example, something\nmeasured in millions might have a covariance in the millions or hundreds\nof thousands. The value of the covariance indicates whether there is a\nrelationship at all and the direction of the relationship --- that is,\nwhether the relationship is positive or negative.\n\nIn this case, a nonzero value indicates that there is some relationship.\nIn the first case (`cov_females_water`) it is a positive relationship;\nin the second case (`cov_females_pov`) it is a negative relationship.\nThe size of the numbers are irrelevant!\n\nTherefore <a class='glossary' title='In statistics, standardization (also called Normalizing) is the process of putting different variables on the same scale. This process allows you to compare scores between different types of variables. Typically, to standardize variables, you calculate the mean and standard deviation for a variable. Then, for each observed value of the variable, you subtract the mean and divide by the standard deviation. (Statistics by Jim) See scale() in R.  (Chap.4)'>standardization</a> by dividing by the\n<a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a> of the two involved variables is\nnecessary. The result is called the\n<a class='glossary' title='Correlation coefficients are a standardized measure of how two variables are related, or co-vary. They are used to measure how strong a relationship is between two variables. There are several types of correlation coefficient, but the most popular is Pearson’s. Pearson’s correlation (also called Pearson’s R) is a correlation coefficient commonly used in linear regression. (Statistics How To)'>correlation coefficient</a> and is referred\nto as *r*.\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-chap08-pearson-r}\n: Computing the Pearson *r* correlation between two variables\n:::\n:::\n\n::: my-theorem-container\n$$\n\\begin{align*}\nr_{xy} = \\frac{cov_{xy}}{s_{x}s_{y}} \\\\\nr_{xy} = \\sum_{i = 1}^{n}\\frac{z_{x}z_{y}}{n-1}\n\\end{align*}\n$$ {#eq-chap08-pearson-r}\n\n------------------------------------------------------------------------\n\nThe second line is also know as the product-moment correlation\ncoefficient. The formula for *r* can be organized in many different\nways, one of which is as the mean of the summed products of\n<a class='glossary' title='A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (StatisticsHowTo)'>z-scores</a>.\n\n::: my-assessment\n::: my-assessment-header\n::: {#cor-chap08-pearson-r}\n: Range of Pearson’s *r* and interpretation of strength\n:::\n:::\n\n::: my-assessment-container\n-   **-1: Negative correlations** occur when one variable goes up and\n    the other goes down.\n-   **0: No correlation** happens when there is no discernable pattern\n    in how two variables vary.\n-   **+1: Positive correlations** occur when one variable goes up, and\n    the other one also goes up (or when one goes down, the other one\n    does too).\n\n------------------------------------------------------------------------\n\n-   **r = –1.0** is perfectly negative\n-   **r = –.8** is strongly negative\n-   **r = –.5** is moderately negative\n-   **r = –.2** is weakly negative\n-   **r = 0** is no relationship\n-   **r = .2** is weakly positive\n-   **r = .5** is moderately positive\n-   **r = .8** is strongly positive\n-   **r = 1.0** is perfectly positive\n:::\n:::\n:::\n:::\n\n::: my-example\n::: my-example-header\n::: {#exm-chap08-correlation}\n: Compute and show correlation\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### compute cor()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-cor-water-pov-female}\n: Compute correlations for water access, poverty and female education\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ <-  base::readRDS(\"data/chap08/water_educ.rds\")\n\nwater_educ |> \n  dplyr::summarize(\n     cor_females_water = cor(\n         x = perc.basic2015water,\n         y = female.in.school,\n         use = \"complete.obs\"\n         ),\n     cor.females.pov = cor(\n         x = perc.1dollar,\n         y = female.in.school,\n         use = \"complete.obs\"\n         )\n     )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 2\n#>   cor_females_water cor.females.pov\n#>               <dbl>           <dbl>\n#> 1             0.809          -0.714\n```\n\n\n:::\n:::\n\n:::\n:::\n\n###### graph1 cor\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-graph1-cor}\n: Display correlation water access and female education with `lm` and\n`loess` smoother with a special constructed legend\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = female.in.school/100, \n          x = perc.basic2015water/100\n          )\n      ) +\n  ggplot2::geom_smooth(\n      ggplot2::aes(color = \"Linear fit line\"),\n      formula = y ~ x,\n      method = \"lm\",\n      se = FALSE, \n      na.rm = TRUE\n      ) +\n    ggplot2::geom_smooth(\n      ggplot2::aes(color = \"Loess line\"),\n      formula = y ~ x,\n      method = \"loess\",\n      se = FALSE, \n      na.rm = TRUE\n      ) +\n  ggplot2::geom_point(\n      ggplot2::aes(size = \"Country\"), \n      color = \"#7463AC\", \n      alpha = .6,\n      na.rm = TRUE\n      ) +\n  ggplot2::labs(\n      y = \"Percent of school-aged females in school\",\n      x = \"Percent with basic water access\"\n      ) +\n  ggplot2::scale_x_continuous(labels = scales::percent) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::scale_color_manual(\n      values = c(\"gray60\", \"darkred\"), \n      name = \"\"\n      ) +      \n  ggplot2::scale_size_manual(values = 2, name = \"\")\n```\n\n::: {.cell-output-display}\n![Display correlation water access and female education with `lm` and `loess` smoother with a special constructed legend](08-correlation_files/figure-html/fig-graph1-cor-1.png){#fig-graph1-cor width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n**`ggplot2::geom_smooth()` layer**\n\n-   The formula argument would not be necessary, because the program\n    assumes y \\~ x for fewer than 1000 observations.\n-   If I haven't specified the method with `lm` than the default value\n    would have been chosen, e.g. (depending on fewer than 1000) which is\n    a local polynomial regression fitting.\n-   To show the difference I had used both `method = lm` and in another\n    layer `method = loess`. The <a class='glossary' title='Loess curve is a graph curve that shows the relationship between two variables without constraining the line to be straight; it can be compared to a linear fit line to determine whether the relationship is close to linear or not (= checking the [linearity] assumption). The procedure originated as LOWESS (LOcally WEighted Scatter-plot Smoother). is a nonparametric method because the linearity assumptions of conventional regression methods have been relaxed. It is called local regression because the fitting at say point x is weighted toward the data nearest to x. (SwR, Glossary and LOESS Curve Fitting (Local Polynomial Regression))'>Loess curve</a>\n    results in the slightly curved line (the red curve). Instead of\n    fitting the whole data at once (= \"lm\"), method \"loess\" creates a\n    local regression because the fitting at say point x is weighted\n    toward the data nearest to x and not to the general mean.\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Legends are generated from attributes inside the\n`ggplot2::aes()` statement\n:::\n\n::: my-watch-out-container\nIt is important to know:\n\n-   If all aesthetics are determined outside the `ggplot2::aes()`\n    functions then there is not legend generated.\n-   The name of the aesthetics are arbitrary and result as labels inside\n    the legend.\n\nIn this case I have used twice the \"color\" aesthetic, but as value I\ngave as argument was the type of line and not an actual color. The\nactual color for the lines you will fin in the\n`ggplot2::scale_color_manual()` layer at the very bottom of the code.\n\nSee also the next two graphs (@fig-graph2-cor and @fig-graph3-cor) about\nwater access and female education where I have explored different types\nof points and lines inside the aesthetic function.\n:::\n:::\n:::\n:::\n\n###### graph2 cor\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-graph3-cor}\n: Display correlation water access and female education with two legends\nexplaining what the different symbols represent\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = female.in.school/100, \n          x = perc.basic2015water/100\n          )\n      ) +\n  ggplot2::geom_smooth(\n      ggplot2::aes(color = \"Linear fit line\"),\n      formula = y ~ x,\n      method = \"lm\",\n      se = FALSE, \n      na.rm = TRUE\n      ) +\n  ggplot2::geom_point(\n      ggplot2::aes(size = \"Country\"), \n      color = \"#7463AC\", \n      alpha = .6,\n      na.rm = TRUE\n      ) +\n  ggplot2::labs(\n      y = \"Percent of school-aged females in school\",\n      x = \"Percent with basic water access\"\n      ) +\n  ggplot2::scale_x_continuous(labels = scales::percent) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::scale_color_manual(values = \"gray60\", name = \"Legend 2\") +\n  ggplot2::scale_size_manual(values = 2, name = \"Legend 1\")\n```\n\n::: {.cell-output-display}\n![Display correlation water access and female education with two legends explaining what the different symbols represent](08-correlation_files/figure-html/fig-graph2-cor-1.png){#fig-graph2-cor width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Legends are generated from attributes inside the\n`ggplot2::aes()` statement\n:::\n\n::: my-watch-out-container\nThe two `ggplot2::aes()` functions used for this graph are\n`ggplot2::aes(size = \"Country\")` and\n`ggplot2::aes(linetype = \"Linear fit line\")`. To get two different\nlegends (point and lines), two different attributes were used within the\n`aes()`.\n:::\n:::\n:::\n:::\n\n###### graph3 cor\n\n::: my-r-code\n::: my-r-code-header\n<div>\n\n: Display correlation water access and female education with a legend\nexplaining what the different symbols represent\n\n</div>\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = female.in.school/100, \n          x = perc.basic2015water/100\n          )\n      ) +\n  ggplot2::geom_smooth(\n      ggplot2::aes(color = \"Linear fit line\"),\n      formula = y ~ x,\n      method = \"lm\",\n      se = FALSE, \n      na.rm = TRUE\n      ) +\n  ggplot2::geom_point(\n      ggplot2::aes(color = \"Country\"), \n      size = 2, \n      alpha = .6,\n      na.rm = TRUE\n      ) +\n  ggplot2::labs(\n      y = \"Percent of school-aged females in school\",\n      x = \"Percent with basic water access\"\n      ) +\n  ggplot2::scale_x_continuous(labels = scales::percent) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::scale_color_manual(\n      name = \"Legend\",\n      values = c(\"#7463AC\", \"gray60\") \n      )\n```\n\n::: {.cell-output-display}\n![Display correlation water access and female education with a legend explaining what the different symbols represent](08-correlation_files/figure-html/fig-graph3-cor-1.png){#fig-graph3-cor width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! The name of the attribute inside the `aes()` is arbitrary\n:::\n\n::: my-watch-out-container\n@fig-graph3-cor has the color attribute for both the points and the line\nwithin `aes()` and so both colors are included in the only legend.\n\nThe name of the attribute inside the `aes()` is arbitrary and will\nresult in the **label of the legend**. The type of this attribute has to\nbe addressed and specified with the correct manual scale\n(`ggplot2::scale_xxx_manual()`) and will display the appropriate symbol\nfor the attribute.\n\n**ATTENTION**: With new versions of {**ggplot2**} the symbols are not\nmerged as in the book’s version. This would have been not correct,\nbecause the line does not go through all points. Points and lines are\ndifferent aesthetics but they are merged under on legend with one common\nattribute, their color.\n:::\n:::\n\n::: callout-tip\nThe Pearson’s product-moment correlation coefficient demonstrated that\nthe percentage of females in school is positively correlated with the\npercentage of citizens with basic access to drinking water (r = 0.81).\nEssentially, as access to water goes up, the percentage of females in\nschool also increases in countries.\n:::\n:::\n:::\n\n###### graph4 cor\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-graph4-cor}\n: Display relationship of percentage of citizens living on less than \\$1\nper day and the percent of school-aged females in school in countries\nworldwide\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = female.in.school/100, \n          x = perc.1dollar/100\n          )\n      ) +\n  ggplot2::geom_smooth(\n      ggplot2::aes(color = \"Linear fit line\"),\n      formula = y ~ x,\n      method = \"lm\",\n      se = FALSE, \n      na.rm = TRUE\n      ) +\n  ggplot2::geom_point(\n      ggplot2::aes(color = \"Country\"), \n      size = 2, \n      alpha = .6,\n      na.rm = TRUE\n      ) +\n  ggplot2::labs(\n      y = \"Percent of school-aged females in school\",\n      x = \"Percent of citizens living on less than $1 per day\"\n      ) +\n  ggplot2::scale_x_continuous(labels = scales::percent) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::scale_color_manual(\n      name = \"\",\n      values = c(\"#7463AC\", \"gray60\") \n      )\n```\n\n::: {.cell-output-display}\n![Display correlation of percentage of citizens living on less than $1 per day and the percent of school-aged females in school in countries worldwide](08-correlation_files/figure-html/fig-graph4-cor-1.png){#fig-graph4-cor width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n:::\n:::\n\n::: callout-tip\nThe Pearson’s product-moment correlation coefficient demonstrated that\nthe percentage of females in school is negatively correlated with the\npercentage of citizens living on less than \\$1 per day (r = -0.71).\nEssentially, as the percentage of citizens living on less than \\$1 per\nday goes up, the percentage of females in school decreases in countries.\n:::\n:::\n:::\n:::\n\n## Achievement 3: Inferential statistical test for Pearson’s r {#sec-chap08-achievement3}\n\n### Introduction\n\nThe null hypothesis is tested using a <a class='glossary' title='The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (Statistics How-To)'>t-statistic</a>\ncomparing the\n<a class='glossary' title='Correlation coefficients are a standardized measure of how two variables are related, or co-vary. They are used to measure how strong a relationship is between two variables. There are several types of correlation coefficient, but the most popular is Pearson’s. Pearson’s correlation (also called Pearson’s R) is a correlation coefficient commonly used in linear regression. (Statistics How To)'>correlation coefficient of r</a> to a\nhypothesized value of zero.\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-chap08-cor-test}\n: One.sample t-test\n:::\n:::\n\n::: my-theorem-container\n$$\nt = \\frac{m_{x}-0}{se_{m_{x}}}\n$$ {#eq-chap08-one-sample-t-test}\n\n------------------------------------------------------------------------\n\n-   $m_{x}$: mean of $x$\n-   $se_{m_{x}}$: standard error of the mean of $x$\n:::\n:::\n\nBut we are not actually working with means, but instead comparing the\ncorrelation of $r_{xy}$ to zero.\n\n::: my-theorem\n::: my-theorem-header\n<div>\n\n: Rewriting @eq-chap08-one-sample-t-test to get the t-statistic for the\nsignificance test of *r*\n\n</div>\n:::\n\n::: my-theorem-container\n$$\nt = \\frac{r_{xy}}{se_{r_{xy}}} \n$$ {#eq-chap08-t-test-for-r}\n:::\n:::\n\nThere are multiple ways to compute the standard error for a correlation\ncoefficient:\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-chap08-se-for-r}\n: Standard error for a correlation coefficient\n:::\n:::\n\n::: my-theorem-container\n$$\nse_{r_{xy}} = \\sqrt\\frac{1-r_{xy}^2}{n-2}\n$$ {#eq-chap08-se-for-r}\n:::\n:::\n\nNow we can substitute $se_{r_{xy}}$ into the t-statistic of\n@eq-chap08-t-test-for-r and simplify the formula.\n\n::: my-theorem\n::: my-theorem-header\n<div>\n\n: t-statistic for the significance test of r\n\n</div>\n:::\n\n::: my-theorem-container\n$$\n\\begin{align*}\nt = \\frac{r_{xy}}{\\sqrt\\frac{1-r_{xy}^2}{n-2}} =\\\\\nt = \\frac{r_{xy}\\sqrt{n-2}}{\\sqrt{1-r_{xy}^2}}\n\\end{align*}\n$$ {#eq-chap08-t-test-for-significance-test-r}\n:::\n:::\n\n### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: callout-note\n-   **H0**: There is no relationship between the two variables (r = 0).\n-   **HA**: There is a relationship between the two variables (r ≠ 0).\n:::\n\n### NHST Step 2\n\nCompute the test statistic.\n\n::: my-example\n::: my-example-header\n::: {#exm-ID-text}\n: Compute t-statistic for the significance test of r\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### manual\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-cor-test-manual}\n: Compute t-statistic for the significance test of *r* manually\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_data <- water_educ |> \n  tidyr::drop_na(perc.basic2015water)  |> \n  tidyr::drop_na(female.in.school) |> \n  dplyr::summarize(\n      cor_females_water = cor(\n          x = perc.basic2015water,\n          y = female.in.school\n          ),\n      sample_n = dplyr::n()\n      )\n\n(test_data$cor_females_water * (sqrt(test_data$sample_n - 2))) /\n    (sqrt(1 - (test_data$cor_females_water^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 13.32774\n```\n\n\n:::\n:::\n\n:::\n:::\n\n###### cor.test()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-cor-test-pearson}\n: Compute t-statistic for the significance test of *r* with\n`stats::cor.test()`\n:::\n:::\n\n::: my-r-code-container\n\n::: {#tbl-cor-test-pearson .cell tbl-cap='T-statistic for the significance test of *r* with `stats::cor.test()`'}\n\n```{.r .cell-code}\n# cor.test(x = water_educ$perc.basic2015water,\n#          y = water_educ$female.in.school)\n\n# using instead the formula interface\ncor.test(\n    formula = ~ female.in.school + perc.basic2015water,\n    data = water_educ\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tPearson's product-moment correlation\n#> \n#> data:  female.in.school and perc.basic2015water\n#> t = 13.328, df = 94, p-value < 2.2e-16\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.7258599 0.8683663\n#> sample estimates:\n#>       cor \n#> 0.8086651\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nI have used the formula interface because it has a different syntax as I\nthought. My first trials were with\n`female.in.school ~ perc.basic2015water` but this didn't work. The\n(last) example in the help page demonstrated to me the other syntax.\n\nNote that it is not necessary to remove NA’s before applying\n`cor.test()` in both cases.\n:::\n:::\n:::\n:::\n:::\n\n------------------------------------------------------------------------\n\n### NHST Step 3\n\nReview and interpret the test statistics: Calculate the probability that\nyour test statistic is at least as big as it is if there is no\nrelationship (i.e., the null is true).\n\nThe very tiny p-value is statistically significant.\n\n### NHST Step 4\n\nConclude and write report.\n\n::: callout-tip\nThe percentage of people who have basic access to water is statistically\nsignificantly, positively, and very strongly correlated with the\npercentage of primary- and secondary-age females in school in a country\n\\[r = .81; t(94) = 13.33; p \\< .05\\]. As the percentage of people living\nwith basic access to water goes up, the percentage of females in school\nalso goes up. While the correlation is .81 in the sample, it is likely\nbetween .73 and .87 in the population (95% CI: .73–.87).\n:::\n\n## Achievement 4: Coefficient of determiniation as effect size {#sec-chap08-achievement4}\n\nPearson’r is already a kind of effect size because it measures the\nstrength of a relationship. But with the\n<a class='glossary' title='Coefficient of determination is the percentage of variance in one variable that is accounted for by another variable or by a group of variables; often referred to as R-squared and used to determine model fit for linear models. (SwR, Glossary)'>coefficient of determination</a> $R^2$\n(also $r^2$) there is another effect size measure with a more direct\ninterpretation. The coefficient of determination is the percentage of\nthe variance in one variable that is shared, or explained, by the other\nvariable.\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-chap08-formula-r-squared}\n: Computing the coefficient of determination $R^2$\n:::\n:::\n\n::: my-theorem-container\n$$\nr_{xy}^2 = (\\frac{cov_{xy}}{s_{x}s_{y}})^2\n$$ {#eq-chap08-r-squared}\n:::\n:::\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-compute-r-squared}\n: Compute r-squared ($R^2$)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n(stats::cor.test(\n    x = water_educ$perc.basic2015water, \n    y = water_educ$female.in.school)$estimate\n)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       cor \n#> 0.6539392\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThe `stats::cor.test()` function creates an object of type `htest` which\nis a list of 9 different object. One of these object is the numeric\nvector `estimate` that holds the correlation value. There are two option\nto calculate r-squared:\n\n1.  Assign the result of `stats::cor.test()` function to a named object.\n    Append `$estimate^2` to this object to get r-squared. I have this\n    done in one step, and appended `$estimate^2` at the end of the\n    function without providing an interim object.\n2.  You could calculate the correlation with `stats:cor()` or\n    `stats::cor.test()` and then take the result and square it to get\n    r-squared. But this method is more error-prone.\n:::\n:::\n\n## Achievement 5: Checking assumptions for Pearson’s r {#sec-chap08-achievement5}\n\n### Introduction {#sec-chap08-check-independence}\n\n::: {#bul-assumptions-pearson-r}\n::: my-bullet-list\n::: my-bullet-list-header\nBullet List\n:::\n\n::: my-bullet-list-container\n-   Observations are independent (@sec-chap08-check-independence).\n-   Both variables are continuous (@sec-chap08-check-continuous).\n-   Both variables are normally distributed\n    (@sec-chap08-check-normality).\n-   The relationship between the two variables is linear\n    (<a class='glossary' title='Linearity is the assumption of some statistical models that requires the outcome, or transformed outcome, to have a linear relationship with numeric predictors, where linear relationships are relationships that are evenly distributed around a line. (SwR, Glossary)'>linearity</a>) (@sec-chap08-check-linearity).\n-   The variance is constant with the points distributed equally around\n    the line (<a class='glossary' title='Homoscedasticity is [homogeneity of variances], contrast is [Heteroscedasticity]. Homoscedasticity is an assumption of correlation and linear regression that requires that the variance of y be constant across all the values of x; visually, this assumption would show points along a fit line between x and y being evenly spread on either side of the line for the full range of the relationship. (SwR, Glossary)'>homoscedasticity</a>)\n    {@sec-chap08-homoscedasticity).\n:::\n:::\n\nAssumptions for Pearson’s r\n:::\n\n------------------------------------------------------------------------\n\nSo far the book had mentioned siblings and other family members or\ntesting the same individuals several time as examples for not\nindependent observations. Now we got two more examples:\n\n-   Countries that are geographically close to each other, or that are\n    in the same geographic region, may be more likely to share\n    characteristics and therefore fail this assumption.\n-   Countries in the analysis were those reporting data on the variables\n    of interest, rather than a random sample of countries. Countries\n    reporting data may be different from countries missing data. For\n    example, they may have better computing infrastructure and more\n    human and financial resources to afford to collect, store, and\n    report data.\n\n### Continuous variables {#sec-chap08-check-continuous}\n\nBoth variables need to be of type `numeric`. In our case we have the\nnumber of countries as integer variable: Counting something is integer,\nmeasuring something is continuous. But in our case it can be treated\nstatistically like a continuous variable.\n\nThe same is true with percent values, but there are some worries how to\nmodel percentages statistically.\n\n> A couple of … papers suggested that percentage variables are\n> problematic for statistical models that have the purpose of predicting\n> values of the outcome because predictions can fall outside the range\n> of 0 to 100.\n\n::: my-resource\n::: my-resource-header\n::: {#lem-chap08-percentage-data}\nDealing with percentage data\n:::\n:::\n\n::: my-resource-container\n-   Logistic regression [@zhao2001]\n-   Beta regression [@schmid2013; @cribari-neto2010; @ferrari2004]\n-   Transforming the percentage\n-   Recoding the variable to categorical and using a nonparametric\n    method like <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a>.\n:::\n:::\n\n### Normality {#sec-chap08-check-normality}\n\nComparing <a class='glossary' title='Histograms are visual displays of data used to examine the distribution of a numeric variable. (SwR, Glossary)'>histograms</a> and\n<a class='glossary' title='A quantile-quantile plot is a visualization of data using probabilities to show how closely a variable follows a normal distribution. (SwR, Glossary) This plot is made up of points below which a certain percentage of the observations fall. On the x-axis are normally distributed values with a mean of 0 and a standard deviation of 1. On the y-axis are the observations from the data. If the data are normally distributed, the values will form a diagonal line through the graph. (SwR, chapter 6)'>Q-Q plots</a> is one of the most applied\ntechniques to test the normality assumption. I am also using histograms\nwith an overlaid normal distribution and have an extra function\ndeveloped for this recurring task.\n\nI will provide all three different graphs here one again, although I\nhave already understood and memorized these practices.\n\n::: my-example\n::: my-example-header\n::: {#exm-chap08-normality-assumption}\n: Checking the normality assumption\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### hist female\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-normality-female-hist}\n: Check normality of `female.in.school` variable\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ  |> \n  ggplot2::ggplot(\n      ggplot2::aes(x = female.in.school / 100)) +\n  ggplot2::geom_histogram(\n      fill = \"#7463AC\", \n      col = \"white\",\n      bins = 30,\n      na.rm = TRUE\n  ) +\n  ggplot2::labs(\n      x = \"Percent of school-aged females in school\",\n      y = \"Number of countries\"\n  ) +\n  ggplot2::scale_x_continuous(\n      labels = scales::percent\n  )\n```\n\n::: {.cell-output-display}\n![Distribution of percentage of school-aged females in school](08-correlation_files/figure-html/fig-normality-female-hist-1.png){#fig-normality-female-hist width=672}\n:::\n:::\n\n:::\n:::\n\n###### dnorm female\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-normality-female-hist-dnorm}\n: Percentage of school-aged females in school with an overlaid normal\ndistribution\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_hist_dnorm(\n    df = water_educ,\n    v = water_educ$female.in.school / 100,\n    n_bins = 30,\n    x_label = \"Percent of school-aged females in school\"\n    ) +\n  ggplot2::scale_x_continuous(labels = scales::percent)\n```\n\n::: {.cell-output-display}\n![Distribution of percentage of school-aged females in school](08-correlation_files/figure-html/fig-normality-female-hist-dnorm-1.png){#fig-normality-female-hist-dnorm width=672}\n:::\n:::\n\n:::\n:::\n\n###### abline()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-normality-female-qqplot-abline}\n: Comparison of the distribution of school-aged females in school with\nthe theoretical normal distribution\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ  |> \n  ggplot2::ggplot(\n      ggplot2::aes(sample = female.in.school)\n  ) +\n  ggplot2::stat_qq(\n      ggplot2::aes(color = \"Country\"),\n      alpha = .6\n  ) +\n  ggplot2::geom_abline(\n      ggplot2::aes(\n          intercept = base::mean(female.in.school),\n          slope = stats::sd(female.in.school),\n          linetype = \"Normally distributed\"\n      ),\n          color = \"gray60\",\n          linewidth = 1\n  ) +\n  ggplot2::labs(\n      x = \"Theoretical normal distribution\",\n      y = \"Observed values of percent of\\nschool-aged females in school\",\n      title = \"Q-Q plot of female.in.school with `geom_abline()` and `ylim()`\") +\n  ggplot2::ylim(0,100) +\n  ggplot2::scale_linetype_manual(values = \"solid\", name = \"\") +\n  ggplot2::scale_color_manual(values = \"purple4\", name = \"\")\n```\n\n::: {.cell-output-display}\n![Q-Q-Plot: Distribution of school-aged females in school compared with the theoretical normal distribution](08-correlation_files/figure-html/fig-normality-female-qqplot-abline-1.png){#fig-normality-female-qqplot-abline width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThis graph is the replication of Figure 8.15. It uses\n`ggplot2::geom_abline()` by calculating the mean as intercept and the\nslop as standard deviation. This is more complex as the\n`ggplot2::geom_qq_line()` resp. `ggplot2::stat_qq_line()` but has the\nadvantage that the legend displays the line symbol with the same slope.\n\nA more simple alternative is `ggplot2::geom_qq_line()` resp.\n`ggplot2::stat_qq_line()` because these commands compute automatically\nthe slope and intercept of the line connecting the points at specified\nquartiles of the theoretical and sample distributions. I have this more\nsimple approach already used when I checked the <a class='glossary' title='A t-test is a type of statistical analysis used to compare the averages of two groups and determine whether the differences between them are more likely to arise from random chance. (Wikipedia)'>t-test</a>\nassumptions in @sec-chap06-achievement6.\n\nBut here we are using percentages, e.g. we need to limit the y-axis to\nvalues between 0 and 100%. And this restrictions prevents to show the\nline of the theoretical normal distribution.\n:::\n:::\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Do not forget, that the required aesthetic for the q-q-plot\nis \"sample\" and not \"x\"!\n:::\n:::\n\n###### stat_qq_line()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-normality-female-qqplot-stat_qq-line}\n: Comparison of the distribution of school-aged females in school with\nthe theoretical normal distribution\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- water_educ  |> \n  ggplot2::ggplot(\n      ggplot2::aes(sample = female.in.school)\n  ) +\n  ggplot2::stat_qq(\n      ggplot2::aes(color = \"Country\"),\n      alpha = .6\n  ) +\n  ggplot2::stat_qq_line(\n      ggplot2::aes(linetype = \"Normally distributed\"),\n     linewidth = 1,\n     color = \"grey60\",\n     fullrange = TRUE\n  ) +\n  ggplot2::labs(\n      x = \"Theoretical normal distribution\",\n      y = \"Observed values of percent of\\nschool-aged females in school\",\n      title = \"Q-Q plot of female.in.school\\nwith `stat__qq_line()` witht `ylim()`\") +\n  ggplot2::ylim(0,100) +\n  ggplot2::scale_linetype_manual(values = \"solid\", name = \"\") +\n  ggplot2::scale_color_manual(values = \"purple4\", name = \"\") +\n  ggplot2::theme(legend.position = \"top\")\n\n\np2 <- water_educ  |> \n  ggplot2::ggplot(\n      ggplot2::aes(sample = female.in.school)\n  ) +\n  ggplot2::stat_qq(\n      ggplot2::aes(color = \"Country\"),\n      alpha = .6\n  ) +\n  ggplot2::stat_qq_line(\n      ggplot2::aes(linetype = \"Normally distributed\"),\n     linewidth = 1,\n     color = \"grey60\",\n     fullrange = TRUE\n  ) +\n  ggplot2::labs(\n      x = \"Theoretical normal distribution\",\n      y = \"Observed values of percent of\\nschool-aged females in school\",\n      title = \"Q-Q plot of female.in.school\\nwith `stat__qq_line()` without `ylim()`\") +\n  ggplot2::scale_linetype_manual(values = \"solid\", name = \"\") +\n  ggplot2::scale_color_manual(values = \"purple4\", name = \"\") +\n  ggplot2::theme(legend.position = \"top\")\n\ngridExtra::grid.arrange(\n    p2, p1, ncol = 2\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Removed 1 row containing missing values or values outside the scale range\n#> (`geom_path()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> `geom_path()`: Each group consists of only one observation.\n#> ℹ Do you need to adjust the group aesthetic?\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Q-Q-Plot: Distribution of school-aged females in school compared with the theoretical normal distribution](08-correlation_files/figure-html/fig-normality-female-qqplot-stat_qq-line-1.png){#fig-normality-female-qqplot-stat_qq-line width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n::: callout-warning\n-   Each group consists of only one observation. Do you need to adjust\n    the aesthetic?\n-   Removed 1 row containing missing values or values outside the scale\n    range (`geom_path()`).\n:::\n\n-   The left panel didn't use the `ggplot2::ylim(0, 100)` restriction.\n    It display the line for the theoretical normal distribution far\n    outside the upper limit.\n-   The right panel used the ylim restriction but failed to show the\n    line for the theoretical normal distribution and displays two\n    warnings.\n:::\n:::\n:::\n:::\n:::\n\nThere is nothing new when checking the normality assumption for basic\nwater access. So I will skip these two graphs.\n\n### Linearity {#sec-chap08-check-linearity}\n\nThe linearity assumption requires that the relationship between the two\nvariables falls along a line. I have already graphed the appropriate\ndata in a previous section. For instance this assumption is met in\n@fig-graph2-cor. If it is difficult to tell, a\n<a class='glossary' title='Loess curve is a graph curve that shows the relationship between two variables without constraining the line to be straight; it can be compared to a linear fit line to determine whether the relationship is close to linear or not (= checking the [linearity] assumption). The procedure originated as LOWESS (LOcally WEighted Scatter-plot Smoother). is a nonparametric method because the linearity assumptions of conventional regression methods have been relaxed. It is called local regression because the fitting at say point x is weighted toward the data nearest to x. (SwR, Glossary and LOESS Curve Fitting (Local Polynomial Regression))'>Loess curve</a> can be added to confirm linearity\nas I have done it in @fig-graph1-cor.\n\nIt is instructive to see relationships that are non-linear. The next\ngraph shows some relationships but they fall along curves instead of\nalong straight lines.\n\n![Examples for nonlinear relationships (Screenshot of book’s Figure\n8.19)](img/chap08/Nonlinear-relationships-min.png){#fig-nonlinear-relations\nfig-alt=\"The two variables seen in these two graphs are labeled x and y and are on the x and y axes respectively. Both graphs have a linear fit line as well as a less curve. The graph on the left titled 1, has x axis values that range from -10 to 5, in intervals of 5. The values on the y axis range from -100, to 100, in intervals of 50. The linear fit line in this graph is a horizontal line at the y axis value of about 30. The loess curve joins the data points in this graph in a U-shape with the midpoint at about (0, 0). The graph on the right titled 2, has x axis values that range from -10 to 5, in intervals of 5. The values on the y axis range from -100, to 100, in intervals of 50. The linear fit line in this graph is an upward-sloping line that starts at about (-65, -10) and ends at about (65, 10). The loess curve joins the data points in this graph in a curve that starts at about (-100, -10), rises sharply until about (-2.5, 0), and is parallel to the x axis until about (0.5, 0) and rises sharply again until about (10, 100). The loess curve intersects the linear fit line at three points, including at (0,0).\"\nfig-align=\"center\"}\n\nAnother example of failing the linearity assumption can be seen after\ndata transforming in @fig-check-linearity-transformed.\n\n### Homoscedasticity {#sec-chap08-homoscedasticity}\n\nAnother assumption is the equal distribution of points around the line,\nwhich is often called the assumption of\n<a class='glossary' title='Homoscedasticity is [homogeneity of variances], contrast is [Heteroscedasticity]. Homoscedasticity is an assumption of correlation and linear regression that requires that the variance of y be constant across all the values of x; visually, this assumption would show points along a fit line between x and y being evenly spread on either side of the line for the full range of the relationship. (SwR, Glossary)'>homoscedasticity</a>.\n\nBesides a visual graphical inspection the\n<a class='glossary' title='Breusch-Pagan is a statistical test for determining whether variance is constant, which is used to test the assumption of homoscedasticity; Breusch-Pagan relies on the [chi-squared] distribution and is used during assumption checking for [homoscedasticity] in [linear regression]. (SwR, Glossary)'>Breusch-Pagan test</a> could be used to\ntest the null hypothesis that the variance is constant around the line.\nThe Breusch-Pagan test relies on the <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a>\ndistribution, and the `lmtest::bptest()` function can be found in the\n{**lmtest**} package (see @pak-lmtest).\n\n::: my-example\n::: my-example-header\n::: {#exm-chap08-test-homoscedasticity}\n: Check if the homoscedasticity assumption is met\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### graph\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-test-homoscedasticity-graph}\n: Examine graphically if the equal distribution of points around the\nline (homoscedasticity assumption) is met\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nplt <- water_educ |> \n  ggplot2::remove_missing(\n        na.rm = TRUE,\n        vars = c(\"perc.basic2015water\", \"female.in.school\")\n  ) |> \n  ggplot2::ggplot(\n        ggplot2::aes(\n            y = female.in.school/100, \n            x = perc.basic2015water/100\n          )\n  ) +\n  ggplot2::geom_point(\n      ggplot2::aes(\n          size = \"Country\"\n          ), \n      color = \"purple4\", \n      alpha = .6\n  ) +\n  ggplot2::geom_smooth(\n      formula = y ~ x,\n      ggplot2::aes(\n          linetype = \"Linear fit line\"\n          ),\n      color = \"grey60\",\n      method = \"lm\",\n      se = FALSE\n      ) +\n  ggplot2::geom_segment(\n      ggplot2::aes(\n          linetype = \"homoscedasticity check\"\n      ),\n      y = 57 / 100, x = 17 / 100,\n      xend = 97 / 100, yend = 100 / 100,\n      linewidth = 0.5,\n      color = \"grey60\"\n  ) +\n    ggplot2::geom_segment(\n      ggplot2::aes(\n          linetype = \"homoscedasticity check\"\n      ),\n      x = 72 / 100, y = 25 / 100,\n      xend = 100 / 100, yend = 80 / 100,\n      linewidth = 0.5,\n      color = \"grey60\"\n  ) +\n  ggplot2::labs(\n      y = \"Percent of school-aged females in school\",\n      x = \"Percent with basic access to water\") +\n  ggplot2::scale_x_continuous(labels = scales::percent) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::scale_color_manual(\n      values = c(\"gray60\", \"darkred\"), name = \"\") +\n  ggplot2::scale_size_manual(values = 2, name = \"\") +\n  ggplot2::scale_linetype_manual(values = c(2, 1), name = \"\")\n\nbase::suppressWarnings(base::print(plt))\n```\n\n::: {.cell-output-display}\n![Check if the homoscedasticity assumption is met](08-correlation_files/figure-html/fig-homoscedasticity-graph-1.png){#fig-homoscedasticity-graph width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThis is the replication of Figure 8.20 of the book, that had no\naccompanying R code. I have applied trial and error for the\n`geom_segment()` layer. Later I noticed that I could have used the\nfigures of the last paragraph of the fig-alt description.\n\nThe funnel shape of the data indicated that the points were not evenly\nspread around the line from right to left. On the left of the graph they\nwere more spread out than on the right, where they were very close to\nthe line. This indicates the data do not meet the homoscedasticity\nassumption.\n:::\n:::\n\n::: callout-warning\nI suppressed two warning from {**ggplot2**}, one for each\n`ggplot2::geom_segment()` layer:\n\n> All aesthetics have length 1, but the data has 96 rows. Did you mean\n> to use `annotate()`?\n:::\n\n###### Breusch-Pagan\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-test-homoscedasticity-breusch-pagan}\n: Check homoscedasticity with the Breusch-Pagan test\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nlmtest::bptest(\n    formula = water_educ$female.in.school ~ water_educ$perc.basic2015water)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tstudentized Breusch-Pagan test\n#> \n#> data:  water_educ$female.in.school ~ water_educ$perc.basic2015water\n#> BP = 12.368, df = 1, p-value = 0.0004368\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThe Breusch-Pagan test statistic has a low p-value (BP = 12.37; p =\n0.0004), indicating that the null hypothesis that the variance is\nconstant would be rejected, e.g., the homoscedasticity assumption is not\nmet.\n:::\n:::\n:::\n:::\n:::\n\n### Conclusion\n\n::: {#bul-chap08-assumptions-pearson-r-summary}\n::: my-bullet-list\n::: my-bullet-list-header\nBullet List\n:::\n\n::: my-bullet-list-container\n-   Observations are independent (@sec-chap08-check-independence):\n    **No**.\n-   Both variables are continuous (@sec-chap08-check-continuous):\n    **Yes**.\n-   Both variables are normally distributed\n    (@sec-chap08-check-normality): **No**.\n-   The relationship between the two variables is linear\n    (<a class='glossary' title='Linearity is the assumption of some statistical models that requires the outcome, or transformed outcome, to have a linear relationship with numeric predictors, where linear relationships are relationships that are evenly distributed around a line. (SwR, Glossary)'>linearity</a>) (@sec-chap08-check-linearity): **Yes**.\n-   The variance is constant with the points distributed equally around\n    the line (<a class='glossary' title='Homoscedasticity is [homogeneity of variances], contrast is [Heteroscedasticity]. Homoscedasticity is an assumption of correlation and linear regression that requires that the variance of y be constant across all the values of x; visually, this assumption would show points along a fit line between x and y being evenly spread on either side of the line for the full range of the relationship. (SwR, Glossary)'>homoscedasticity</a>)\n    {@sec-chap08-homoscedasticity): **No**.\n:::\n:::\n\nSummary of testing the assumptions for Pearson’s r in the example data\n:::\n\n------------------------------------------------------------------------\n\nThe big question is: What can be done that several of the assumptions\nare not met? The books gives some words of advice:\n\n-   Report the results and explain that the analysis does not meet\n    assumptions, so that it is unclear if what is happening in the\n    sample is a good reflection of what is happening in the population.\n-   Transform the two variables to try and meet the assumptions for\n    Pearson’s r and conduct the analysis again.\n-   Choose a different type of analysis with assumptions that can be met\n    by these data.\n\n::: my-remark\n::: my-remark-header\nMy opinion to the three tips\n:::\n\n::: my-remark-container\n-   The first advice is no solution. This strategy declares that the\n    inferential process has failed.\n-   Yes, this is a promising strategy. But as it turns out that works\n    (more or less) for the `female.in.school` but not for the\n    `perc.basic2015water` variable. More on this analysis in\n    @sec-chap08-achievement6.\n-   Instead of using <a class='glossary' title='Pearson’s r is a statistic that indicates the strength and direction of the relationship between two numeric variables that meet certain assumptions. (SwR, Glossary)'>Pearson’s r</a> one could\n    use <a class='glossary' title='Spearman’s rho a statistical test used to examine the strength, direction, and significance of the relationship between two numeric variables when they do not meet the assumptions for [Pearson]’s r. (SwR, Glossary)'>Spearman’s rho</a> which does not have\n    the same strict assumptions. See @sec-chap08-achievement7.\n\nCompare testing the assumptions for Pearson’s r in the example data in\n@bul-chap08-assumptions-pearson-r-summary with testing the assumptions\nfor Pearson’s r with the transformed data in\n@bul-chap08-assumptions-pearson-r-transformed-summary.\n:::\n:::\n\n## Achievement 6: Transforming data {#sec-chap08-achievement6}\n\n### Introduction\n\nOne of the ways to deal with data that do not meet assumptions for\nPearson’s *r* is to use a data transformation and examine the\nrelationship between the transformed variables.\n\n::: my-bulletbox\n::: my-bulletbox-header\n::: my-bulletbox-icon\n:::\n\n::: {#bul-chap08-data-transformation-types}\n:::\n\n: Types of data transformations\n:::\n\n::: my-bulletbox-body\n-   <a class='glossary' title='Nonlinear transformations are transformations that increases (or decreases) the linear relationship between two variables by applying an exponent (i.e., [power transformation]) or other function to one or both of the variables. (SwR, Glossary)'>Linear transformations</a> keep existing linear\n    relationships between variables, often by multiplying or dividing\n    one or both of the variables by some amount.\n-   <a class='glossary' title='Nonlinear transformations are transformations that increases (or decreases) the linear relationship between two variables by applying an exponent (i.e., [power transformation]) or other function to one or both of the variables. (SwR, Glossary)'>Nonlinear transformations</a> increase (or decrease) the\n    linear relationship between two variables by applying an exponent\n    (i.e., <a class='glossary' title='Power transformations are transformations of a measure using an exponent like squaring or cubing or taking the square root or cube root; power transformations are nonlinear transformations. (SwR, Glossary)'>power transformations</a>) or other function to\n    one or both of the variables.\n-   <a class='glossary' title='Logit transformations are transformations that takes the log value of p/(1-p); this transformation is often used to normalize percentage data and is used in the logistic model to transform the outcome. (SwR, Glossary)'>Logit transformations</a> and\n    <a class='glossary' title='Arcsine transformation are data transformation techniques often recommended to normalize percent or proportion data; the arcsine transformation uses the inverse of the sine function and the square root of the variable to transform. (SwR, Glossary)'>arcsine transformations</a> are often used for variables\n    that are percentages or proportions because they account for\n    <a class='glossary' title='A floor effect happens when a variable has many observations that take the lowest value of the variable, which can indicate that the range of values was insufficient to capture the true variability of the data. (SwR, Glossary)'>floor</a> and <a class='glossary' title='A ceiling effect happens when many observations are at the highest possible value for a variable. (SwR, Glossary)'>ceiling</a> effects.\n:::\n:::\n\n### Logit transformation\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-chap08-logit-transformation}\n: Formula for the logit transformation\n:::\n:::\n\n::: my-theorem-container\n$$\ny_{logit} = log(\\frac{y}{1-y})\n$$ {#eq-chap08-logit}\n\n------------------------------------------------------------------------\n\n-   **y**: is a ~~percent~~ proportion ranging from 0 to\n    1[^08-correlation-1].\n\nThe logit transformation uses @eq-chap08-logit to make percentage data\nmore normally distributed.\n:::\n:::\n\n[^08-correlation-1]: The book says 'percent', but I believe proportion\n    would be correct, as the range of percents is from 0-100%\n\n### Arcsine transformation\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-chap08-arcsine}\n: Formula for the arcsine transformation\n:::\n:::\n\n::: my-theorem-container\n$$\ny_{arcsine} = arcsine(\\sqrt{y})\n$$ {#eq-chap08-arcsine}\n\n-   **y**: proportion ranging from 0 to 1\n\n------------------------------------------------------------------------\nThe arcsine transformation is the inverse of the sine function. This function is\nalso used to normalize percentage or proportion data by using\n@eq-chap08-arcsine to transform the variable $y$. Besides calculating manually, it could also be computed with `car::logit()` (see @pak-car and a practical example in @lst-chap09-logit-no-insurance).\n\n:::\n:::\n\n### Folded power transformation\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-chap08-folded-power}\n: Formula for the folded power transformation\n:::\n:::\n\n::: my-theorem-container\n$$\ny_{folded.power} = y^\\frac{1}{p} - (1-y)^\\frac{1}{p}\n$$ {#eq-chap08-folded-power}\n\n------------------------------------------------------------------------\n\n$p$ is the power to raise that could be calculated with\n`rcompanion::transformTukey()` (see @pak-rcompanion).\n:::\n:::\n\n### Data transforming and checking normality assumption {#sec-chap08-transformed-normality}\n\n::: my-example\n::: my-example-header\n::: {#exm-chap08-transform-data}\n: Transforming data and checking normality assumptions\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### logit & arcsine\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-transform-logit-arcsine}\n: Logit and arcsine transformation of variables `female.in.school` &\n`perc.basic2015water`\n:::\n:::\n\n::: my-r-code-container\n\n::: {#lst-chap08-transform-logit-arcsine}\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ2 <- water_educ  |> \n  dplyr::mutate(\n      logit.female.school = base::log(\n          x = (female.in.school / 100) / (1 - female.in.school / 100)\n          )\n    ) |> \n  dplyr::mutate(\n      logit.perc.basic.water = base::log(\n          x = (perc.basic2015water / 100) / (1 - perc.basic2015water / 100)\n          )\n      )  |> \n  \n  dplyr::mutate(\n      arcsin.female.school = asin(\n          x = base::sqrt(female.in.school / 100)\n          )\n      )  |> \n  dplyr::mutate(\n      arcsin.perc.basic.water = asin(\n          x = base::sqrt(perc.basic2015water/100)\n          )\n      )\n\nsave_data_file(\"chap08\", water_educ2, \"water_educ2.rds\")\n\n# check the data\nskimr::skim(water_educ2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: There was 1 warning in `dplyr::summarize()`.\n#> ℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n#>   mangled_skimmers$funs)`.\n#> ℹ In group 0: .\n#> Caused by warning:\n#> ! There was 1 warning in `dplyr::summarize()`.\n#> ℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n#>   mangled_skimmers$funs)`.\n#> Caused by warning in `inline_hist()`:\n#> ! Variable contains Inf or -Inf value(s) that were converted to NA.\n```\n\n\n:::\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |            |\n|:------------------------|:-----------|\n|Name                     |water_educ2 |\n|Number of rows           |97          |\n|Number of columns        |14          |\n|_______________________  |            |\n|Column type frequency:   |            |\n|character                |1           |\n|numeric                  |13          |\n|________________________ |            |\n|Group variables          |None        |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|country       |         0|             1|   4|  52|     0|       97|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable           | n_missing| complete_rate|  mean|    sd|    p0|   p25|   p50|    p75|   p100|hist  |\n|:-----------------------|---------:|-------------:|-----:|-----:|-----:|-----:|-----:|------:|------:|:-----|\n|med.age                 |         0|          1.00| 30.33|  8.69| 15.00| 22.50| 29.70|  39.00|  45.90|▆▇▇▆▇ |\n|perc.1dollar            |        33|          0.66| 13.63| 20.52|  1.00|  1.00|  1.65|  17.12|  83.80|▇▁▁▁▁ |\n|perc.basic2015sani      |         0|          1.00| 79.73| 27.18|  7.00| 73.00| 93.00|  99.00| 100.00|▁▁▁▁▇ |\n|perc.safe2015sani       |        47|          0.52| 71.50| 25.84|  9.00| 61.25| 76.50|  93.00| 100.00|▂▂▁▆▇ |\n|perc.basic2015water     |         1|          0.99| 90.16| 15.82| 19.00| 88.75| 97.00| 100.00| 100.00|▁▁▁▁▇ |\n|perc.safe2015water      |        45|          0.54| 83.38| 22.34| 11.00| 73.75| 94.00|  98.00| 100.00|▁▁▁▂▇ |\n|perc.in.school          |         0|          1.00| 87.02| 13.94| 33.32| 83.24| 92.02|  95.81|  99.44|▁▁▁▂▇ |\n|female.in.school        |         0|          1.00| 87.06| 15.10| 27.86| 83.70| 92.72|  96.61|  99.65|▁▁▁▂▇ |\n|male.in.school          |         0|          1.00| 87.00| 12.95| 38.66| 82.68| 91.50|  95.57|  99.36|▁▁▁▂▇ |\n|logit.female.school     |         0|          1.00|  2.46|  1.31| -0.95|  1.64|  2.54|   3.35|   5.66|▂▅▇▆▂ |\n|logit.perc.basic.water  |         1|          0.99|   Inf|   NaN| -1.45|  2.07|  3.48|    Inf|    Inf|▁▅▅▇▇ |\n|arcsin.female.school    |         0|          1.00|  1.24|  0.20|  0.56|  1.16|  1.30|   1.39|   1.51|▁▁▂▇▇ |\n|arcsin.perc.basic.water |         1|          0.99|  1.34|  0.25|  0.45|  1.23|  1.40|   1.57|   1.57|▁▁▂▃▇ |\n\n\n:::\n:::\n\n\nLogit and arcsine transformation of variables `female.in.school` &\n`perc.basic2015water`\n\n:::\n------------------------------------------------------------------------\n\nWe got several warnings. We can easily see that our new variable\n`logit.perc.basic.water` has problems because it contains infinite\nvalues. The reason is the formula for the logit function, that has as\ndenominator $1-y$. When $y = 1$ for 100%, the denominator is zero and it\nis impossible to divide by zero.\n\nThe intuitive idea to change the original data slightly, e.g., to\nsubtract a very tiny amount so that the results is not zero anymore, is\na bad idea. It destroys the reproducibility and adds error into the\ndataset. A better solution is to try instead another transformation.\n\nThe suggestion is to use the folded power transformation\n(@thm-chap08-folded-power). But before we can apply the formula we need\nto compute the power for the transforming of the variables.\n:::\n:::\n\n###### transformTukey1\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-transform-tukey}\n: Tukey transformation to get power for transforming the variables\n`female.in.school` & `perc.basic2015water`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n## use Tukey transformation to get power for transforming\n## female in school variable to more normal distribution\np_female <- rcompanion::transformTukey(\n    x = water_educ$female.in.school,\n    plotit = FALSE,\n    quiet = TRUE,\n    returnLambda = TRUE\n    )\n\n\n# use Tukey transformation to get power for transforming\n# basic 2015 water variable to more normal distribution\np_water <- rcompanion::transformTukey(\n    x = water_educ$perc.basic2015water,\n    plotit = FALSE,\n    quiet = TRUE,\n    returnLambda = TRUE\n    )\n```\n:::\n\n\n------------------------------------------------------------------------\n\nUsing the Tukey transformation we get as power for transforming for the\n\n-   female.in.school variable: 8.85 and for the\n-   perc.basic2015water variabe: 9.975.\n:::\n:::\n\n###### transformTukey2\n\n::: my-r-code\n::: my-r-code-header\n<div>\n\n: Tukey transformation to get power for transforming the variables\n`female.in.school` & `perc.basic2015water` with accompanying plots\n\n</div>\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\np_female <- rcompanion::transformTukey(\n    x = water_educ$female.in.school,\n    plotit = TRUE,\n    quiet = TRUE,\n    returnLambda = TRUE\n    )\n\np_water <- rcompanion::transformTukey(\n    x = water_educ$perc.basic2015water,\n    plotit = TRUE,\n    quiet = TRUE,\n    returnLambda = TRUE,\n    statistic = 2\n    )\n```\n\n::: {.cell-output-display}\n![Tukey transformation to get plots of Shapiro-Wilks W or Anderson-Darling A vs. lambda, a histogram of transformed values, and a quantile-quantile plot of transformed values.](08-correlation_files/figure-html/fig-transform-folded-power-1.png){#fig-transform-folded-power-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Tukey transformation to get plots of Shapiro-Wilks W or Anderson-Darling A vs. lambda, a histogram of transformed values, and a quantile-quantile plot of transformed values.](08-correlation_files/figure-html/fig-transform-folded-power-2.png){#fig-transform-folded-power-2 width=672}\n:::\n\n::: {.cell-output-display}\n![Tukey transformation to get plots of Shapiro-Wilks W or Anderson-Darling A vs. lambda, a histogram of transformed values, and a quantile-quantile plot of transformed values.](08-correlation_files/figure-html/fig-transform-folded-power-3.png){#fig-transform-folded-power-3 width=672}\n:::\n\n::: {.cell-output-display}\n![Tukey transformation to get plots of Shapiro-Wilks W or Anderson-Darling A vs. lambda, a histogram of transformed values, and a quantile-quantile plot of transformed values.](08-correlation_files/figure-html/fig-transform-folded-power-4.png){#fig-transform-folded-power-4 width=672}\n:::\n\n::: {.cell-output-display}\n![Tukey transformation to get plots of Shapiro-Wilks W or Anderson-Darling A vs. lambda, a histogram of transformed values, and a quantile-quantile plot of transformed values.](08-correlation_files/figure-html/fig-transform-folded-power-5.png){#fig-transform-folded-power-5 width=672}\n:::\n\n::: {.cell-output-display}\n![Tukey transformation to get plots of Shapiro-Wilks W or Anderson-Darling A vs. lambda, a histogram of transformed values, and a quantile-quantile plot of transformed values.](08-correlation_files/figure-html/fig-transform-folded-power-6.png){#fig-transform-folded-power-6 width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nAfter changing the argument from `plotit = FALSE` to the default value\nof `plotit = TRUE` we get different plots for the normality assumption.\n\n-   The first three graphs are plots for\n    <a class='glossary' title='The Shapiro-Wilk test is a statistical test to determine or confirm whether a variable has a normal distribution; it is sensitive to small deviations from normality and not useful for sample sizes above 5,000 because it will nearly always find non-normality. (SwR, Glossary)'>Shapiro-Wilks W</a> vs. lambda with\n    <a class='glossary' title='Histograms are visual displays of data used to examine the distribution of a numeric variable. (SwR, Glossary)'>histogram</a> and\n    <a class='glossary' title='A quantile-quantile plot is a visualization of data using probabilities to show how closely a variable follows a normal distribution. (SwR, Glossary) This plot is made up of points below which a certain percentage of the observations fall. On the x-axis are normally distributed values with a mean of 0 and a standard deviation of 1. On the y-axis are the observations from the data. If the data are normally distributed, the values will form a diagonal line through the graph. (SwR, chapter 6)'>quantile-quantile plot</a> for the\n    `female.in.school` variable (argument: `statistic = 1`, the default\n    value).\n-   The last three graphs are plots for\n    <a class='glossary' title='The Anderson-Darling Goodness of Fit Test (AD-Test) is a measure of how well your data fits a specified distribution. It’s commonly used as a test for normality. (Statistics How-To)'>Anderson-Darling A</a> vs. lambda\n    with histogram and Q-Q-plot for the `perc.basic2015water` variable\n    (argument: `statistic = 2`).\n\nFor more information about Shapiro-Wilk and Anderson-Darling tests see\n@sec-chap06-omnibus-tests.\n:::\n:::\n\n###### power\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-folded-power-transformation}\n: Compute variable with folded power transformation\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n## create new transformation variables\nwater_educ3 <- water_educ  |> \n  dplyr::mutate(\n      arcsin.female.school = \n          base::asin(x = base::sqrt(female.in.school/100))\n      )  |> \n  dplyr::mutate(\n      arcsin.perc.basic.water = \n          base::asin(x = base::sqrt(perc.basic2015water/100))\n      ) |> \n  dplyr::mutate(\n      folded.p.female.school = \n          (female.in.school/100)^(1/p_female) - \n          (1-female.in.school/100)^(1/p_female)\n      )  |> \n  dplyr::mutate(\n      folded.p.basic.water = \n          (perc.basic2015water/100)^(1/p_water) - \n          (1-perc.basic2015water/100)^(1/p_water)\n      )\n\nsave_data_file(\"chap08\", water_educ3, \"water_educ3.rds\")\n\n# check the data\nskimr::skim(water_educ3)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |            |\n|:------------------------|:-----------|\n|Name                     |water_educ3 |\n|Number of rows           |97          |\n|Number of columns        |14          |\n|_______________________  |            |\n|Column type frequency:   |            |\n|character                |1           |\n|numeric                  |13          |\n|________________________ |            |\n|Group variables          |None        |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|country       |         0|             1|   4|  52|     0|       97|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable           | n_missing| complete_rate|  mean|    sd|    p0|   p25|   p50|    p75|   p100|hist  |\n|:-----------------------|---------:|-------------:|-----:|-----:|-----:|-----:|-----:|------:|------:|:-----|\n|med.age                 |         0|          1.00| 30.33|  8.69| 15.00| 22.50| 29.70|  39.00|  45.90|▆▇▇▆▇ |\n|perc.1dollar            |        33|          0.66| 13.63| 20.52|  1.00|  1.00|  1.65|  17.12|  83.80|▇▁▁▁▁ |\n|perc.basic2015sani      |         0|          1.00| 79.73| 27.18|  7.00| 73.00| 93.00|  99.00| 100.00|▁▁▁▁▇ |\n|perc.safe2015sani       |        47|          0.52| 71.50| 25.84|  9.00| 61.25| 76.50|  93.00| 100.00|▂▂▁▆▇ |\n|perc.basic2015water     |         1|          0.99| 90.16| 15.82| 19.00| 88.75| 97.00| 100.00| 100.00|▁▁▁▁▇ |\n|perc.safe2015water      |        45|          0.54| 83.38| 22.34| 11.00| 73.75| 94.00|  98.00| 100.00|▁▁▁▂▇ |\n|perc.in.school          |         0|          1.00| 87.02| 13.94| 33.32| 83.24| 92.02|  95.81|  99.44|▁▁▁▂▇ |\n|female.in.school        |         0|          1.00| 87.06| 15.10| 27.86| 83.70| 92.72|  96.61|  99.65|▁▁▁▂▇ |\n|male.in.school          |         0|          1.00| 87.00| 12.95| 38.66| 82.68| 91.50|  95.57|  99.36|▁▁▁▂▇ |\n|arcsin.female.school    |         0|          1.00|  1.24|  0.20|  0.56|  1.16|  1.30|   1.39|   1.51|▁▁▂▇▇ |\n|arcsin.perc.basic.water |         1|          0.99|  1.34|  0.25|  0.45|  1.23|  1.40|   1.57|   1.57|▁▁▂▃▇ |\n|folded.p.female.school  |         0|          1.00|  0.23|  0.12| -0.10|  0.17|  0.25|   0.31|   0.47|▂▂▆▇▂ |\n|folded.p.basic.water    |         1|          0.99|  0.48|  0.39| -0.13|  0.18|  0.29|   1.00|   1.00|▃▇▂▁▇ |\n\n\n:::\n:::\n\n:::\n:::\n\n###### graphs\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-check-normality-transformed-data-graphs}\n: Check the normality assumptions of the transformed data with\nhistograms\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ3 <- base::readRDS(\"data/chap08/water_educ3.rds\")\n\n# histogram of arcsin females in school (Figure 8.21)\nplt1 <- my_hist_dnorm(\n    df = water_educ3,\n    v = water_educ3$arcsin.female.school,\n    n_bins = 30,\n    x_label = \"Arcsine transformation of females in school\"\n    )\n\n# histogram of folded power transf females in school (Figure 8.22)\nplt2 <- my_hist_dnorm(\n    df = water_educ3,\n    v = water_educ3$folded.p.female.school,\n    n_bins = 30,\n    x_label = \"Folded power transformation of females in school\"\n    )\n\n# histogram of arcsine of water variable (Figure 8.23)\nplt3 <- my_hist_dnorm(\n    df = water_educ3,\n    v = water_educ3$arcsin.perc.basic.water,\n    n_bins = 30,\n    x_label = \"Arcsine transformed basic water access\"\n    )\n\n# histogram of folded power transformed water variable (Figure 8.24)\nplt4 <- my_hist_dnorm(\n    df = water_educ3,\n    v = water_educ3$folded.p.basic.water,\n    n_bins = 30,\n    x_label = \"Folded power transformed basic water access\"\n    )\n\ngridExtra::grid.arrange(\n    plt1, plt2, plt3, plt4, nrow = 2\n)\n```\n\n::: {.cell-output-display}\n![Histograms for checking normality assumptions of the transformed data](08-correlation_files/figure-html/fig-check-normality-transformed-data-graphs-1.png){#fig-check-normality-transformed-data-graphs width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n-   **Top-Left**: The arcsine transformation of females in school looks\n    better than the not transformed data in\n    @fig-normality-female-hist-dnorm. But still it is not a normal but a\n    left-skewed distribution.\n-   **Top-Right**: The folded power transformation looks much better. It\n    is still somewhat left-skewed but approaches quite well a normal\n    distribution.\n-   **Bottom-Left**: The arcsine transformation of basic water access\n    looks terrible.\n-   **Bottom-Right**: The folded power transformation of basic water\n    access looks not better, maybe even worse.\n\nThe book suggests that the `perc.basic2015water` variable should better\nbe recoded into categories. Since so many countries have 100% access,\nthe variable could be binary, with 100% access in one category and less\nthan 100% access in another category.\n\nAlthough `perc.basic2015water` did not meet the normality assumption the\nbook applies the <a class='glossary' title='Null Hypothesis Significance Testing (NHST) is a process for organizing inferential statistical tests. (SwR, Glossary)'>NHST</a> procedure. I think the reason was\njust to practice the procedure because in my opinion it would not make\nsense to apply a significance test for correlation if several of the\nassumptions are not met.\n:::\n:::\n:::\n:::\n:::\n\n### Testing assumptions for Pearson’s *r* with transformed data\n\n#### Normality\n\nThis assumption is not met, see the different graphs in\n@sec-chap08-transformed-normality.\n\n#### Linearity {#sec-chap08-linearity-transformed}\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-check-linearity-transformed}\n: Check linearity assumption with transformed data with linear fit line\nand Loess curve\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n# explore plot of transformed females in school and basic water\n# with linear fit line and Loess curve (Figure 8.25)\nwater_educ3 |> \n  tidyr::drop_na(c(\n      folded.p.female.school,\n      folded.p.basic.water\n  )) |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = folded.p.female.school, \n          x = folded.p.basic.water)\n      ) +\n  ggplot2::geom_smooth(\n      formula = y ~x,\n      ggplot2::aes(\n          color = \"linear fit line\"\n          ), \n      method = \"lm\", \n      se = FALSE\n      ) +\n  ggplot2::geom_smooth(\n      formula = y ~x,\n      ggplot2::aes(\n          color = \"Loess curve\"\n          ), \n      method = \"loess\",\n      se = FALSE\n      ) +\n  ggplot2::geom_point(\n      ggplot2::aes(\n          size = \"Country\"\n          ), \n      color = \"#7463AC\", \n      alpha = .6\n      ) +\n  ggplot2::labs(\n      y = \"Power transformed percent of females in school\",\n      x = \"Power transformed percent with basic water access\"\n      ) +\n  ggplot2::scale_color_manual(\n      name = \"Type of fit line\", \n      values = c(\"gray60\",\"darkred\")) +\n  ggplot2::scale_size_manual(values = 2)\n```\n\n::: {.cell-output-display}\n![Check linearity assumption with transformed data](08-correlation_files/figure-html/fig-check-linearity-transformed-1.png){#fig-check-linearity-transformed width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThe plot shows a pretty terrible deviation from linearity, which looks\nlike it is mostly due to all the countries with 100% basic water access.\nAn indicator for this guess is that both lines are bent by the right\nvertical line of countries with 100% basic water access. Without the\nlines would end around 0.45x / 0.5y.\n\nTransforming the data worsened the linearity assumption, as you can see\nwith a comparison of @fig-graph1-cor.\n:::\n:::\n\n#### Homoscedasticity {#sec-chap08-homoscedasticity-transformed}\n\n##### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: callout-note\n-   **H0**: The data is spread equal around the regression line.\n-   **HA**: The data is not spread equal around the regression line.\n:::\n\n##### NHST Step 2\n\nCompute the test statistic.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-check-homoscedasticity-transformed}\n: Check homoscedasticity assumption with transformed data\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\n# testing for homoscedasticity\nlmtest::bptest(\n    formula = \n        water_educ3$folded.p.female.school ~ \n        water_educ3$folded.p.basic.water\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tstudentized Breusch-Pagan test\n#> \n#> data:  water_educ3$folded.p.female.school ~ water_educ3$folded.p.basic.water\n#> BP = 6.3816, df = 1, p-value = 0.01153\n```\n\n\n:::\n:::\n\n:::\n:::\n\n##### NHST Step 3\n\nReview and interpret the test statistics: Calculate the probability that\nyour test statistic is at least as big as it is if there is no\nrelationship (i.e., the null is true).\n\nThe p-value of .01 is below .05 therefore statistically significant.\n\n##### NHST Step 4\n\nConclude and write report.\n\nWith a p-value of .01, the null hypothesis is rejected and the\nassumption fails. The data transformation worked to mostly address the\nproblem of normality for the females in school variable, but the\ntransformed data were not better for linearity or homoscedasticity.\n\n::: callout-tip\nThere was a statistically significant, positive, and strong (r = .67; t\n= 8.82; p \\< .05; 95% CI: .55–.77) relationship between the transformed\nvariables for percentage of females in school and percentage of citizens\nwith basic water access in a sample of countries. As the percentage of\ncitizens with basic water access increases, so does the percentage of\nschool-age females in school. The data failed several of the assumptions\nfor \\*\\*r\\* and so these results should not be generalized outside the\nsample.\n:::\n\n::: my-remark\n::: my-remark-header\nInferential statistics without generalizing conclusion outside the\nsample?\n:::\n\n::: my-remark-container\nI find it very disappointing that most of the time the assumptions for\nthe different tests are not met. As far as I understand all the many\ntests and hypotheses failed, so that one can't say anything outside the\ndata of the sample.\n\nIn the above summary are included a <a class='glossary' title='The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary)'>p-value</a> and a\n<a class='glossary' title='A range of values, calculated from the sample observations, that is believed, with a particular probability, to contain the true parameter value. (Cambridge Dictionary of Statistics, 4th ed., p.98)'>confidence interval</a>. As both values are for generalizing\nfrom a sample to a population and some of the assumptions are not met,\nit is --- in my opinion --- not allowed to mention these values. They\n\"could\" not omitted as the book claims but I think the should mandatory\nnot included in the summary. These results are not reliable when the\nassumptions are failed and should not be mentioned at all because that\ncreates more informative results as effectively is the case.\n\nAnother thing that annoys me after eight chapter is the high redundancy\nwith all the tests and NHST procedures. I got the impression that the\nhonest account of the book shows that there is something wrong with the\nfrequentist approach of statistics. Most of the frequentist textbooks I\nalready have read do not so thoroughly check their assumptions. I am\neager to learn more about the Bayesian alternative!\n:::\n:::\n\n### Conclusion\n\n::: {#bul-chap08-assumptions-pearson-r-transformed-summary}\n::: my-bullet-list\n::: my-bullet-list-header\nBullet List\n:::\n\n::: my-bullet-list-container\n-   Observations are independent (@sec-chap08-check-independence):\n    **No**.\n-   Both variables are continuous (@sec-chap08-check-continuous):\n    **Yes**.\n-   Both variables are normally distributed\n    (@sec-chap08-transformed-normality): **No**.\n-   The relationship between the two variables is linear\n    (<a class='glossary' title='Linearity is the assumption of some statistical models that requires the outcome, or transformed outcome, to have a linear relationship with numeric predictors, where linear relationships are relationships that are evenly distributed around a line. (SwR, Glossary)'>linearity</a>) (@sec-chap08-linearity-transformed):\n    **No**.\n-   The variance is constant with the points distributed equally around\n    the line (<a class='glossary' title='Homoscedasticity is [homogeneity of variances], contrast is [Heteroscedasticity]. Homoscedasticity is an assumption of correlation and linear regression that requires that the variance of y be constant across all the values of x; visually, this assumption would show points along a fit line between x and y being evenly spread on either side of the line for the full range of the relationship. (SwR, Glossary)'>homoscedasticity</a>)\n    {@sec-chap08-homoscedasticity-transformed): **No**.\n:::\n:::\n\nSummary of testing the assumptions for Pearson’s r with transformed data\n:::\n\n------------------------------------------------------------------------\n\nAll in all the situation has deteriorated: In contrast to the example\ndata the linearity assumption is not met after the transformation.\nCompare @fig-graph1-cor with @sec-chap08-linearity-transformed.\n\n## Achievement 7: Spearman’s rho {#sec-chap08-achievement7}\n\n### Introduction\n\n<a class='glossary' title='Spearman’s rho a statistical test used to examine the strength, direction, and significance of the relationship between two numeric variables when they do not meet the assumptions for [Pearson]’s r. (SwR, Glossary)'>Spearman’s rho</a> rank correlation coefficient\nis the most common alternative for\n<a class='glossary' title='Pearson’s r is a statistic that indicates the strength and direction of the relationship between two numeric variables that meet certain assumptions. (SwR, Glossary)'>Pearson’s r</a>. Spearman’s rho or $r_{s}$ is\njust using another transformation, but instead of computing the arcsine\nor raising the variables to a power, the values of the variables are\ntransformed into ranks, like with some of the alternatives to the\n<a class='glossary' title='A t-test is a type of statistical analysis used to compare the averages of two groups and determine whether the differences between them are more likely to arise from random chance. (Wikipedia)'>t-tests</a>. The values of the variables are\nranked from lowest to highest, and the calculations for correlation are\nconducted using the ranks instead of the raw values for the variables.\n\n### Computing Spearman’s rho\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-formula-spearman-rho}\n: Compute Spearman’s rho\n:::\n:::\n\n::: my-theorem-container\n$$\np = \\frac{6 \\sum{d^2}}{n(n^2-1)}\n$$ {#eq-chap08-spearman-rho}\n\n------------------------------------------------------------------------\n\n-   **d** is the difference between the ranks of the two variables\n-   **n** is the number of observations\n:::\n:::\n\nFor females in school and basic water access Spearman’s rho would be\ncomputed by first ranking the values of percentage of females in school\nfrom lowest to highest and then ranking the values of basic water access\nfrom lowest to highest.\n\n#### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: callout-note\n-   **H0**: There is no correlation between the percentage of females in\n    school and the percentage of citizens with basic water access (ρ =\n    0).\n-   **HA**: There is a correlation between the percentage of females in\n    school and the percentage of citizens with basic water access (ρ ≠\n    0).\n:::\n\n#### NHST Step 2\n\nCompute the test statistic.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-compute-spearman-rho}\n: Computing Spearman’s rho\n:::\n:::\n\n::: my-r-code-container\n\n::: {#tbl-compute-spearman-rho .cell tbl-cap='Computing Spearman’s rho'}\n\n```{.r .cell-code}\n(\n    spearman_female_water <- stats::cor.test(\n        x = water_educ$perc.basic2015water,\n        y = water_educ$female.in.school,\n        method = \"spearman\")\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning in cor.test.default(x = water_educ$perc.basic2015water, y =\n#> water_educ$female.in.school, : Cannot compute exact p-value with ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tSpearman's rank correlation rho\n#> \n#> data:  water_educ$perc.basic2015water and water_educ$female.in.school\n#> S = 34050, p-value < 2.2e-16\n#> alternative hypothesis: true rho is not equal to 0\n#> sample estimates:\n#>       rho \n#> 0.7690601\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nWhile Pearson’s r between females in school and basic water access in\n@cnj-chap08-cor-test-pearson was 0.81, $r_{s}$ is slightly lower at\n0.77.\n:::\n:::\n\nInstead of a t-statistic, the output for $r_{s}$ reports the S test\nstatistic.\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-chap08-spearman-rho-s-test-statistic}\n: Formula for Spearman’s rho S test statistic\n:::\n:::\n\n::: my-theorem-container\n$$\nS = (n^3 - n) \\frac{1-r_{s}}{6}\n$$ {#eq-chap08-spearman-rho-s-test-statistic}\n\n-   $r_{s}$: Spearman’s correlation coefficient\n-   **n**: Sample size\n:::\n:::\n\nThe <a class='glossary' title='The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary)'>p-value</a> in the output of the `stats::cor.test()`\nfunction is not from the S test statistic. Instead, it is determined by\ncomputing an approximation of the <a class='glossary' title='The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (Statistics How-To)'>t-statistic</a> and\n<a class='glossary' title='Degree of Freedom (df) is the number of pieces of information that are allowed to vary in computing a statistic before the remaining pieces of information are known; degrees of freedom are often used as parameters for distributions (e.g., chi-squared, F). (SwR, Glossary)'>degrees of freedom</a>.\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-chap08-approx-t-statistic}\n: Approximation of t-statistic\n:::\n:::\n\n::: my-theorem-container\n$$\nt_{s} = r_{s}\\sqrt\\frac{n-2}{1-r_{s}^2}\n$$ {#eq-chap08-approx-t-statistic}\n:::\n:::\n\nWhile it is not included in the output from R, the t-statistic can be\ncomputed easily by using R as a calculator.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-approx-t-statistics-spearman}\n: Approximating t-statistics for Spearman’s rho manually\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ |> \n  tidyr::drop_na(c(\n      perc.basic2015water,\n      female.in.school\n      )\n  ) |> \n  dplyr::summarize(\n      n = dplyr::n(),\n      t_spearman = \n          spearman_female_water$estimate * \n          base::sqrt((n - 2) / (1 - spearman_female_water$estimate^2))\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 2\n#>       n t_spearman\n#>   <int>      <dbl>\n#> 1    96       11.7\n```\n\n\n:::\n:::\n\n:::\n:::\n\n#### NHST Step 3\n\nReview and interpret the test statistics: Calculate the probability that\nyour test statistic is at least as big as it is if there is no\nrelationship (i.e., the null is true).\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-t-dist-value}\n: Display a student-t distribution\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot() +\n    ggplot2::xlim(-5, 15) +\n    ggplot2::geom_function(\n        fun = dt,\n        args = list(df = 94)\n        ) +\n    ggplot2::geom_vline(\n        xintercept = 11.67,\n        color = \"darkred\") +\n    ggplot2::labs(\n        x = \"t-value\",\n        y = \"Density\"\n    )\n```\n\n::: {.cell-output-display}\n![Student-t distribution with 94 degress of freedom and a vertical line at t = 11.67](08-correlation_files/figure-html/fig-t-dist-94-df-with-t-value-1.png){#fig-t-dist-94-df-with-t-value width=192}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nIn this case, t is 11.67 with 94 degrees of freedom (n = 96). A quick\nplot of the t-distribution with 94 degrees of freedom revealed that the\nprobability of a t-statistic this big or bigger would be very tiny if\nthe null hypothesis were true.\n:::\n:::\n\n#### NHST Step 4\n\nConclude and write report. With this tiny p-value we have to reject the\nNull.\n\n::: callout-tip\nThere was a statistically significant positive correlation between basic\naccess to water and females in school $(r_{s} = 0.77; p < .001)$. As the\npercentage of the population with basic access to water increases, so\ndoes the percentage of females in school.\n:::\n\n### Checking assumptions for Spearman’s rho\n\n#### Introduction\n\n::: {#bul-assumptions-spearman-rho}\n::: my-bullet-list\n::: my-bullet-list-header\nBullet List\n:::\n\n::: my-bullet-list-container\n-   Observations are independent (@sec-chap08-check-independence).\n-   Both variables must be at least ordinal or even closer to\n    continuous. (@sec-chap08-check-continuous).\n-   The relationship between the two variables must be monotonic.\n:::\n:::\n\nAssumptions for Spearman’s rho\n:::\n\n------------------------------------------------------------------------\n\n#### Independence of observations\n\nNothing has changed with the data source. This assumption is still not\nmet. (See @sec-chap08-check-independence for the argumentation.)\n\n#### At least ordinal data {#sec-chap08-ordinal}\n\nThis assumption is met, because both variables are continuous.\n\n#### Monotonic {#sec-chap08-monotonic}\n\nA <a class='glossary' title='Monotonic is a statistical relationship that, when visualized, goes up or down, but not both. (SwR, Glossary)'>monotonic</a> relationship is a relationship that goes in\nonly one direction, e.g. the relationship can have curves as long as it\ngoes always in the same direction. For instance in a positive\ncorrelation the rate of the ascending y-value can change but must not be\nunder 0, e.g. reversing the direction.\n\nThe following screenshot from the books demonstrates this with different\nexamples:\n\n![Monotonic relationship examples (Screenshot of book’s Figure\n8.27)](img/chap08/monotonic-demonstration-min.png){#fig-monotonic-examples\nfig-alt=\"The variables on the x and y axes are labeled x and y respectively, on all three graphs. The values of x on the x axis range from 0 to 10, in intervals of 5 and the y values on the y axis range from -20 to 500, in intervals of 250. The first graph is titled, monotonic (negative corr), and the data points on this graph are clustered along the y axis value of 0 and -125, and between the x axis values of -2.5 and 5. The loess curve seen in this graph starts at about (0, 0) and curves downward, steeply until about (8, -245). The second graph is titled, monotonic (positive corr), and the data points on this graph are clustered along the y axis value of 0 and 125, and between the x axis values of -2.5 and 5. The loess curve seen in this graph starts at about (0, 0) and curves upward, steeply until about (8, 450). The last graph is titled, not monotonic, and the data points on this graph are clustered closer along the y axis value of 0 and 375, and between the x axis values of -2.5 and 5. The loess curve seen in this graph starts at about (0, 187) and curves upward to form two small peaks before falling below the start point and then rising against steeply to form the third peak at about (7, 310) an then falling again.\"\nfig-align=\"center\"}\n\nThe Loess curve in @fig-check-linearity-transformed only goes up, which\ndemonstrates that the relationship between females in school and basic\nwater access meets the monotonic assumption.\n\n#### Conclusion\n\n<div>\n\n-   Observations are independent (@sec-chap08-check-independence).\n    **No**\n-   Both variables must be at least ordinal or even closer to\n    continuous. (@sec-chap08-ordinal). **Yes**\n-   The relationship between the two variables must be monotonic.\n    (@sec-chap08-monotonic) **Yes**\n\nSummary of testing the assumptions for Spearman’s rho\n\n</div>\n\nSpearman’s rho met more assumptions than Pearson’s r with the original\ndata or with the transformed variables. Even so, the independent\nobservation assumption failed, so any reporting should stick to\ndescriptive statistics about the sample and not generalize to the\npopulation.\n\n::: callout-tip\nThere was a positive correlation between basic access to water and\nfemales in school ($r_{s} = 0.77$). As the percentage of the population\nwith basic access to water increases, so does the percentage of females\nin school.\n:::\n\n::: my-remark\n::: my-remark-header\nMany assumptions not met\n:::\n\n::: my-remark-container\nWith the exception of the <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a> test of systolic\nblood pressure in @sec-chap05 all of the test assumptions have failed.\n\nThis is not only disappointing but I believe also a disaster for\nfrequentist inferential statistics. As far as I understood it means that\n--- with the one mentioned exception --- we cannot say anything about\nthe population parameters and have to stick with the description of the\nsample. Instead of inferential statistics just descriptive statistics.\n\nI am not sure if the situation would be better with Bayesian statistics.\nI still have alsmost no experience with Bayesian statistics. But after a\nquick research I got the impression that all the assumptions that hold\nfor the frequentist approach must also to be met with the Bayesian\nframework. (See: [What are the assumptions in bayesian\nstatistics?](https://stats.stackexchange.com/questions/435298/what-are-the-assumptions-in-bayesian-statistics)\nin CrossValidated)\n:::\n:::\n\n## Achievement 8: Partial correlation {#sec-chap08-achievement8}\n\n### Introduction\n\n<a class='glossary' title='Partial correlation is a standardized measure of the amount of variance two variables share after accounting for variance they both share with a third variable. (SwR, Glossary)'>Partial corrections</a> is a method for\nexamining how multiple variables share variance with each other.\n\nFor instance it could be the case that females in school and basic water\naccess might both be related to poverty, and that poverty might be the\nreason both of these variables increase at the same time. The\nargumentation is: Countries with higher poverty have fewer females in\nschool and lower percentages of people with basic water access. So in\nthe end poverty was the reason for the shared variance between females\nin school and basic water access.\n\n### Computing Pearson’ r partial correlation\n\n::: my-example\n::: my-example-header\n::: {#exm-chap08-partial-correlation}\n: Computing Pearson’ r partial correlation\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### stats::cor()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-pearson-cor}\n: Examine bivariate Pearson’s r correlation\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ4 <- water_educ |> \n    dplyr::select(\n        female.in.school,\n        perc.basic2015water,\n        perc.1dollar\n    ) |> \n    tidyr::drop_na() \n\nsave_data_file(\"chap08\", water_educ4, \"water_educ4.rds\")\n\nwater_educ4 |> \n    dplyr::summarize(\n        female_water = stats::cor(\n            x = female.in.school,\n            y = perc.basic2015water\n        ),\n        female_dollar = stats::cor(\n            x = female.in.school,\n            y = perc.1dollar\n        ),\n        water_dolloar = stats::cor(\n            x = perc.basic2015water,\n            y = perc.1dollar\n        )\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 3\n#>   female_water female_dollar water_dolloar\n#>          <dbl>         <dbl>         <dbl>\n#> 1        0.765        -0.714        -0.832\n```\n\n\n:::\n:::\n\n\nAll three correlations are strong related. Using `ppcor::pcor()`\ndetermines how they were interrelated.\n:::\n:::\n\n###### ppcor::pcor()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-partial-corr-poverty-pearson}\n: Partial correlation of Pearson’s r of poverty\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ4 |> \n    ppcor::pcor(method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $estimate\n#>                     female.in.school perc.basic2015water perc.1dollar\n#> female.in.school           1.0000000           0.4395917   -0.2178859\n#> perc.basic2015water        0.4395917           1.0000000   -0.6336436\n#> perc.1dollar              -0.2178859          -0.6336436    1.0000000\n#> \n#> $p.value\n#>                     female.in.school perc.basic2015water perc.1dollar\n#> female.in.school        0.0000000000        3.125684e-04 8.626064e-02\n#> perc.basic2015water     0.0003125684        0.000000e+00 2.490386e-08\n#> perc.1dollar            0.0862606413        2.490386e-08 0.000000e+00\n#> \n#> $statistic\n#>                     female.in.school perc.basic2015water perc.1dollar\n#> female.in.school            0.000000            3.822455    -1.743636\n#> perc.basic2015water         3.822455            0.000000    -6.397046\n#> perc.1dollar               -1.743636           -6.397046     0.000000\n#> \n#> $n\n#> [1] 64\n#> \n#> $gp\n#> [1] 1\n#> \n#> $method\n#> [1] \"pearson\"\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nWhile Pearson’s r between females in school and basic water access in\n@tbl-cor-test-pearson was 0.81, Speaman’s $r_{s}$ is in\n@tbl-compute-spearman-rho slightly lower at 0.77.\n\nLooking at the first section of the output from `ppcor::pcor()` under\nthe `$estimate` subheading, it shows the partial correlations between\nall three of the variables. The partial correlation between females in\nschool and basic water access is $r_{partial} = .44$. So, after\naccounting for poverty, the relationship between females in school and\nbasic water access is a moderate .44.\n:::\n:::\n:::\n:::\n:::\n\n### Interpretation\n\nTo understand partial correlation better some screenshots from the book\nmay help:\n\n**Shared variance and Venn diagrams for two variables**\n\n![Visualizing percentage of shared variance (Screenshot of Figure\n8.13)](img/chap08/shared-variance-examples-min.png){#fig-shared-variance-examples\nfig-alt=\"This figure has six scatter plots in two rows of three scatter plots each, and three Venn diagrams on the third row. In the scatterplots along the first column, the x axis is labeled x and the values on this axis range from -2 to 2, in intervals of 1. The y axis on both these scatterplots is labeled y, and the values on this axis range from -3 to 3, in intervals of 1. The data points on both these graphs are widely dispersed at the center of the plot area. The first graph in the first row also has a fit line that slopes slightly upward from left to right just below and above the 0 value on the y axis. The text above this graph reads, r=.1, r-squared = .01 The first graph in the second row also has a fit line that slopes slightly downward from left to right just above and below the 0 value on the y axis. The text above this graph read, r=. -1, r-squared = .01. In the second column of scatterplots, the data points are seen closer to the fit line. In the second graph on the first row, the fit line is steeper than that seen on the first graph, and slopes from the bottom left to the top right. The text above this graph reads, r=.5, rsquared = .25. In the second graph on the second row, the fit line is steeper than that seen on the first graph, and slopes from the top left to the bottom right. The text above this graph reads, r=-.5, r-squared = .25. In the third column of scatterplots, the data points are seen clustered along the fit line. In the third graph on the first row, the fit line is steepest and slopes from almost the bottom left corner, to a point close to the top right corner of the plot area. The text above this graph reads, r=.9, r-squared = .81. In the third graph on the second row, the fit line is steepest and slopes from almost the top left corner, to a point close to the bottom right corner of the plot area. The text above this graph reads, r=-.9, r-squared = .81. The third row has three Venn diagrams with two circles in each labeled y and x, on the left and right respectively. In the first Venn diagram, the two circles barely intersect and the text above reads, 1% shared variance. In the second Venn diagram, the circles intersect and overlap and the text above reads, 25% shared variance. In the third Venn diagram, the two circles intersect up to almost the middle of both circles and the text above reads, 81% shared variance. Each of these Venn diagrams align to the first, second, and third columns of scatterplots described above.\"\nfig-align=\"center\"}\n\n**Shared variance with Venn diagrams with three variables**\n\n![Visualizing shared variance with Venn diagrams with three variables\n(Screenshot Figure\n8.29)](img/chap08/shared-variance-three-variables-min.png){#fig-shared-variance-three-variables\nfig-alt=\"Three overlapping circles in different colors named 'female-school', 'basic.water' and 'less.than.dollar'. There are places where two of the circles overlap and a central place where all three circles overlap each other.\"\nfig-align=\"center\" width=\"60%\"}\n\nThere are two ways the variables overlap in Figure 8.28. There are\nplaces where just two of the variables overlap in\nfig-shared-variance-three-variables (`female.in.school` and\n`perc.basic2015water` overlap, `female.in.school` and `perc.1dollar`\noverlap, `perc.basic2015water` and `perc.1dollar` overlap), and there is\nthe space where `female.in.school` and `perc.basic2015water` and\n`perc.1dollar` all overlap in the center of the diagram.\n\n**The overlap between just two colors is the\n<a class='glossary' title='Partial correlation is a standardized measure of the amount of variance two variables share after accounting for variance they both share with a third variable. (SwR, Glossary)'>partial correlation</a> between the two\nvariables. It is the extent to which they vary in the same way after\naccounting for how they are both related to the third variable\ninvolved.**\n\nTo get the percentage of shared variance, this\n<a class='glossary' title='Coefficient of determination is the percentage of variance in one variable that is accounted for by another variable or by a group of variables; often referred to as R-squared and used to determine model fit for linear models. (SwR, Glossary)'>coefficient of determination</a> $r^2$\ncould be computed and reported as a percentage. The squared value of .44\nis .194, so 19.4% of the variance in percentage of females in school is\nshared with the percentage who have basic access to water.\n\n::: callout-note\nThe assumptions that applied to the two variables for a Pearson’s r\ncorrelation would apply to all three variables for a partial Pearson’s r\ncorrelation. Each variable would be continuous and normally distributed,\neach pair of variables would demonstrate linearity, and each pair would\nhave to have constant variances (homoscedasticity).\n:::\n\n### Computing Spearman’s rho partial correlation\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-partial-corr-poverty-spearman}\n: Partial correlation of Spearman’s rho of poverty\n:::\n:::\n\n::: my-r-code-container\n\n::: {#tbl-partial-corr-poverty-spearman .cell tbl-cap='Partial correlation of Spearman’s rho of poverty'}\n\n```{.r .cell-code}\nwater_educ4 |> \n    ppcor::pcor(method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $estimate\n#>                     female.in.school perc.basic2015water perc.1dollar\n#> female.in.school           1.0000000           0.4305931   -0.2841782\n#> perc.basic2015water        0.4305931           1.0000000   -0.5977239\n#> perc.1dollar              -0.2841782          -0.5977239    1.0000000\n#> \n#> $p.value\n#>                     female.in.school perc.basic2015water perc.1dollar\n#> female.in.school        0.0000000000        4.272781e-04 2.399667e-02\n#> perc.basic2015water     0.0004272781        0.000000e+00 2.312820e-07\n#> perc.1dollar            0.0239966731        2.312820e-07 0.000000e+00\n#> \n#> $statistic\n#>                     female.in.school perc.basic2015water perc.1dollar\n#> female.in.school            0.000000            3.726169    -2.314944\n#> perc.basic2015water         3.726169            0.000000    -5.823078\n#> perc.1dollar               -2.314944           -5.823078     0.000000\n#> \n#> $n\n#> [1] 64\n#> \n#> $gp\n#> [1] 1\n#> \n#> $method\n#> [1] \"spearman\"\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nSpeaman’s $r_{s}$ was originally in @tbl-compute-spearman-rho 0.77, but\nthe partial Spearman’s rs correlation between females in school and\nbasic water access after accounting for poverty was .43. Including\npoverty reduced the magnitude of the correlation by nearly half.\n:::\n:::\n\n### Testing significance of partial correlation\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-partial-corr-p-value}\n: Compute p-value of partial correlation\n:::\n:::\n\n::: my-r-code-container\n\n::: {#tbl-partial-corr-p-value .cell tbl-cap='Compute p-value of partial correlation with `ppcor::pcor.test()`'}\n\n```{.r .cell-code}\nppcor::pcor.test(\n    x = water_educ4$female.in.school,\n    y = water_educ4$perc.basic2015water,\n    z = water_educ4$perc.1dollar,\n    method = \"spearman\"\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    estimate      p.value statistic  n gp   Method\n#> 1 0.4305931 0.0004272781  3.726169 64  1 spearman\n```\n\n\n:::\n:::\n\n\nWith `ppcor::pcor.test()` there is a special function for testing\nsignificance of a partial correlation. But you can also take the\np-values from the result of the `ppcor::pcor()` function. Compare the\nsecond section titled `$p.value` in @tbl-partial-corr-poverty-spearman\nwith the result in @tbl-partial-corr-p-value.\n\n------------------------------------------------------------------------\n\n::: {#rep-partial-corr-wrong}\n::: callout-tip\n## Significance test wrongly reported as the assumptions for partial correlation are not met\n\nThe partial correlation between percentage of females in school and the\npercentage of citizens who have basic water access was moderate,\npositive, and statistically significant\n($r_{s}.partial = 0.43; t = 3.73; p < .05$). Even after poverty is\naccounted for, increased basic water access was moderately, positively,\nand significantly associated with an increased percentage of females in\nschool.\n\nCompare this report with @rep-partial-corr-changed-report.\n:::\n\nSignificance test wrongly reported as the assumptions for partial\ncorrelation are not met\n:::\n\n------------------------------------------------------------------------\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Assumptions not met therefore no statistically significance\ntest possible\n:::\n\n::: my-watch-out-container\nAs several assumption for the partial correlation are not met it is not\ncorrect to report about the result of a statistically significance test.\n:::\n:::\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: {#rep-partial-corr-changed-report}\n::: callout-tip\n## Report about partial correlation when the assumptions are not met\n\nThe partial correlation between the percentage of females in school and\nthe percentage of citizens who have basic water access was moderate and\npositive ($r_{s.partial} = 0.43$). Even after poverty is accounted for,\nincreased basic water access was moderately and positively associated\nwith an increased percentage of females in school. The assumptions were\nnot met, so it is not clear that the partial correlation from the sample\nof countries can be generalized to the population of all countries.\n\nCompare this report with @rep-partial-corr-wrong.\n:::\n\nReport about partial correlation when the assumptions are not met\n:::\n\n------------------------------------------------------------------------\n\n### Checking assumptions\n\nBefore reporting we have to check the assumptions. We know already that\n\n-   the independent observation assumption is not met\n-   the at least ordinal variables assumptions is met for all three\n    variables\n-   that the monotonic assumption for `female.in.school` and\n    `perc.basic2015.water` is met.\n\nWe still have to check\n\n-   the monotonic assumption for `female.in.school` and `perc.1dollar`\n-   the monotonic assumption for `perc.basic2015.water` and\n    `perc.1dollar`\n\n::: my-example\n::: my-example-header\n::: {#exm-chap08-check-monotonic-assumptionwith-poverty}\n: Check the monotonic assumptions with female in schools and basic water\naccess\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### female -poverty\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-monotonic-female-poverty}\n: Check the monotonic assumption for females and poverty\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ4 |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = female.in.school/100, \n          x = perc.1dollar/100\n          )\n      ) +\n  ggplot2::geom_smooth(\n      formula = 'y ~ x',\n      ggplot2::aes(\n            color = \"Linear fit line\"\n          ), \n      method = \"lm\", \n      se = FALSE\n      ) +\n  ggplot2::geom_smooth(\n      formula = 'y ~ x',\n      ggplot2::aes(\n            color = \"Loess curve\"\n          ), \n      method = \"loess\",\n      se = FALSE\n      ) +\n  ggplot2::geom_point(\n      ggplot2::aes(\n            size = \"Country\"\n          ), \n      color = \"#7463AC\", \n      alpha = .6\n      ) +\n  ggplot2::labs(\n      y = \"Percent of school-aged females in school\",\n      x = \"Percent living on < $1 per day\"\n      ) +\n  ggplot2::scale_x_continuous(labels = scales::percent) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::scale_color_manual(name = \"\", values = c(\"gray60\", \"darkred\")) +\n  ggplot2::scale_size_manual(name = \"\", values = 2)\n```\n\n::: {.cell-output-display}\n![Checking the monotonic assumption for females and poverty](08-correlation_files/figure-html/fig-monotonic-female-poverty-1.png){#fig-monotonic-female-poverty width=672}\n:::\n:::\n\n:::\n:::\n\n###### water - poverty\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap08-monotonic-water-poverty}\n: Check the monotonic assumption for water access and poverty\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_educ4 |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = perc.basic2015water/100, \n          x = perc.1dollar/100\n          )\n      ) +\n  ggplot2::geom_smooth(\n      formula = 'y ~ x',\n      ggplot2::aes(\n            color = \"Linear fit line\"\n          ), \n      method = \"lm\", \n      se = FALSE\n      ) +\n  ggplot2::geom_smooth(\n      formula = 'y ~ x',\n      ggplot2::aes(\n            color = \"Loess curve\"\n          ), \n      method = \"loess\",\n      se = FALSE\n      ) +\n  ggplot2::geom_point(\n      ggplot2::aes(\n            size = \"Country\"\n          ), \n      color = \"#7463AC\", \n      alpha = .6\n      ) +\n  ggplot2::labs(\n      y = \"Percent of basic water access\",\n      x = \"Percent living on < $1 per day\"\n      ) +\n  ggplot2::scale_x_continuous(labels = scales::percent) +\n  ggplot2::scale_y_continuous(labels = scales::percent) +\n  ggplot2::scale_color_manual(name = \"\", values = c(\"gray60\", \"darkred\")) +\n  ggplot2::scale_size_manual(name = \"\", values = 2)\n```\n\n::: {.cell-output-display}\n![Checking the monotonic assumption for water access and poverty](08-correlation_files/figure-html/fig-monotonic-water-poverty-1.png){#fig-monotonic-water-poverty width=672}\n:::\n:::\n\n:::\n:::\n:::\n:::\n:::\n\nIt turned out that the monotonic assumption was met for\n`female.in.school` and `perc.1dollar` but not for `perc.basic2015water`\nand `perc.1dollar`.\n\n::: my-remark\n::: my-remark-header\nAssumption of independent observations not met\n:::\n\n::: my-remark-container\nThe book mentions twice the idea to recode variables that do not met\ntheir normality, linearity or monotonic assumptions. For instance one\ncould recode the basic access to water into a binomial variable (has\nbasic access, does not have basic access). The second idea was to recode\nthe poverty variable into an ordinal variable. The ordinal variable\ncould then be used in place of the original version of the variable and\nthe $r_{s}$ analysis could be conducted again.\n\nBut then there is still the problem that the assumption of the\nindependent observation are not met. This failure has two aspects:\n\n1.  It is not a sample but a collection of data some countries have\n    provided. Therefore it could be the case that those countries\n    without data had a reason not to provide their data. They could\n    therefore different to those countries that provided data.\n2.  Neighboring countries could influence each other or have other\n    similar characteristics, e.g. similar soil or climate conditions.\n\nAd 1) The only way to overcome failed assumption is to get data of all\ncountries. We would then work with the population and not a sample. Or\nwe could draw a sample from this population. --- I wonder what different\nframework of analysis is necessary if working with population data\ninstead with data from a sample. One obvious change is that we would not\nneed significance tests and confidence intervals. Also we would not need\n<a class='glossary' title='Bessel’s correction is the use of n − 1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance. It also partially corrects the bias in the estimation of the population standard deviation. However, the correction often increases the mean squared error in these estimations. This technique is named after Friedrich Bessel. (Wikipedia)'>Bessel’s correction</a> for the variance calculation. What\nelse?\n\nAd 2) I see no way to overcome the influence of neighboring countries.\nBut maybe with a real sample or population data, this assumption would\nnot be a problem. One could argue that similar natural conditions like\nclimate or regional cultural customs are a fact that we should take into\naccount and should not classify as a failed assumption.\n:::\n:::\n\n## Exercises (empty)\n\n## Glossary\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Anderson-Darling </td>\n   <td style=\"text-align:left;\"> The Anderson-Darling Goodness of Fit Test (AD-Test) is a measure of how well your data fits a specified distribution. It’s commonly used as a test for normality. (&lt;a href=\"https://www.statisticshowto.com/anderson-darling-test/\"&gt;Statistics How-To&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Arcsine Transformations </td>\n   <td style=\"text-align:left;\"> Arcsine transformation are data transformation techniques often recommended to normalize percent or proportion data; the arcsine transformation uses the inverse of the sine function and the square root of the variable to transform. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Bessel’s Correction </td>\n   <td style=\"text-align:left;\"> Bessel's correction is the use of n − 1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance. It also partially corrects the bias in the estimation of the population standard deviation. However, the correction often increases the mean squared error in these estimations. This technique is named after Friedrich Bessel. (&lt;a href=\"https://en.wikipedia.org/wiki/Bessel%27s_correction\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Breusch-Pagan </td>\n   <td style=\"text-align:left;\"> Breusch-Pagan is a statistical test for determining whether variance is constant, which is used to test the assumption of homoscedasticity; Breusch-Pagan relies on the [chi-squared] distribution and is used during assumption checking for [homoscedasticity] in [linear regression]. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Ceiling </td>\n   <td style=\"text-align:left;\"> A ceiling effect happens when many observations are at the highest possible value for a variable. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Chi-squared </td>\n   <td style=\"text-align:left;\"> Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Confidence Interval </td>\n   <td style=\"text-align:left;\"> A range of values, calculated from the sample observations, that is believed, with a particular probability, to contain the true parameter value. (Cambridge Dictionary of Statistics, 4th ed., p.98) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Correlation </td>\n   <td style=\"text-align:left;\"> Correlation coefficients are a standardized measure of how two variables are related, or co-vary. They are used to measure how strong a relationship is between two variables. There are several types of correlation coefficient, but the most popular is Pearson’s. Pearson’s correlation (also called Pearson’s R) is a correlation coefficient commonly used in linear regression. ([Statistics How To](https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/)) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Covariance cov </td>\n   <td style=\"text-align:left;\"> Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together. ([Statistics How To](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/covariance/)) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Degrees of Freedom </td>\n   <td style=\"text-align:left;\"> Degree of Freedom (df) is the number of pieces of information that are allowed to vary in computing a statistic before the remaining pieces of information are known; degrees of freedom are often used as parameters for distributions (e.g., chi-squared, F). (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Determination </td>\n   <td style=\"text-align:left;\"> Coefficient of determination is the percentage of variance in one variable that is accounted for by another variable or by a group of variables; often referred to as R-squared and used to determine model fit for linear models. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Floor </td>\n   <td style=\"text-align:left;\"> A floor effect happens when a variable has many observations that take the lowest value of the variable, which can indicate that the range of values was insufficient to capture the true variability of the data. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Histograms </td>\n   <td style=\"text-align:left;\"> Histograms are visual displays of data used to examine the distribution of a numeric variable. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Homoscedasticity </td>\n   <td style=\"text-align:left;\"> Homoscedasticity is [homogeneity of variances], contrast is [Heteroscedasticity]. Homoscedasticity is an assumption of correlation and linear regression that requires that the variance of y be constant across all the values of x; visually, this assumption would show points along a fit line between x and y being evenly spread on either side of the line for the full range of the relationship. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Linearity </td>\n   <td style=\"text-align:left;\"> Linearity is the assumption of some statistical models that requires the outcome, or transformed outcome, to have a linear relationship with numeric predictors, where linear relationships are relationships that are evenly distributed around a line. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Loess </td>\n   <td style=\"text-align:left;\"> Loess curve is a graph curve that shows the relationship between two variables without constraining the line to be straight; it can be compared to a linear fit line to determine whether the relationship is close to linear or not (= checking the [linearity] assumption). The procedure originated as LOWESS (LOcally WEighted Scatter-plot Smoother). is a nonparametric method because the linearity assumptions of conventional regression methods have been relaxed. It is called local regression because the fitting at say point x is weighted toward the data nearest to x. (SwR, Glossary and &lt;a href=\"https://www.statsdirect.com/help/Default.htm#nonparametric_methods/loess.htm\"&gt;LOESS Curve Fitting (Local Polynomial Regression&lt;/a&gt;)) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Logit Transformations </td>\n   <td style=\"text-align:left;\"> Logit transformations are transformations that takes the log value of p/(1-p); this transformation is often used to normalize percentage data and is used in the logistic model to transform the outcome. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Monotonic </td>\n   <td style=\"text-align:left;\"> Monotonic is a statistical relationship that, when visualized, goes up or down, but not both. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> NHST </td>\n   <td style=\"text-align:left;\"> Null Hypothesis Significance Testing (NHST) is a process for organizing inferential statistical tests. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Nonlinear Transformations </td>\n   <td style=\"text-align:left;\"> Nonlinear transformations are transformations that increases (or decreases) the linear relationship between two variables by applying an exponent (i.e., [power transformation]) or other function to one or both of the variables. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p-value </td>\n   <td style=\"text-align:left;\"> The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PartialCorr </td>\n   <td style=\"text-align:left;\"> Partial correlation is a standardized measure of the amount of variance two variables share after accounting for variance they both share with a third variable. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Pearson </td>\n   <td style=\"text-align:left;\"> Pearson’s r is a statistic that indicates the strength and direction of the relationship between two numeric variables that meet certain assumptions. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Power Transformations </td>\n   <td style=\"text-align:left;\"> Power transformations are transformations of a measure using an exponent like squaring or cubing or taking the square root or cube root; power transformations are nonlinear transformations. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Q-Q-Plot </td>\n   <td style=\"text-align:left;\"> A quantile-quantile plot is a visualization of data using probabilities to show how closely a variable follows a normal distribution. (SwR, Glossary) This plot is made up of points below which a certain percentage of the observations fall. On the x-axis are normally distributed values with a mean of 0 and a standard deviation of 1. On the y-axis are the observations from the data. If the data are normally distributed, the values will form a diagonal line through the graph. (SwR, chapter 6) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Shapiro-Wilk </td>\n   <td style=\"text-align:left;\"> The Shapiro-Wilk test is a statistical test to determine or confirm whether a variable has a normal distribution; it is sensitive to small deviations from normality and not useful for sample sizes above 5,000 because it will nearly always find non-normality. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Spearman </td>\n   <td style=\"text-align:left;\"> Spearman’s rho a statistical test used to examine the strength, direction, and significance of the relationship between two numeric variables when they do not meet the assumptions for [Pearson]’s r. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standard Deviation </td>\n   <td style=\"text-align:left;\"> The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter $\\sigma$ (sigma), for the population standard deviation, or the Latin letter $s$ for the sample standard deviation. ([Wikipedia] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standardization </td>\n   <td style=\"text-align:left;\"> In statistics, standardization (also called Normalizing) is the process of putting different variables on the same scale. This process allows you to compare scores between different types of variables. Typically, to standardize variables, you calculate the mean and standard deviation for a variable. Then, for each observed value of the variable, you subtract the mean and divide by the standard deviation. ([Statistics by Jim](https://statisticsbyjim.com/glossary/standardization/)) See `scale()` in R.  (Chap.4) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> T-Statistic </td>\n   <td style=\"text-align:left;\"> The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (&lt;a href=\"https://www.statisticshowto.com/t-statistic/\"&gt;Statistics How-To&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> T-Test </td>\n   <td style=\"text-align:left;\"> A t-test is a type of statistical analysis used to compare the averages of two groups and determine whether the differences between them are more likely to arise from random chance. (&lt;a href=\"https://en.wikipedia.org/wiki/Student%27s_t-test\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Z-score </td>\n   <td style=\"text-align:left;\"> A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (&lt;a href=\"https://www.statisticshowto.com/probability-and-statistics/z-score/#Whatisazscore\"&gt;StatisticsHowTo&lt;/a&gt;) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Session Info {.unnumbered}\n\n::: my-r-code\n::: my-r-code-header\nSession Info\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.3.3 (2024-02-29)\n#>  os       macOS Sonoma 14.4.1\n#>  system   x86_64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2024-04-22\n#>  pandoc   3.1.13 @ /usr/local/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package      * version    date (UTC) lib source\n#>  base64enc      0.1-3      2015-07-28 [1] CRAN (R 4.3.0)\n#>  boot           1.3-30     2024-02-26 [2] CRAN (R 4.3.2)\n#>  cellranger     1.1.0      2016-07-27 [1] CRAN (R 4.3.0)\n#>  class          7.3-22     2023-05-03 [2] CRAN (R 4.3.3)\n#>  cli            3.6.2      2023-12-11 [1] CRAN (R 4.3.0)\n#>  codetools      0.2-20     2024-03-31 [1] CRAN (R 4.3.3)\n#>  coin           1.4-3      2023-09-27 [1] CRAN (R 4.3.0)\n#>  colorspace     2.1-1      2024-01-03 [1] R-Forge (R 4.3.2)\n#>  commonmark     1.9.1      2024-01-30 [1] CRAN (R 4.3.2)\n#>  curl           5.2.1      2024-03-01 [1] CRAN (R 4.3.2)\n#>  data.table     1.15.4     2024-03-30 [1] CRAN (R 4.3.2)\n#>  DescTools      0.99.54    2024-02-03 [1] CRAN (R 4.3.2)\n#>  digest         0.6.35     2024-03-11 [1] CRAN (R 4.3.2)\n#>  dplyr          1.1.4      2023-11-17 [1] CRAN (R 4.3.0)\n#>  e1071          1.7-14     2023-12-06 [1] CRAN (R 4.3.0)\n#>  evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.0)\n#>  Exact          3.2        2022-09-25 [1] CRAN (R 4.3.0)\n#>  expm           0.999-9    2024-01-11 [1] CRAN (R 4.3.0)\n#>  fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.0)\n#>  farver         2.1.1      2022-07-06 [1] CRAN (R 4.3.0)\n#>  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#>  generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#>  ggplot2        3.5.0      2024-02-23 [1] CRAN (R 4.3.2)\n#>  gld            2.6.6      2022-10-23 [1] CRAN (R 4.3.0)\n#>  glossary     * 1.0.0.9000 2023-08-12 [1] Github (debruine/glossary@819e329)\n#>  glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.0)\n#>  gridExtra      2.3        2017-09-09 [1] CRAN (R 4.3.0)\n#>  gtable         0.3.4      2023-08-21 [1] CRAN (R 4.3.0)\n#>  here           1.0.1      2020-12-13 [1] CRAN (R 4.3.0)\n#>  highr          0.10       2022-12-22 [1] CRAN (R 4.3.0)\n#>  htmltools      0.5.8.1    2024-04-04 [1] CRAN (R 4.3.2)\n#>  htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.3.0)\n#>  httr           1.4.7      2023-08-15 [1] CRAN (R 4.3.0)\n#>  jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.0)\n#>  kableExtra     1.4.0      2024-01-24 [1] CRAN (R 4.3.2)\n#>  knitr          1.46       2024-04-06 [1] CRAN (R 4.3.3)\n#>  labeling       0.4.3      2023-08-29 [1] CRAN (R 4.3.0)\n#>  lattice        0.22-6     2024-03-20 [2] CRAN (R 4.3.2)\n#>  libcoin        1.0-10     2023-09-27 [1] CRAN (R 4.3.0)\n#>  lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.3.0)\n#>  lmom           3.0        2023-08-29 [1] CRAN (R 4.3.0)\n#>  lmtest         0.9-40     2022-03-21 [1] CRAN (R 4.3.0)\n#>  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#>  markdown       1.12       2023-12-06 [1] CRAN (R 4.3.0)\n#>  MASS           7.3-60.0.1 2024-01-13 [2] CRAN (R 4.3.3)\n#>  Matrix         1.6-5      2024-01-11 [1] CRAN (R 4.3.0)\n#>  matrixStats    1.3.0      2024-04-11 [1] CRAN (R 4.3.2)\n#>  mgcv           1.9-1      2023-12-21 [1] CRAN (R 4.3.0)\n#>  modeltools     0.2-23     2020-03-05 [1] CRAN (R 4.3.0)\n#>  multcomp       1.4-25     2023-06-20 [1] CRAN (R 4.3.0)\n#>  multcompView   0.1-10     2024-03-08 [1] CRAN (R 4.3.2)\n#>  munsell        0.5.1      2024-04-01 [1] CRAN (R 4.3.2)\n#>  mvtnorm        1.2-4      2023-11-27 [1] CRAN (R 4.3.2)\n#>  nlme           3.1-164    2023-11-27 [1] CRAN (R 4.3.2)\n#>  nortest        1.0-4      2015-07-30 [1] CRAN (R 4.3.0)\n#>  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#>  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#>  plyr           1.8.9      2023-10-02 [1] CRAN (R 4.3.0)\n#>  ppcor          1.1        2015-12-03 [1] CRAN (R 4.3.0)\n#>  proxy          0.4-27     2022-06-09 [1] CRAN (R 4.3.0)\n#>  purrr          1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n#>  R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#>  rcompanion     2.4.35     2024-02-17 [1] CRAN (R 4.3.2)\n#>  Rcpp           1.0.12     2024-01-09 [1] CRAN (R 4.3.0)\n#>  readxl         1.4.3      2023-07-06 [1] CRAN (R 4.3.0)\n#>  repr           1.1.7      2024-03-22 [1] CRAN (R 4.3.3)\n#>  rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.0)\n#>  rmarkdown      2.26       2024-03-05 [1] CRAN (R 4.3.2)\n#>  rootSolve      1.8.2.4    2023-09-21 [1] CRAN (R 4.3.1)\n#>  rprojroot      2.0.4      2023-11-05 [1] CRAN (R 4.3.0)\n#>  rstudioapi     0.16.0     2024-03-24 [1] CRAN (R 4.3.2)\n#>  rversions      2.1.2      2022-08-31 [1] CRAN (R 4.3.0)\n#>  sandwich       3.1-0      2023-12-11 [1] CRAN (R 4.3.0)\n#>  scales         1.3.0      2023-11-28 [1] CRAN (R 4.3.2)\n#>  sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n#>  skimr          2.1.5      2022-12-23 [1] CRAN (R 4.3.0)\n#>  stringi        1.8.3      2023-12-11 [1] CRAN (R 4.3.0)\n#>  stringr        1.5.1      2023-11-14 [1] CRAN (R 4.3.0)\n#>  survival       3.5-8      2024-02-14 [2] CRAN (R 4.3.3)\n#>  svglite        2.1.3      2023-12-08 [1] CRAN (R 4.3.0)\n#>  systemfonts    1.0.6      2024-03-07 [1] CRAN (R 4.3.2)\n#>  TH.data        1.1-2      2023-04-17 [1] CRAN (R 4.3.0)\n#>  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#>  tidyr          1.3.1      2024-01-24 [1] CRAN (R 4.3.2)\n#>  tidyselect     1.2.1      2024-03-11 [1] CRAN (R 4.3.2)\n#>  utf8           1.2.4      2023-10-22 [1] CRAN (R 4.3.0)\n#>  vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n#>  viridisLite    0.4.2      2023-05-02 [1] CRAN (R 4.3.0)\n#>  withr          3.0.0      2024-01-16 [1] CRAN (R 4.3.0)\n#>  xfun           0.43       2024-03-25 [1] CRAN (R 4.3.2)\n#>  xml2           1.3.6      2023-12-04 [1] CRAN (R 4.3.0)\n#>  yaml           2.3.8      2023-12-11 [1] CRAN (R 4.3.0)\n#>  zoo            1.8-12     2023-04-13 [1] CRAN (R 4.3.0)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.3-x86_64/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n:::\n:::\n",
    "supporting": [
      "08-correlation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}