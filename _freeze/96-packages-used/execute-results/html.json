{
  "hash": "65ef7d80ec8ee9a760cafa61b320091d",
  "result": {
    "engine": "knitr",
    "markdown": "# Packages used {#sec-annex-a}\n\n## BSDA\n\n:::::{.my-package}\n:::{.my-package-header}\nBSDA: Basic Statistics and Data Analysis \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-BDSA}\n\n***\n\n{**BSDA**}: [Basic Statistics and Data Analysis](https://alanarnholt.github.io/BSDA/) [@BSDA]\n\n(*There is no hexagon logo for {**BSDA**} available*)\n\nFunctions and data sets for the text Basic Statistics and Data Analysis (BSDA) [@kitchens2002].\n\n\n{**BSDA**}: Basic Statistics and Data Analysis\n:::\n\n***\n::::\n:::::\n\n## broom\n\n:::::{.my-package}\n:::{.my-package-header}\nbroom: Convert Statistical Objects into Tidy Tibbles \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-broom}\n\n***\n\n{**broom**}: [Convert Statistical Objects into Tidy Tibbles](https://broom.tidymodels.org/) [@broom]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap09/logoi/logo-broom-min.png){width=\"176\"}\n\n\nSummarizes key information about statistical objects in tidy tibbles. This makes it easy to report results, create plots and consistently work with large numbers of models at once.\n\n:::\n\n{**broom**} provides three verbs to make it convenient to interact with model objects:\n\n- `tidy()` summarizes information about model components\n- `glance()` reports information about the entire model\n- `augment()` adds informations about observations to a dataset\n\nFor a detailed introduction, please see [Introduction to broom](https://broom.tidymodels.org/articles/broom.html).\n\nbroom tidies 100+ models from popular modelling packages and almost all of the model objects in the stats package that comes with base R. \n\nThe vignette [Available methods](https://broom.tidymodels.org/articles/available-methods.html) lists method availability.\n\n{**broom**}: Convert Statistical Objects into Tidy Tibbles\n:::\n\n\n***\n::::\n:::::\n\n\n\n## car\n\n:::::{.my-package}\n:::{.my-package-header}\ncar: Companion to Applied Regression \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-car}\n\n***\n\n{**car**}: [Companion to Applied Regression](https://www.john-fox.ca/Companion/index.html) [@car]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap06/logoi/logo-car-min.png){width=\"176\"}\n\nFunctions to Accompany J. Fox and S. Weisberg, An R Companion to Applied Regression, Third Edition, Sage, 2019. [@fox2018]\n\n:::\nAn R Companion to Applied Regression is a broad introduction to the R statistical computing environment in the context of applied regression analysis. The book provides a step-by-step guide to using the free statistical software R, and emphasizes integrating statistical computing in R with the practice of data analysis.  The R packages car and effects, written to facilitate the application and interpretation of regression analysis, are extensively covered in the book.\n\n{**car**}: Companion to Applied Regression\n:::\n\n***\n::::\n:::::\n\n\n\n## colorblindcheck\n\n:::::{.my-package}\n:::{.my-package-header}\ncolorblindcheck: Check Color Palettes for Problems with Color Vision Deficiency\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-colorblindcheck}\n\n***\n\n{**colorblindcheck**}: [Check Color Palettes for Problems with Color Vision Deficiency](https://jakubnowosad.com/colorblindcheck/) [@colorblindcheck]\n\n{*There is no hexagon sticker available for {**colorblindcheck**}.*}\n\nCompare color palettes with simulations of color vision deficiencies - deuteranopia, protanopia, and tritanopia. It includes calculation of distances between colors, and creating summaries of differences between a color palette and simulations of color vision deficiencies.\n\nDeciding if a color palette is a colorblind friendly is a hard task. This cannot be done in an entirely automatic fashion, as the decision needs to be confirmed by visual judgments. The goal of {**colorblindcheck**} is to provide tools to decide if the selected color palette is colorblind friendly, including:\n\n- `palette_dist()` - Calculation of the distances between the colors in the input palette and between the colors in simulations of the color vision deficiencies: deuteranopia, protanopia, and tritanopia.\n- `palette_plot()` - Plotting of the original input palette and simulations of color vision deficiencies: deuteranopia, protanopia, and tritanopia.\n- `palette_check()` - Creating summary statistics comparing the original input palette and simulations of color vision deficiencies: deuteranopia, protanopia, and tritanopia.\n\n\n\n{**colorblindcheck**}: A Package for Check Color Palettes for Problems with Color Vision Deficiency\n:::\n\n\n***\n::::\n:::::\n\n\n## colorblindr\n\n:::::{.my-package}\n:::{.my-package-header}\ncolorblindr: Simulate colorblindness in R figures \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-colorblindr}\n\n***\n\n{**colorblindr**}: [Simulate colorblindness in R figures](https://github.com/clauswilke/colorblindr) [@colorblindr]\n\n\n(*There is no hexagon logo for {**colorblindr**} available*)\n\nProvides a variety of functions that are helpful to simulate the effects of colorblindness in R figures. Complete figures can be modified to simulate the effects of various types of colorblindness. The resulting figures are standard grid objects and can be further manipulated or outputted as usual.\n\n{**colorblindr**}: A Package for Simulate colorblindness in R figures\n:::\n\n***\n::::\n:::::\n\n## colorspace\n\n:::::{.my-package}\n:::{.my-package-header}\ncolorspace: A Toolbox for Manipulating and Assessing Colors and Palettes \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-colorspace}\n\n***\n\n{**colorspace**}: [A Toolbox for Manipulating and Assessing Colors and Palettes](URL to package) [biblio]\n\n(*There is no hexagon logo for {**colorspace**} available*)\n\nThe colorspace package provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualizations.\n\nAt the core of the package there are various utilities for computing with color spaces (as the name of the package conveys). Thus, the package helps to map various three-dimensional representations of color to each other. A particularly important mapping is the one from the perceptually-based and device-independent color model HCL (Hue-Chroma-Luminance) to standard Red-Green-Blue (sRGB) which is the basis for color specifications in many systems based on the corresponding hex codes (e.g., in HTML but also in R). For completeness further standard color models are included as well in the package: polarLUV() (= HCL), LUV(), polarLAB(), LAB(), XYZ(), RGB(), sRGB(), HLS(), HSV().\n\nThe HCL space (= polar coordinates in CIELUV) is particularly useful for specifying individual colors and color palettes as its three axes match those of the human visual system very well: Hue (= type of color, dominant wavelength), chroma (= colorfulness), luminance (= brightness).\n\nThere is extensive documentation available. See also the website on [HCL Color Space](https://hclwizard.org/):\n\n> The hclwizard provides tools for manipulating and assessing colors and palettes based on the underlying colorspace software (available in R and Python). It leverages the HCL color space: a color model that is based on human color perception and thus makes it easy to choose good color palettes by varying three color properties: Hue (= type of color, dominant wavelength) - Chroma (= colorfulness) - Luminance (= brightness). As shown in the color swatches below each property can be varied while keeping the other two properties fixed.\n\n{**colorspace**}: A Toolbox for Manipulating and Assessing Colors and Palettes\n:::\n\nThis toolbox package is very important: All of the other color palette related package uses {**colorspace**} as a bases for their functionality. \n\n***\n::::\n:::::\n\n\n## cowplot\n\n::: {.my-package}\n::: {.my-package-header}\ncowplot: Streamlined Plot Theme and Plot Annotations for 'ggplot2'\n:::\n\n::: {.my-package-container}\n\n------------------------------------------------------------------------\n\n::: {#pak-cowplot}\n\n------------------------------------------------------------------------\n\n{**cowplot**}: [Streamlined Plot Theme and Plot Annotations for\n'ggplot2'](https://wilkelab.org/cowplot/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap03/logoi/logo-cowplot-min.png){width=\"176\"}\n\nThe {**cowplot**} package provides various features that help with\ncreating publication-quality figures, such as a set of themes, functions\nto align plots and arrange them into complex compound figures, and\nfunctions that make it easy to annotate plots and or mix plots with\nimages. The package was originally written for internal use in the Wilke\nlab, hence the name (Claus O. Wilke’s plot package). It has also been\nused extensively in the book [Fundamentals of Data\nVisualization](https://www.amazon.com/gp/product/1492031089).\n:::\n\nThere are several packages that can be used to align plots. The most\nwidely used ones beside {**cowplot**} are {**egg**} and {**patchwork**}\n(see @pak-patchwork). All these packages use slightly different\napproaches to plot alignment, and the respective approaches have\ndifferent strengths and weaknesses. If you cannot achieve your desired\nresult with one of these packages try another one.\n\nMost importantly, while {**egg**} and {**patchwork**} align and arrange\nplots at the same time, {**cowplot**} aligns plots independently of how\nthey are arranged. This makes it possible to align plots and then\nreproduce them separately, or even overlay them on top of each other.\n\nThe {**cowplot**} package now provides a set of complementary themes\nwith different features. I now believe that there isn’t one single theme\nthat works for all figures, and therefore I recommend that you always\nexplicitly set a theme for every plot you make.\n\n{**cowplot**}: A Package for Streamlined Plot Themes and Plot\nAnnotations for {**ggplot2**}\n:::\n\n:::\n:::\n\n## cranlogs\n\n:::::{.my-package}\n:::{.my-package-header}\ncranlogs: Download Logs from the 'RStudio' 'CRAN' Mirror \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-cranlogs}\n\n***\n\n{**cranlogs**}: [Download Logs from the RStudio CRAN Mirror](https://r-hub.github.io/cranlogs/) [@cranlogs]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap05/logoi/logo-cranlogs-min.png){width=\"176\"}\n\n\n<a class='glossary' title='An API, or application programming interface, is a set of defined rules that enable different applications to communicate with each other. It acts as an intermediary layer that processes data transfers between systems, letting companies open their application data and functionality to external third-party developers, business partners, and internal departments within their companies. (IBM)'>API</a> to the database of <a class='glossary' title='Comprehensive R Archive Network'>CRAN</a> package downloads from the RStudio CRAN mirror. The database itself is at <http://cranlogs.r-pkg.org>,\n    see <https://github.com/r-hub/cranlogs.app> for the raw API.\n\n:::\n\nRStudio publishes the download logs from their CRAN package mirror daily at http://cran-logs.rstudio.com.\n\nThis R package queries a web API maintained by R-hub that contains the daily download numbers for each package.\n\nThe RStudio CRAN mirror is not the only CRAN mirror, but it’s a popular one: it’s the default choice for RStudio users. The actual number of downloads over all CRAN mirrors is unknown.\n\n\n{**cranlogs**}: Download Logs from the 'RStudio' 'CRAN' Mirror\n:::\n\n***\n::::\n:::::\n\n\n## crosstable\n\n:::::{.my-package}\n:::{.my-package-header}\ncrosstable: Crosstables for Descriptive Analyses \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-crosstable}\n\n***\n\n{**crosstable**}: [Crosstables for Descriptive Analyses](https://danchaltiel.github.io/crosstable/) [@crosstable]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap05/logoi/logo-crosstable-min.png){width=\"176\"}\n\n\nCrosstable is a package centered on a single function, crosstable, which easily computes descriptive statistics on datasets. It can use the {**tidyverse**} syntax and is interfaced with the package {**officer**} to create automatized reports.\n\n:::\n\nCreate descriptive tables for continuous and categorical variables. Apply summary statistics and counting function, with or without a grouping variable, and create beautiful reports using {**rmarkdown**} or {**officer**}. You can also compute effect sizes and statistical tests if needed.\n\n{**crosstable**}: Crosstables for Descriptive Analyses\n:::\n\nI believe that the main usage for this package is to prepare ready-to-print tables. Similar like {**gtsummary**} (see @pak-gtsummary) it provides some descriptive statistics with many display options. But I got the impression that analysis of data is not the main usage of these packages. \n\nFor instance you could use `crosstable::display_test(chisq.test(x, y))` to get as result a string, for instance: p value: <0.0001 \\n(Pearson's Chi-squared test)\". This is nice to include for a table, but for the analysis one would also need the values of the different cells.\n\n***\n::::\n:::::\n\n\n\n## curl\n\n:::::{.my-package}\n:::{.my-package-header}\ncurl: A Modern and Flexible Web Client for R \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-curl}\n\n***\n\n{**curl**}: [A Modern and Flexible Web Client for R](https://cran.r-project.org/web/packages/curl/vignettes/intro.html) [@curl]\n\n\n(*There is no hexagon logo for {**curl**} available*)\n\nThe `curl()` and `curl_download(`) functions provide highly configurable drop-in replacements for base `url()` and `download.file()` with better performance, support for encryption (https, ftps), gzip compression, authentication, and other 'libcurl' goodies. \n\nThe core of the package implements a framework for performing fully customized requests where data can be processed either in memory, on disk, or streaming via the callback or connection interfaces. Some knowledge of 'libcurl' is recommended; for a more-user-friendly web client see the 'httr' package which builds on this package with http specific tools and logic.\n\n{**curl**}: A Modern and Flexible Web Client for R        \n:::\n\n\n***\n::::\n:::::\n\n\n\n## data.table\n\n:::::{.my-package}\n:::{.my-package-header}\ndata.table: Extension of `data.frame` \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-data-table}\n\n***\n\n{**data.table**}: [Extension of `data.frame`](URL to package) [biblio]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-data.table-min.png){width=\"176\"}\n{**data.table**} provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed.\n\n:::\n\nFast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, friendly and fast character-separated-value read/write. Offers a natural and flexible syntax, for faster development.\n\n**Features**\n\n- fast and friendly delimited file reader: `data.table::fread()`, see also convenience features for small data\n- fast and feature rich delimited file writer: `data.table::fwrite()`\n- low-level parallelism: many common operations are internally parallelized to use multiple CPU threads\n- fast and scalable aggregations; e.g. 100GB in RAM (see benchmarks on up to two billion rows)\n- fast and feature rich joins: ordered joins (e.g. rolling forwards, backwards, nearest and limited staleness), overlapping range joins (similar to IRanges::findOverlaps), non-equi joins (i.e. joins using operators >, >=, <, <=), aggregate on join (by=.EACHI), update on join\n- fast add/update/delete columns by reference by group using no copies at all\n- fast and feature rich reshaping data: `data.table::dcast()` (pivot/wider/spread) and `data.table::melt()` (unpivot/longer/gather)\n- any R function from any R package can be used in queries not just the subset of functions made available by a database backend, also columns of type list are supported\n- has no dependencies at all other than base R itself, for simpler production/maintenance\n- the R dependency is as old as possible for as long as possible, dated April 2014, and we continuously test against that version; e.g. v1.11.0 released on 5 May 2018 bumped the dependency up from 5 year old R 3.0.0 to 4 year old R 3.1.0\n\n{**data.table**}: A Package for the Extension of `data.frame`\n:::\n\nI believe the most important application of {**data.table**} is working with huge amount of data (several GB). In the book SwR it is used in this first chapter with the `data.table::fread()` function. I have used here the `readr::read_csv()` as part of the {**tidyverse**} collection, because the dataset is very small (29 kB).\n\n\nWith {**DT**} there is another package that seems important. It is a wrapper of the JavaScript library 'DataTables' (See @pak-DT). I was using already {**DT**} to display interactive tables on websites, but I do not understand completely the difference between {**data.table**} and {**DT**}. \nAs far as I understood:\n\n- Using {**data.table**} is very fast but you will loose the inutitive function of the tidyverse.\n- {**DT**} is for the presentation of (large) interactive tables whereas {**data.table**} is for the fast manipulation of huge data sets.\n\n***\n::::\n:::::\n\n## datawizard\n\n:::::{.my-package}\n:::{.my-package-header}\ndatawizard: Easy Data Wrangling and Statistical Transformations \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-datawizard}\n\n***\n\n{**datawizard**}: [Easy Data Wrangling and Statistical Transformations](https://easystats.github.io/datawizard/) [@datawizard]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap06/logoi/logo-datawizard-min.png){width=\"176\"}\n\n\n\n{**datawizard**} is a lightweight package to easily manipulate, clean, transform, and prepare your data for analysis. It is part of the {**easystats**} ecosystem, a suite of R packages to deal with your entire statistical analysis, from cleaning the data to reporting the results.\n\n:::\n\n{**datawizard**} covers two aspects of data preparation:\n\n- **Data manipulation**: datawizard offers a very similar set of functions to that of the tidyverse packages, such as a {**dplyr**} and {**tidyr**}, to select, filter and reshape data, with a few key differences. \n    1) All data manipulation functions start with the prefix `data_*` (which makes them easy to identify). \n    2) Although most functions can be used exactly as their tidyverse equivalents, they are also string-friendly (which makes them easy to program with and use inside functions). \n    3) Finally, datawizard is super lightweight (no dependencies, similar to {**poorman**}), which makes it awesome for developers to use in their packages.\n- Statistical transformations: {**datawizard**} also has powerful functions to easily apply common data transformations, including standardization, normalization, rescaling, rank-transformation, scale reversing, recoding, binning, etc.\n\n{**datawizard**}: Easy Data Wrangling and Statistical Transformations\n:::\n\n\n***\n::::\n:::::\n\n\n\n## descr\n\n:::::{.my-package}\n:::{.my-package-header}\ndescr: Descriptive Statistics\n:::\n::::{.my-package-container}\n***\n\n::: {#pak-descr}\n\n***\n\n{**descr**}: [Descriptive Statistics](https://github.com/jalvesaq/descr)\n\nThere is no logo available for {**descr**}\n\n\nWeighted frequency and contingency tables of categorical variables and of the comparison of the mean value of a numerical variable by the levels of a factor, and methods to produce xtable objects of the tables and to plot them. There are also functions to facilitate the character encoding conversion of objects, to quickly convert fixed width files into csv ones, and to export a data.frame to a text file with the necessary R and SPSS codes to reread the data. [@descr]\n\nPackage for Descriptive Statistics\n:::\n\n***\n\n::::\n:::::\n\n## DescTools\n\n:::::{.my-package}\n:::{.my-package-header}\nDescTools: Tools for Descriptive Statistics \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-DescTools}\n\n***\n\n{**DescTools**}: [Tools for Descriptive Statistics](https://andrisignorell.github.io/DescTools/) [@DescTools]\n\n\n(*There is no hexagon logo for {**DescTools**} available*)\n\n:::\n\nA collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data. The author's intention was to create a toolbox, which facilitates the (notoriously time consuming) first descriptive tasks in data analysis, consisting of calculating descriptive statistics, drawing graphical summaries and reporting the results. \n\nThe package contains furthermore functions to produce documents using MS Word (or PowerPoint) and functions to import data from Excel. Many of the included functions can be found scattered in other packages and other sources written partly by Titans of R. The reason for collecting them here, was primarily to have them consolidated in ONE instead of dozens of packages (which themselves might depend on other packages which are not needed at all), and to provide a common and consistent interface as far as function and arguments naming, NA handling, recycling rules etc. are concerned. Google style guides were used as naming rules (in absence of convincing alternatives). The 'BigCamelCase' style was consequently applied to functions borrowed from contributed R packages as well.\n\n{**DescTools**}: Tools for Descriptive Statistics\n:::\n\n***\n::::\n:::::\n\n\n\n\n\n\n\n## dichromat\n\n:::::{.my-package}\n:::{.my-package-header}\ndichromat: Color Schemes for Dichromats \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-dichromat}\n\n***\n\n{**dichromats**}: [Color Schemes for Dichromats](https://cran.r-project.org/package=dichromat) [@dichromat]\n\n\nCollapse red-green or green-blue distinctions to simulate the effects of different types of color-blindness.\n\n{**dichromat**}: A Package for Color Schemes for Dichromats\n:::\n\n<Other text not included in the package reference>\n\n***\n::::\n:::::\n\n\n\n\n## dplyr\n\n:::::{.my-package}\n:::{.my-package-header}\ndplyr:: A Grammar of Data Manipulation\n:::\n::::{.my-package-container}\n\n\n\n***\n\n::: {#pak-dplyr}\n\n***\n\n{**package**}: [A Grammar of Data Manipulation](https://dplyr.tidyverse.org/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-dplyr-min.png){width=\"176\"}\n\n{**dplyr**} is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n- mutate() adds new variables that are functions of existing variables\n- select() picks variables based on their names.\n- filter() picks cases based on their values.\n- summarise() reduces multiple values down to a single summary.\n- arrange() changes the ordering of the rows. [@dplyr]\n\n:::\n\nThese all combine naturally with group_by() which allows you to perform any operation “by group”. You can learn more about them in [vignette(\"dplyr\")](https://dplyr.tidyverse.org/articles/dplyr.html). As well as these single-table verbs, dplyr also provides a variety of two-table verbs, which you can learn about in [vignette(\"two-table\")](https://dplyr.tidyverse.org/articles/two-table.html). [@dplyr]\n\n\nPackage for Data Manipulation\n:::\n\n\n\n\n\n::::\n:::::\n\n## DT\n\n:::::{.my-package}\n:::{.my-package-header}\nDT: A Wrapper of the JavaScript Library 'DataTables'\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-DT}\n\n***\n\n{**DT**}: [https://github.com/rstudio/DT](https://rstudio.github.io/DT/)\n\n(*There is no hexagon icon available for {**DT**}*)\n\nData objects in R can be rendered as HTML tables using the JavaScript library [DataTables](https://datatables.net/) (typically via R Markdown or Shiny). The 'DataTables' library has been included in this R package. The package name {**DT**} is an abbreviation of 'DataTables'.\n\n{**DT**}: A Package as Wrapper of the JavaScript Library 'DataTables'\n:::\n\nWhat I do not understand: What is the relationship of {**DT**} to the {**data.table**} package?\n***\n::::\n:::::\n\n## dunn.test\n\n:::::{.my-package}\n:::{.my-package-header}\ndunn.test: Dunn's Test of Multiple Comparisons Using Rank Sums \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-dunn.test}\n\n***\n\n{**dunn.test**}: [Dunn's Test of Multiple Comparisons Using Rank Sums](https://cran.r-project.org/package=dunn.test) [@dunn.test]\n\nComputes Dunn's test [@dunn1964] for stochastic dominance and reports the results among multiple pairwise comparisons after a Kruskal-Wallis test for stochastic dominance among k groups [@kruskal1952. The interpretation of stochastic dominance requires an assumption that the CDF of one group does not cross the CDF of the other. \n\n{**dunn.test**} makes k(k-1)/2 multiple pairwise comparisons based on Dunn's z-test-statistic approximations to the actual rank statistics. The null hypothesis for each pairwise comparison is that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group equals one half; this null hypothesis corresponds to that of the <a class='glossary' title='Mann-Whitney U test, also called Wilcoxon rank sum test, is an alternative for comparing a numeric or ordinal variable across two groups when the independent-samples t-test assumption of normality is not met. (SwR, Glossary)'>Wilcoxon-Mann-Whitney rank-sum test</a>. Like the rank-sum test, if the data can be assumed to be continuous, and the distributions are assumed identical except for a difference in location, Dunn's test may be understood as a test for median difference. {**dunn.test**} accounts for tied ranks.\n\n{**dunn.test**}: Dunn's Test of Multiple Comparisons Using Rank Sums\n:::\n\n***\n::::\n:::::\n\n\n## e1071\n\n:::::{.my-package}\n:::{.my-package-header}\ne1071: Misc. functions \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-e1071}\n\n***\n\n{**e1071**}: [Misc. functions](https://cran.r-project.org/web/packages/e1071/index.html) [@e1071]\n\n\n(*There is no hexagon logo for {**e1071**} available*)\n\nFunctions for latent class analysis, short time Fourier transform, fuzzy clustering, support vector machines, shortest path computation, bagged clustering, naive Bayes classifier, generalized k-nearest neighbour ...\n\n\n{**e1071**}: Misc. functions\n:::\n\n\n***\n::::\n:::::\n\n\n\n\n\n## effectsize\n\n:::::{.my-package}\n:::{.my-package-header}\neffectsize: Indices of Effect Size \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-effectsize}\n\n***\n\n{**effectsize**}: [Indices of Effect Size](https://easystats.github.io/effectsize/) [@effectsize]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap06/logoi/logo-effectsize-min.png){width=\"176\"}\n\nThe goal of this package is to provide utilities to work with indices of effect size and standardized parameters, allowing computation and conversion of indices such as Cohen’s d, r, odds-ratios, etc.\n\n:::\n\nProvide utilities to work with indices of effect size for a wide  variety of models and hypothesis tests (see list of supported models using the function 'insight::supported_models()'), allowing computation of and  conversion between indices such as Cohen's d, r, odds, etc.\n\n{**effectsize**}: Indices of Effect Size\n:::\n\n\n\n***\n::::\n:::::\n\n\n\n\n## fmsb\n\n:::::{.my-package}\n:::{.my-package-header}\nfmsb: Functions for Medical Statistics Book with some Demographic Data \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-fmsb}\n\n***\n\n{**fmsb**}: [Functions for Medical Statistics Book with some Demographic Data](https://cran.r-project.org/package=fmsb) [@fmsb]\n\n(*There is no hexagon logo for {**fmsb**} available*)\n\n\nSeveral utility functions for the book entitled \"Practices of Medical and Health Data Analysis using R\" (Pearson Education Japan, 2007) with Japanese demographic data and some demographic analysis related functions.\n\n{**fmsb**}: Functions for Medical Statistics Book with some Demographic Data\n:::\n\n***\n::::\n:::::\n\n\n## forcats\n\n:::::{.my-package}\n:::{.my-package-header}\nforcats: Tools for Working with Categorical Variables\n:::\n::::{.my-package-container}\n\n\n\n***\n\n::: {#pak-forcats}\n\n***\n\n{**forcats**}: [Tools for Working with Categorical Variables (Factors)](https://forcats.tidyverse.org/)\n\n{**forcats**} provide a suite of useful tools that solve common problems with factors.\n\"Forcats\" is an anagram of \"factors\" and part of the {**tidyverse**} suite of packages.\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-forcats-min.png){width=\"176\"}\n\n(1) reordering factor levels\n    - moving specified levels to front, \n    - ordering by first appearance, \n    - reversing, and \n    - randomly shuffling\n(2) tools for modifying factor levels\n    - collapsing rare levels into other, \n    - 'anonymizing', and\n    - manually 'recoding'\n:::\n\nforcats: A Package with Tools for Working with Categorical Variables\n\n:::\n\n***\n\n\n::::\n:::::\n\n## GGally\n\n:::::{.my-package}\n:::{.my-package-header}\n{**GGally**}: Extension to {**ggplot2**} \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-GGally}\n\n***\n\n{**GGally**}: [Extension to {**ggplot2**}](https://ggobi.github.io/ggally/) [@GGally]\n\nThe R package {**ggplot2**} is a plotting system based on the grammar of graphics. {**GGally**} extends {**ggplot2**} by adding several functions to reduce the complexity of combining geometric objects with transformed data. Some of these functions include \n\n- a pairwise plot matrix, \n- a two group pairwise plot matrix, \n- a parallel coordinates plot, \n- a survival plot, \n- and several functions to plot networks.\n\n{**GGally**}: Extension to {**ggplot2**}\n:::\n\n***\n::::\n:::::\n\n\n## ggmosaic\n\n::: {.my-package}\n::: {.my-package-header}\n{**ggmosaic**}: Mosaic Plots in the {**ggplot2**} Framework\n:::\n\n::: {.my-package-container}\n\n------------------------------------------------------------------------\n\n::: {#pak-ggmosaic}\n\n------------------------------------------------------------------------\n\n{**ggmosaic**}: [Mosaic Plots in the {**ggplot2**}\nFramework](https://haleyjeppson.github.io/ggmosaic/) [@ggmosaic]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap03/logoi/logo-ggmosaic-min.png){width=\"176\"}\n\n{**ggmosaic**} is designed to create visualizations of categorical data\nand is capable of producing bar charts, stacked bar charts, mosaic\nplots, and double decker plots and therefore offers a wide range of\npotential plots.\n:::\n\nFurthermore, {**ggmosaic**} allows various features to be customized:\n\n-   the order of the variables,\n-   the formula setup of the plot,\n-   faceting,\n-   the type of partition, and\n-   the space between the categories.\n\n{**ggmosaic**}: A Package for Mosaic Plots in the {**ggplot2**} Framework\n:::\n\n\n:::\n:::\n\n## ggokabeito\n\n:::::{.my-package}\n:::{.my-package-header}\nggokabeito: 'Okabe-Ito' Scales for {**ggplot2**} and {**ggraph**} \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-ggokabeito}\n\n***\n\n{**ggokabeito**}: ['Okabe-Ito' Scales for {**ggplot2**} and {**ggraph**}](https://malcolmbarrett.github.io/ggokabeito/index.html) [@ggokabeito]\n\n(*There is no hexagon logo for {**ggokabeito**} available*)\n\nDiscrete scales for the colorblind-friendly `Okabe-Ito` palette, including 'color', 'fill', and 'edge_colour'. {**ggokabeito**} provides {**ggplot2**} and {**ggraph**} scales to easily use the discrete, colorblind-friendly ‘Okabe-Ito’ palette in your data visualizations.\n\nCurrently, {**ggokabeito**} provides the following scales:\n\n- `scale_color_okabe_ito(`)/`scale_colour_okabe_ito()`\n- `scale_fill_okabe_ito()`\n- `scale_edge_color_okabe_ito()`/`scale_edge_colour_okabe_ito()`\n\n\n{**ggokabeito**}: `Okabe-Ito` Scales for {**ggplot2**} and {**ggraph**}\n:::\n\n***\n::::\n:::::\n\n## ggplot2\n\n:::::{.my-package}\n:::{.my-package-header}\nggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics\n:::\n::::{.my-package-container}\n\n\n***\n\n::: {#pak-ggplot2}\n\n***\n\n{**ggplot2**}: [Create Elegant Data Visualisations Using the Grammar of Graphics](https://ggplot2.tidyverse.org/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-ggplot2-min.png){width=\"176\"}\n\n{**ggplot2**} is a system for declaratively creating graphics, based on [The Grammar of Graphics](https://link.springer.com/book/10.1007/0-387-28695-0). You provide the data, tell {**ggplot2**} how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. [@ggplot2]\n\n:::\n\n\nPackage for Creating Data Visualisations\n:::\n\n***\n\n\n::::\n:::::\n\n## gplots\n\n:::::{.my-package}\n:::{.my-package-header}\ngplots: Various R Programming Tools for Plotting Data \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-gplots}\n\n***\n\n{**gplots**}: [Various R Programming Tools for Plotting Data](https://github.com/talgalili/gplots) [@gplots]\n\n\n(*There is no hexagon logo for {**gplots**} available*)\n\nVarious R programming tools for plotting data, including:\n\n- calculating and plotting locally smoothed summary function as\n('bandplot', 'wapply'),\n- enhanced versions of standard plots ('barplot2', 'boxplot2', \n'heatmap.2', 'smartlegend'),\n- manipulating colors ('col2hex', 'colorpanel', 'redgreen',\n'greenred', 'bluered', 'redblue', 'rich.colors'),\n- calculating and plotting two-dimensional data summaries \n('ci2d', 'hist2d'),\n- enhanced regression diagnostic plots ('lmplot2', 'residplot'),\n- formula-enabled interface to 'stats::lowess' function ('lowess'),\n- displaying textual data in plots ('textplot', 'sinkplot'),\n- plotting a matrix where each cell contains a dot whose size \nreflects the relative magnitude of the elements ('balloonplot'),\n- plotting \"Venn\" diagrams ('venn'),\n- displaying Open-Office style plots ('ooplot'),\n- plotting multiple data on same region, with separate axes ('overplot'),\n- plotting means and confidence intervals ('plotCI', 'plotmeans'),\n- spacing points in an x-y plot so they don't overlap ('space').\n\n{**gplots**}: Various R Programming Tools for Plotting Data\n:::\n\n<Other text not included in the package reference>\n\n***\n::::\n:::::\n\n\n\n## gridExtra\n\n::: {.my-package}\n::: {.my-package-header}\ngridExtra: Miscellaneous Functions for \"Grid\" Graphics\n:::\n\n::: {.my-package-container}\n\n------------------------------------------------------------------------\n\n::: {#pak-gridExtra}\n\n------------------------------------------------------------------------\n\n{**gridExtra**}: [Miscellaneous Functions for \"Grid\"\nGraphics](https://cran.r-project.org/package=gridExtra)\n\n(*There is no hexagon logo for {**gridExtra**} available*)\n\nProvides a number of user-level functions to work with \"grid\" graphics,\nnotably to arrange multiple grid-based plots on a page, and draw tables.\n\nThe {**grid**) package (= part of the R system library) provides\nlow-level functions to create graphical objects (`grobs`), and position\nthem on a page in specific viewports. The {**gtable**} package\nintroduced a higher-level layout scheme, arguably more amenable to\nuser-level interaction. With the `gridExtra::arrangeGrob()` /\n`gridExtra::grid.arrange()` pair of functions, {**gridExtra**} builds\nupon {**gtable**} to arrange multiple `grobs` on a page.\n\n{**gridExtra**}: A Package for Miscellaneous Functions for \"Grid\"\nGraphics\n:::\n\n:::\n:::\n\n\n\n## ggrepel\n\n:::::{.my-package}\n:::{.my-package-header}\nggrepel: Automatically Position Non-Overlapping Text Labels with 'ggplot2' \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-ggrepel}\n\n***\n\n{**ggrepel**}: [Automatically Position Non-Overlapping Text Labels with 'ggplot2'](https://ggrepel.slowkow.com/) [@ggrepel]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap03/logoi/logo-ggrepel-min.png){width=\"176\"}\n\n\nProvides text and label geoms for 'ggplot2' that help to avoid overlapping text labels. Labels repel away from each other and away from the data points.\n\n:::\n\n{**ggrepel**} provides two geoms for {**ggplot2**} to repel overlapping text labels:\n\n- `ggrepel::geom_text_repel()`\n- `ggrepel::geom_label_repel()`\n\n{**ggrepel**}: A Package for Automatically Position Non-Overlapping Text Labels with 'ggplot2'\n:::\n\n\n***\n::::\n:::::\n\n\n## ggtext\n\n::: {.my-package}\n::: {.my-package-header}\nggtext: Improved Text Rendering Support for 'ggplot2'\n:::\n\n::: {.my-package-container}\n\n------------------------------------------------------------------------\n\n::: {#pak-ggtext}\n\n------------------------------------------------------------------------\n\n{**ggtext**}: [Improved Text Rendering Support for\n'ggplot2'](https://wilkelab.org/ggtext/)\n\n(*There is no hexagon logo for {**ggtext**} available*)\n\nThe ggtext package provides simple Markdown and HTML rendering for\n{**ggplot2.**} Under the hood, the package uses the {**gridtext**}\npackage for the actual rendering, and consequently it is limited to the\n[feature set provided by gridtext](https://wilkelab.org/gridtext/).\n\nSupport is provided for Markdown both in theme elements (plot titles,\nsubtitles, captions, axis labels, legends, etc.) and in geoms (similar\nto `ggplot2::geom_text()`). In both cases, there are two alternatives,\none for creating simple text labels and one for creating text boxes with\nword wrapping.\n\n{**ggtext**}: A Package for Improved Text Rendering Support for\n'ggplot2'\n:::\n\n:::\n:::\n\n\n\n\n\n\n\n## glue\n\n:::::{.my-package}\n:::{.my-package-header}\nglue: Interpreted String Literals\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-glue}\n\n***\n\n{**glue**}: [Interpreted String Literals](https://glue.tidyverse.org/)\n\nAn implementation of interpreted string literals, inspired by Python's Literal String Interpolation\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap02/logoi/logo-glue-min.png){width=\"176\"}\n\nGlue offers interpreted string literals that are small, fast, and dependency-free. Glue does this by embedding R expressions in curly braces which are then evaluated and inserted into the argument string.\n\n:::\n\n{**glue**}: A Package for Interpreting Literal Strings\n:::\n\n\n\n***\n::::\n:::::\n\n\n## gssr\n\n:::::{.my-package}\n:::{.my-package-header}\ngssr: US General Social Survey (GSS) Data for R\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-gssr}\n\n***\n\n{**gssr**}: [US General Social Survey (GSS) Data for R](https://kjhealy.github.io/gssr/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-gssr-min.png){width=\"376\"}\n\n[GSSR Package](https://kjhealy.github.io/gssr/): The General Social\nSurvey Cumulative Data (1972-2022) and Panel Data files packaged for\neasy use in R. {**gssr**} is a data package, developed and maintained by\n[Kieran Healy](https://kieranhealy.org/), the author of [Data\nVisualization](https://kieranhealy.org/publications/dataviz/). The\npackage bundles several datasets into a convenient format. Because of\nits large size {**gssr**} is not hosted on CRAN but as a [GitHub\nrepository](https://github.com/kjhealy/gssr/).\n\n:::\n\nInstead of browsing and examining the complex dataset with the [GSS Data\nExplorer](https://gssdataexplorer.norc.org/) or [download datasets\ndirectly](https://gss.norc.org/Get-The-Data) from the The National\nOpinion Research Center ([NORC](http://norc.org/)) you can now just work\ninside R. The current package 0.4 (see: [gssr\nUpdate](https://kieranhealy.org/blog/archives/2023/12/02/gssr-update/))\nprovides the GSS Cumulative Data File (1972-2022), three GSS Three Wave\nPanel Data Files (for panels beginning in 2006, 2008, and 2010,\nrespectively), and the 2020 panel file.\n\nVersion 0.40 also integrates survey code book information about\nvariables directly into R’s help system, allowing them to be accessed\nvia the help browser or from the console with ?, as if they were\nfunctions or other documented objects.\n\nPackage for Getting the US General Social Survey (GSS) Data More Easily\n\n:::\n***\n\n\n::::\n:::::\n\n## gt\n\n:::::{.my-package}\n:::{.my-package-header}\ngt: Easily Create Presentation-Ready Display Tables\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-gt}\n\n***\n\n{**gt**}: [Easily Create Presentation-Ready Display Tables](https://gt.rstudio.com)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap02/logoi/logo-gt-min.png){width=\"176\"}\n\nWith the {**gt**} package, anyone can make wonderful-looking tables using the R programming language. The gt philosophy: we can construct a wide variety of useful tables with a cohesive set of table parts. These include the table header, the stub, the column labels and spanner column labels, the table body, and the table footer.\n\n\n\n:::\n\n{**gt**}: A Package for Presentation-Ready Display Tables\n:::\n\n***\n::::\n:::::\n\n\n## gtsummary\n\n:::::{.my-package}\n:::{.my-package-header}\n\ngtsummary: Presentation-Ready Data Summary and Analytic Result Tables\n\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-gtsummary}\n\n***\n\n{**gtsummary**}: [Presentation-Ready Data Summary and Analytic Result Tables](https://www.danieldsjoberg.com/gtsummary/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-gtsummary-min.png){width=\"176\"}\n\nCreates presentation-ready tables summarizing data sets, regression models, and more. The code to create the tables is concise and highly customizable. Data frames can be summarized with any function, e.g. mean(), median(), even user-written functions. Regression models are summarized and include the reference rows for categorical variables. Common regression models, such as logistic regression and Cox proportional hazards regression, are automatically identified and the tables are pre-filled with appropriate column headers.\n\n\n\n:::\n\n- Summarize data frames or tibbles easily in R. Perfect for creating a <a class='glossary' title='Descriptive statistics are often displayed in the first table in a published article or report and are therefore often called Table 1 statistics or the descriptives. (SwR)'>Table 1</a>. \n- Summarize regression models in R and include reference rows for categorical variables. \n- Customize {**gtsummary**} tables using a growing list of formatting/styling functions. \n- Report statistics inline from summary tables and regression summary tables in R markdown. Make your reports completely reproducible!\n\nBy leveraging {**broom**}, {**gt**}, and {**labelled**} packages, {**gtsummary**} creates beautifully formatted, ready-to-share summary and result tables in a single line of R code!\n\nPackage for Presentation-Ready Data Summary and Analytic Result Tables\n\n:::\n\n\n\n***\n\n\n\n::::\n:::::\n\n\n\n## haven\n\n:::::{.my-package}\n:::{.my-package-header}\nhaven: Import and Export 'SPSS', 'Stata' and 'SAS' Files\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-haven}\n\n***\n\n{**haven**}: [Import and Export 'SPSS', 'Stata' and 'SAS'\nFiles](https://haven.tidyverse.org/index.html)\n\n{**haven**} enables R to read and write various data formats used by\nother statistical packages. Currently it supports\n[SAS](https://www.sas.com/en_us/home.html),\n[SPSS](https://www.ibm.com/spss) and [STATA](https://www.stata.com/).\n{**haven**} output object has four important features:\n\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-haven-min.png){width=\"176\"}\n\n(1) It creates `tibbles::tibble()` which a better print method for very\n    long and very wide files.\n(2) Dates and times are converted to R date/time classes.\n(3) Character vectors are not converted to factors.\n(4) Value labels are translated into a new `haven::labelled()` class,\n    which preserves the original semantics and can easily be coerced to\n    factors with `haven::as_factor()`. Special missing values are\n    preserved. See details in the vignette [Conversion\n    semantics](https://haven.tidyverse.org/articles/semantics.html).\n:::\n\nhaven: A Package for Import and Export of 'SPSS', 'Stata' and 'SAS'\nFiles\n:::\n\nI am here interested especially in the fourth feature.\n\n::::\n:::::\n\n## Hmisc\n\n:::::{.my-package}\n:::{.my-package-header}\nHmisc: Harrell Miscellaneous\n:::\n::::{.my-package-container}\n***\n\n::: {#pak-Hmisc}\n\n***\n\n{**Hmisc**}: [Harrell Miscellaneous](https://hbiostat.org/r/hmisc/)\n\nThere is no hexagon sticker available for {**Hmisc**}.\n\nThe {**Hmisc**} has it names from Frank Harrell Jr. It contains many functions useful for \n\n- data analysis, \n- high-level graphics, \n- utility operations, \n- computing sample size and power, \n- simulation, \n- importing and annotating datasets,\n- imputing missing values, \n- advanced table making, \n- variable clustering,\n- character string manipulation, \n- conversion of R objects to {{< latex >}} and HTML code,\n- recoding variables, \n- caching, \n- simplified parallel computing, \n- encrypting and decrypting data using a safe workflow, \n- general moving window statistical estimation, \n- assistance in interpreting principal component analysis [@Hmisc]\n\n\nThis is big variety of functions. In contrast to other packages that are specific directed to solve one problem {**Hmisc**} seems to be an all-in-one-solution.\n\n\n{**Hmisc**}: A package for miscellaneous functions for data analysis and related tasks\n\n:::\n\nTo learn more about I should visit Frank E. Harrell's Jr [Hmisc start page](https://hbiostat.org/r/hmisc/). Especially his [book on R Workflow for Reproducible Data Analysis and Reporting](https://hbiostat.org/rflow/) seems to me very interesting!\n\n***\n\n::::\n:::::\n\n## httr2\n\n::: {.my-package}\n::: {.my-package-header}\nhttr2: Perform HTTP Requests and Process the Responses\n:::\n\n::: {.my-package-container}\n\n------------------------------------------------------------------------\n\n::: {#pak-httr2}\n\n------------------------------------------------------------------------\n\n{**httr2**}: [Perform HTTP Requests and Process the\nResponses](https://httr2.r-lib.org/)\n\nTools for creating and modifying HTTP requests, then performing them and\nprocessing the results.\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap03/logoi/logo-httr2-min.png){width=\"176\"}\n\n{**httr2**} (pronounced hitter2) is a ground-up rewrite of {**httr**}\nthat provides a pipeable <a class='glossary' title='An API, or application programming interface, is a set of defined rules that enable different applications to communicate with each other. It acts as an intermediary layer that processes data transfers between systems, letting companies open their application data and functionality to external third-party developers, business partners, and internal departments within their companies. (IBM)'>API</a> with an explicit request\nobject that solves more problems felt by packages that wrap APIs (e.g.\nbuilt-in rate-limiting, retries, OAuth, secure secrets, and more). ---\n{**httr2**} is designed to map closely to the underlying\n<a class='glossary' title='The Hypertext Transfer Protocol (HTTP) is an application layer protocol in the Internet protocol suite model for distributed, collaborative, hypermedia information systems.[1] HTTP is the foundation of data communication for the World Wide Web, where hypertext documents include hyperlinks to other resources that the user can easily access, for example by a mouse click or by tapping the screen in a web browser. (Wikipedia)'>HTTP</a> <a class='glossary' title='A protocol is a system of rules that define how data is exchanged within or between computers. Communications between devices require that the devices agree on the format of the data that is being exchanged. The set of rules that defines a format is called a protocol. (MDN web docs)'>protocol</a>. For more details, read\n[An overview of\nHTTP](https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview) from\n<a class='glossary' title='MDN Web Docs, previously Mozilla Developer Network and formerly Mozilla Developer Center, is a documentation repository and learning resource for web developers. MDN Web Docs content is maintained by Mozilla, Google employees, and volunteers (community of developers and technical writers). It also contains content contributed by Microsoft, Google, and Samsung. Topics include HTML5, JavaScript, CSS, Web APIs, Django, Node.js, WebExtensions, MathML, and others. (Wikipedia)'>MDN</a>.\n:::\n\n{**httr2**}: A Package to Perform HTTP Requests and Process the\nResponses\n:::\n\n:::\n:::\n\n## janitor\n\n::: {.my-package}\n::: {.my-package-header}\njanitor: Simple Tools for Examining and Cleaning Dirty Data\n:::\n\n::: {.my-package-container}\n\n------------------------------------------------------------------------\n\n::: {#pak-janitor}\n\n------------------------------------------------------------------------\n\n{**janitor**}: [Simple Tools for Examining and Cleaning Dirty\nData](https://sfirke.github.io/janitor/) [@janitor]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap03/logoi/logo-janitor-min.png){width=\"176\"}\n\n{**janitor**} has simple functions for examining and cleaning dirty\ndata. It was built with beginning and intermediate R users in mind and\nis optimized for user-friendliness. Advanced R users can perform many of\nthese tasks already, but with janitor they can do it faster and save\ntheir thinking for the fun stuff.\n:::\n\n**The main janitor functions:**\n\n-   perfectly format data.frame column names;\n-   create and format frequency tables of one, two, or three variables -\n    think an improved `base::table()`; and\n-   provide other tools for cleaning and examining data.frames.\n\nThe tabulate-and-report functions approximate popular features of SPSS\nand Microsoft Excel.\n\n{**janitor**} is a {**tidyverse**}-oriented package. Specifically, it\nplays nicely with the `%>%` pipe and is optimized for cleaning data\nbrought in with the {**readr**} and {**readxl**} packages.\n\n{**janitor**}: A Package for Simple Tools for Examining and Cleaning\nDirty Data\n:::\n\n------------------------------------------------------------------------\n\nI am using {**janitor**} mostly in two ways:\n\n1.  as better `base::table()` function, using `janitor::tabyl()`\n    -   `base::table()` doesn't accept data.frames and is therefore not\n        compatible with the pipe\n    -   `base::table()` doesn't output data.frames\n    -   `base::table()` results are hard to format (the most annoying\n        \"feature\" for me)\n\n-   to add information and formatting to the table with the\n    `janitor::adorn_*` functions\n    -   `janitor::adorn_totals()`\n    -   `janitor::adorn_percentages()`\n    -   `janitor::adorn_pct_formatting()`\n    -   `janitor::adorn_rounding()`\n    -   `janitor::adorn_ns()` (adding Ns = number of counts)\n    -   `janitor::adorn_title()`\n\nYou could also use {**tidyverse**} commands (for instance for a two\ntable `dplyr::count()` followed by `tidyr::pivot_wider()`) but the many\n`adorn_*`-functions make it easy to enhance the results. BTW: The prefix\n`adorn` comes from 'adornment' (ornament, decoration).\n:::\n:::\n\n\n\n## kableExtra\n\n:::::{.my-package}\n:::{.my-package-header}\nkableExtra: Construct Complex Table with `knitr::kable()` and Pipe Syntax\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-kableExtra}\n\n***\n\n{**kableExtra**}: [Construct Complex Table with 'kable' and Pipe Syntax](URL to package)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap02/logoi/logo-kableExtra-min.png){width=\"176\"}\n\nBuild complex HTML or {{< latex >}} tables using `kable()` from {**knitr**} and the piping syntax from {**magrittr**} Function `kable()` is a light weight table generator coming from {**knitr**}. This package simplifies the way to manipulate the HTML or {{< latex >}} codes generated by `kable()` and allows users to construct complex tables and customize styles using a readable syntax. \n\n:::\n\n{**pkg-name**}: A Package to Construct Complex Table with 'kable' and Pipe Syntax\n:::\n\n***\n::::\n:::::\n\n\n## knitr\n\n:::::{.my-package}\n:::{.my-package-header}\nknitr: A General-Purpose Package for Dynamic Report Generation in R\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-knitr}\n\n***\n\n{**knitr**}: [A General-Purpose Package for Dynamic Report Generation in R](https://yihui.org/knitr/)\n\n(*There is no hexagon logo available*)\n\nProvides a general-purpose tool for dynamic report generation in R using Literate Programming techniques.\n\n{**knitr**}: A Package for Dynamic Report Generation in R\n:::\n\n\n***\n::::\n:::::\n\n\n## labelled\n\n:::::{.my-package}\n:::{.my-package-header}\nlabelled: Manipulating Labelled Data\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-labelled}\n\n***\n{**labelled**}: [Manipulating Labelled\nData](https://larmarange.github.io/labelled/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-labelled-min.png){width=\"176\"}\n\nWork with labelled data imported from [IBM\nSPSS](https://www.ibm.com/spss) or [STATA](https://www.stata.com/) with\n{**haven**} or {**foreign**}. This package provides useful functions to\ndeal with \"haven_labelled\" and \"haven_labelled_spss\" classes introduced\nby {**haven**} package. (With the free [gnu\nPSPP](https://www.gnu.org/software/pspp/) exists also a SPSS like open\nsource version.) See details in the vignette [Introduction to\nlabelled](https://larmarange.github.io/labelled/articles/intro_labelled.html)\nand the [GitHub website for\nlabelled](https://larmarange.github.io/labelled/). There are other\nvignettes as well and a [cheat sheet as PDF for\ndownload](https://github.com/larmarange/labelled/raw/main/cheatsheet/labelled_cheatsheet.pdf).\n\n:::\n\nlabelled: A Package for Manipulating Labelled Data\n:::\n\n\n::::\n:::::\n\n## lmtest\n\n:::::{.my-package}\n:::{.my-package-header}\nlmtest: Testing Linear Regression Models \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-lmtest}\n\n***\n\n{**lmtest**}: [Testing Linear Regression Models](https://cran.r-project.org/package=lmtest) [@lmtest]\n\nA collection of tests, data sets, and examples for diagnostic checking in linear regression models. Furthermore, some generic tools for inference in parametric models are provided.\n\nVignette [Diagnostic Checking in Regression Relationships](https://cran.r-project.org/web/packages/lmtest/vignettes/lmtest-intro.pdf)\n\n{**lmtest**}: Testing Linear Regression Models\n:::\n\n***\n::::\n:::::\n\n\n## lsr\n\n:::::{.my-package}\n:::{.my-package-header}\nlsr: Companion to \"Learning Statistics with R\" \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-lsr}\n\n***\n\n{**lsr**}: [Companion to \"Learning Statistics with R\"](https://learningstatisticswithr.com/) [@lsr]\n\n(*There is no hexagon logo for {**lsr**} available*)\n\nA collection of tools intended to make introductory statistics easier to teach, including wrappers for common hypothesis tests and basic data manipulation. It accompanies Navarro, D. J. (2015). Learning Statistics with R: A Tutorial for Psychology Students and Other Beginners, Version 0.6. \n\n\n{**lsr**}: Companion to \"Learning Statistics with R\"\n:::\n\n***\n::::\n:::::\n\n\n\n\n\n\n## modeest\n\n:::::{.my-package}\n:::{.my-package-header}\nmodeest: Mode Estimation\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-modeest}\n\n***\n\n{**modeest**}: [Mode Estimation](https://github.com/paulponcet/modeest)\n\nThere is no hexagon sticker available for {**modeest**}.\n\n\nThe {**modeest**} package provides estimators of the mode of univariate unimodal (and sometimes multimodal) data and values of the modes of usual probability distributions.\n\n{**modeest**} is a package specialized for mode estimation. It implements many different mode estimation reported in scientific articles. There is a long [list of references](https://www.rdocumentation.org/packages/modeest/versions/2.4.0/topics/modeest) on different methods of mode estimations.\n\n{**modeest**}: A Package for Mode Estimation\n:::\n\n\n::::\n:::::\n\n\n## moments\n\n:::::{.my-package}\n:::{.my-package-header}\nmoments: Moments, Cumulants, Skewness, Kurtosis and Related Tests \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-moments}\n\n***\n\n{**moments**}: [Moments, Cumulants, Skewness, Kurtosis and Related Tests](https://cran.r-project.org/package=moments) [@moments]\n\n\n\n(*There is no hexagon logo for {**moments**} available*)\n\nFunctions to calculate: moments, Pearson's kurtosis, Geary's kurtosis and skewness; tests related to them.\n\n\n{**moments**}: Moments, Cumulants, Skewness, Kurtosis and Related Tests\n:::\n\n***\n::::\n:::::\n\n## misty\n\n:::::{.my-package}\n:::{.my-package-header}\nmisty: Miscellaneous Functions 'T. Yanagida'  \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-misty}\n\n***\n\n{**misty**}: [Miscellaneous Functions 'T. Yanagida' ](https://cran.r-project.org/package=misty) [@misty]\n\nMiscellaneous functions for \n\n- (1) data management (e.g., grand-mean and group-mean centering, coding variables and reverse coding items, scale and cluster scores, reading and writing Excel and SPSS files), \n- (2) descriptive statistics (e.g., frequency table, cross tabulation, effect size measures), \n- (3) missing data (e.g., descriptive statistics for missing data, missing data pattern, Little's test of Missing Completely at Random, and auxiliary variable analysis), \n- (4) multilevel data (e.g., multilevel descriptive statistics, within-group and between-group correlation matrix, multilevel confirmatory factor analysis, level-specific fit indices, cross-level measurement equivalence evaluation,  multilevel composite reliability, and multilevel R-squared measures), \n- (5) item analysis (e.g., confirmatory factor analysis, coefficient alpha and omega, between-group and longitudinal measurement equivalence evaluation), and \n- (6) statistical analysis (e.g., confidence intervals, collinearity and residual diagnostics, dominance analysis, between- and within-subject analysis of variance, latent class analysis, t-test, z-test, sample size determination).\n\n{**misty**}: Miscellaneous Functions 'T. Yanagida' \n:::\n\n\n***\n::::\n:::::\n\n\n\n## naniar\n\n:::::{.my-package}\n:::{.my-package-header}\nnaniar: Data Structures, Summaries, and Visualisations for Missing Data \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-naniar}\n\n***\n\n{**naniar**}: [https://github.com/njtierney/naniar](https://naniar.njtierney.com/) [@naniar]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap05/logoi/logo-naniar-min.png){width=\"176\"}\n\n\n{**naniar**} provides principled, tidy ways to summarise, visualise, and manipulate missing data with minimal deviations from the workflows in ggplot2 and tidy data.\n\n:::\n\nMissing values are ubiquitous in data and need to be explored and handled in the initial stages of analysis. {**naniar**} provides data structures and functions that facilitate the plotting of missing values and examination of imputations. This allows missing data dependencies to be  explored with minimal deviation from the common work patterns of 'ggplot2' and tidy data. The work is fully discussed in Tierney & Cook [-@tierney2023].\n\n{**naniar**}: https://github.com/njtierney/naniar\n:::\n\n\n***\n::::\n:::::\n\n## nhanesA\n\n:::::{.my-package}\n:::{.my-package-header}\nnhanesA: NHANES Data Retrieval \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-nhanesA}\n\n***\n\n{**nhanesA**}: [NHANES Data Retrieval](https://github.com/cjendres1/nhanes/) [@nhanesA]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap06/logoi/logo-nhanesA-min.png){width=\"176\"}\n\n\nUtility to retrieve data from the National Health and Nutrition Examination Survey (NHANES) website <https://www.cdc.gov/nchs/nhanes/index.htm>.\n\n:::\n\n{**nhanesA**} is an R package for browsing and retrieving data from the National Health And Nutrition Examination Survey (NHANES). This package is designed to be useful for research and instructional purposes.\n\nThe functions in the {**nhanesA**} package allow for fully customizable selection and import of data directly from the NHANES website thus it is essential to have an active network connection.\n\nThere are other similar packages also available, but the are more restricted as newer data than 2014 cant be downloaded: \n\n- {**NHANES**}: For the years 2009-2012\n- {**RNHANES**}: For the years 1999-2014\n\n\n{**nhanesA**}: NHANES Data Retrieval\n:::\n\nSee for my other reflection of packages for downloading NHANES data in @imp-chap01-nhanesA-pkg and @sec-chap03-rnhanes.\n\n***\n::::\n:::::\n\n## nortest\n\n:::::{.my-package}\n:::{.my-package-header}\nnortest: Tests for Normality \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-nortest}\n\n***\n\n{**nortest**}: [Tests for Normality](https://cran.r-project.org/package=nortest) [@nortest]\n\n\n(*There is no hexagon logo for {**nortest**} available*)\n\nFive omnibus tests for testing the composite hypothesis of normality.\n\n{**nortest**}: Tests for Normality\n:::\n\n***\n::::\n:::::\n\n## onewaytests\n\n:::::{.my-package}\n:::{.my-package-header}\nonewaytests: One-Way Tests in Independent Groups Designs \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-onewaytests}\n\n***\n\n{**onewaytests**}: [One-Way Tests in Independent Groups Designs](https://cran.r-project.org/package=onewaytests) [@onewaytests]\n\nPerforms one-way tests in independent groups designs including homoscedastic and heteroscedastic tests. These are \n\n- one-way analysis of variance (ANOVA), \n- Welch's heteroscedastic F test, \n- Welch's heteroscedastic F test with trimmed means and Winsorized variances,\n- Brown-Forsythe test, \n- Alexander-Govern test, \n- James second order test, \n- Kruskal-Wallis test, \n- Scott-Smith test, \n- Box F test, \n- Johansen F test, \n- Generalized tests equivalent to Parametric Bootstrap and Fiducial tests, \n- Alvandi's F test, \n- Alvandi's generalized p-value, \n- approximate F test, \n- B square test, \n- Cochran test, \n- Weerahandi's generalized F test, \n- modified Brown-Forsythe test, \n- adjusted Welch's heteroscedastic F test, \n- Welch-Aspin test, \n- Permutation F test.\n\nThe package performs pairwise comparisons and graphical approaches. \n\nAlso, the package includes \n\n- Student's t test, \n- Welch's t test and \n- Mann-Whitney U test for two samples. \n\nMoreover, it assesses variance homogeneity and normality of data in each group via tests and plots (Dag et al., 2018, <https://journal.r-project.org/archive/2018/RJ-2018-022/RJ-2018-022.pdf>).\n\n{**onewaytests**}: One-Way Tests in Independent Groups Designs\n:::\n\n***\n::::\n:::::\n\n\n\n\n## openintro\n\n:::::{.my-package}\n:::{.my-package-header}\nopenintro: Data Sets and Supplemental Functions from 'OpenIntro' Textbooks and Labs\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-openintro}\n\n***\n\n{**openintro**}: [Data Sets and Supplemental Functions from 'OpenIntro' Textbooks and Labs](https://openintrostat.github.io/openintro/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap02/logoi/logo-openintro-min.png){width=\"176\"}\n\nThe package contains data sets used in our open-source textbooks along with custom plotting functions for reproducing book figures. The package also contains the datasets used in {**OpenIntro**} labs. Note that many functions and examples include color transparency; some plotting elements may not show up properly (or at all) when run in some versions of Windows operating system.\n\n:::\n\n{**openintro**}: A Package to Supplement 'OpenIntro' Textbooks and Labs\n:::\n\n\n***\n::::\n:::::\n\n\n## paletteer\n\n:::::{.my-package}\n:::{.my-package-header}\npaletteer: Comprehensive Collection of Color Palettes \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-paletteer}\n\n***\n\n{**paletteer**}: [Comprehensive Collection of Color Palettes](https://emilhvitfeldt.github.io/paletteer/index.html) [@paletteer]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap03/logoi/logo-paletteer-min.png){width=\"176\"}\n\n\nThe choices of color palettes in R can be quite overwhelming with palettes spread over many packages with many different API's. This packages aims to collect all color palettes across the R ecosystem under the same package with a streamlined API.\n\n:::\n\nThe palettes are divided into 2 groups; discrete and continuous. For discrete palette you have the choice between the fixed width palettes and dynamic palettes.\n\n1. discrete\n    - fixed width palettes: These are the most common discrete palettes. They have a set amount of colors which doesn’t change when the number of colors requested vary.\n    - dynamic palettes: The colors of dynamic palettes depend on the number of colors you need.\n2. continuous: These palettes provides as many colors as you need for a smooth transition of color.\n\nThis package includes 2759 palettes from 75 different packages and information about these can be found in the following data.frames: `palettes_c_names`, `palettes_d_names` and `palettes_dynamic_names`. Additionally this [github repo](https://github.com/EmilHvitfeldt/r-color-palettes/blob/main/README.md) showcases all the palettes included in the package and more.\n\n{**paletteer**}: A Package for Comprehensive Collection of Color Palettes\n:::\n\n\n***\n::::\n:::::\n\n\n\n\n## patchwork\n\n::: {.my-package}\n::: {.my-package-header}\npatchwork: The Composer of Plots\n:::\n\n::: {.my-package-container}\n\n------------------------------------------------------------------------\n\n::: {#pak-patchwork}\n\n------------------------------------------------------------------------\n\n{**patchwork**}: [The Composer of\nPlots](https://patchwork.data-imaginist.com/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap03/logoi/logo-patchwork-min.png){width=\"176\"}\n\nThe goal of {**patchwork**} is to make it ridiculously simple to combine\nseparate `ggplots` into the same graphic. As such it tries to solve the\nsame problem as `gridExtra::grid.arrange()` and `cowplot::plot_grid` but\nusing an <a class='glossary' title='An API, or application programming interface, is a set of defined rules that enable different applications to communicate with each other. It acts as an intermediary layer that processes data transfers between systems, letting companies open their application data and functionality to external third-party developers, business partners, and internal departments within their companies. (IBM)'>API</a> that incites exploration and iteration, and\nscales to arbitrarily complex layouts.\n:::\n\nThe {**ggplot2**} package provides a strong API for sequentially\nbuilding up a plot, but does not concern itself with composition of\nmultiple plots. {**patchwork**} is a package that expands the API to\nallow for arbitrarily complex composition of plots by, among others,\nproviding mathematical operators for combining multiple plots. Other\npackages that try to address this need (but with a different approach)\nare {**gridExtra**} and {**cowplot**} (see @pak-gridExtra and\n@pak-cowplot).\n\nBefore plots can be laid out, they have to be assembled. Arguably one of\npatchwork’s biggest selling points is that it expands on the use of `+`\nin ggplot2 to allow plots to be added together and composed, creating a\nnatural extension of the {**ggplot2**} API.\n\nWhile quite complex compositions can be achieved using `+`, `|`, and\n`/`, it may be necessary to take even more control over the layout. All\nof this can be controlled using the `patchwork::plot_layout()` function\nalong with a couple of special placeholder objects.\n\n{**patchwork**}: A Package for Composing Plots\n:::\n\nSee [Using plot arithmetic functions with `::` syntax](https://github.com/thomasp85/patchwork/issues/351#issuecomment-1931140157)\n\n| operator | function                  |effect         |\n|----------|---------------------------|---------------|\n| +        | ggplot2:::\"+.gg\"()        | side by side  |\n| -        | patchwork:::\"-.ggplot\"()  |               |\n| \\|       | patchwork:::\"\\|.ggplot\"() |               |\n| /        | patchwork:::\"/.ggplot\"()  | stacked       |\n| *        | patchwork:::\"*.gg\"()      |               |\n| &        | patchwork:::\"&.gg\"()      |               |\n\n\n:::\n:::\n\n## nhanesA\n\n:::::{.my-package}\n:::{.my-package-header}\nnhanesA: NHANES Data Retrieval \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-nhanesA}\n\n***\n\n{**nhanesA**}: [NHANES Data Retrieval](https://github.com/cjendres1/nhanes/) [@nhanesA]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap06/logoi/logo-nhanesA-min.png){width=\"176\"}\n\n\nUtility to retrieve data from the National Health and Nutrition Examination Survey (NHANES) website <https://www.cdc.gov/nchs/nhanes/index.htm>.\n\n:::\n\n{**nhanesA**} is an R package for browsing and retrieving data from the National Health And Nutrition Examination Survey (NHANES). This package is designed to be useful for research and instructional purposes.\n\nThe functions in the {**nhanesA**} package allow for fully customizable selection and import of data directly from the NHANES website thus it is essential to have an active network connection.\n\nThere are other similar packages also available, but the are more restricted as newer data than 2014 cant be downloaded: \n\n- {**NHANES**}: For the years 2009-2012\n- {**RNHANES**}: For the years 1999-2014\n\n\n{**nhanesA**}: NHANES Data Retrieval\n:::\n\nSee for my other reflection of packages for downloading NHANES data in @imp-chap01-nhanesA-pkg and @sec-chap03-rnhanes.\n\n***\n::::\n:::::\n\n## nortest\n\n:::::{.my-package}\n:::{.my-package-header}\nnortest: Tests for Normality \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-nortest}\n\n***\n\n{**nortest**}: [Tests for Normality](https://cran.r-project.org/package=nortest) [@nortest]\n\n\n(*There is no hexagon logo for {**nortest**} available*)\n\nFive omnibus tests for testing the composite hypothesis of normality.\n\n{**nortest**}: Tests for Normality\n:::\n\n***\n::::\n:::::\n\n## ppcor\n\n:::::{.my-package}\n:::{.my-package-header}\nppcor: Partial and Semi-Partial (Part) Correlation \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-ppcor}\n\n***\n\n{**ppcor**}: [Partial and Semi-Partial (Part) Correlation](https://cran.r-project.org/package=ppcor) [@ppcor]\n\nCalculates partial and semi-partial (part) correlations along with p value. [@kim2015]\n\n\n{**ppcor**}: Partial and Semi-Partial (Part) Correlation\n:::\n\n\n\n***\n::::\n:::::\n\n\n## psych\n\n:::::{.my-package}\n:::{.my-package-header}\npsych: Procedures for Psychological, Psychometric, and Personality Research \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-psych}\n\n***\n\n{**psych**}: [Procedures for Psychological, Psychometric, and Personality Research](https://personality-project.org/r/psych/) [@psych]\n\n\n(*There is no hexagon logo for {**psych**} available*)\n\nA general purpose toolbox developed originally for personality, psychometric theory and experimental psychology.  \n\n- Functions are primarily for multivariate analysis and scale construction using factor analysis, principal component analysis, cluster analysis and reliability analysis, although others provide basic descriptive statistics. \n- Item Response Theory is done using factor analysis of tetrachoric and polychoric correlations. \n- Functions for analyzing data at multiple levels include within and between group statistics, including correlations and factor analysis.  \n- Validation and cross validation of scales developed using basic machine learning algorithms are provided, as are functions for simulating and testing particular item and test structures. \n- Several functions serve as a useful front end for structural equation modeling. \n- Graphical displays of path diagrams, including mediation models, factor analysis and structural equation models are created using basic graphics. \n- Some of the functions are written to support a book on psychometric theory as well as publications in personality research.\n\n{**psych**}: Procedures for Psychological, Psychometric, and Personality Research\n:::\n\n\n\n***\n::::\n:::::\n\n\n## purrr\n\n:::::{.my-package}\n:::{.my-package-header}\npurrr: Functional Programming Tools\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-purrr}\n\n***\n\n{**purrr**}: [Functional Programming Tools](https://purrr.tidyverse.org/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap02/logoi/logo-purrr-min.png){width=\"176\"}\n\n{**purrr**} enhances R’s functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. If you’ve never heard of FP before, the best place to start is the family of `purrr::map()` functions which allow you to replace many for loops with code that is both more succinct and easier to read. The best place to learn about the `purrr::map()` functions is the [iteration chapter](https://r4ds.had.co.nz/iteration.html) in R for data science.\n\n:::\n\n\n{**purrr**}: A Package for Functional Programming Tools\n:::\n\n***\n::::\n:::::\n\n## rcompanion\n\n:::::{.my-package}\n:::{.my-package-header}\nrcompanion: Functions to Support Extension Education Program Evaluation \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-rcompanion}\n\n***\n\n{**rcompanion**}: [Functions to Support Extension Education Program Evaluation](https://rcompanion.org/handbook/) [@rcompanion-2]\n\nFunctions and datasets to support Summary and Analysis of Extension Program Evaluation in Companion for the Handbook of Biological Statistics. Vignettes are available at <https://rcompanion.org>. (See also the [PDF book](https://rcompanion.org/documents/RHandbookProgramEvaluation.pdf) [@mangiafico2023].)\n\n{**rcompanion**}: Functions to Support Extension Education Program Evaluation\n:::\n\nThis book explains many different tests as you can see from the table of content:\n\n**TABLE OF CONTENT**\n\n \n\n**Introduction**\n\n- Purpose of this Book\n- Author of this Book\n- Using R\n- [Statistics Textbooks and Other Resources](https://rcompanion.org/handbook/A_04.html)\n \n\n**Statistics for Educational Program Evaluation**\n\n- Why Statistics?\n- Evaluation Tools and Surveys\n \n\n**Variables, Descriptive Statistics, and Plots**\n\n- Types of Variables\n- Descriptive Statistics\n- Confidence Intervals\n- Basic Plots\n \n\n**Understanding Statistics and Hypothesis Testing**\n\n- Hypothesis Testing and p-values\n- Reporting Results of Data and Analyses\n- [Choosing a Statistical Test](https://rcompanion.org/handbook/D_03.html)\n- Independent and Paired Values\n \n\n**Likert Data**\n\n- Introduction to Likert Data\n- Descriptive Statistics for Likert Item Data\n- Descriptive Statistics with the likert Package\n- Confidence Intervals for Medians\n- Converting Numeric Data to Categories\n \n\n**Traditional Nonparametric Tests**\n\n- Introduction to Traditional Nonparametric Tests\n- One-sample Wilcoxon Signed-rank Test\n- Sign Test for One-sample Data\n- Two-sample Mann–Whitney U Test\n- Mood’s Median Test for Two-sample Data\n- Two-sample Paired Signed-rank Test\n- Sign Test for Two-sample Paired Data\n- Kruskal–Wallis Test\n- Mood’s Median Test\n- Friedman Test\n- Quade Test\n- Scheirer–Ray–Hare Test\n- Aligned Ranks Transformation ANOVA\n- Nonparametric Regression and Local Regression\n- Nonparametric Regression for Time Series\n \n\n**Permutation Tests**\n\n- Introduction to Permutation Tests\n- One-way Permutation Test for Ordinal Data\n- One-way Permutation Test for Paired Ordinal Data\n- Permutation Tests for Medians and Percentiles\n \n\n**Tests for Ordinal Data in Tables**\n\n- Association Tests for Ordinal Tables\n- Measures of Association for Ordinal Tables\n \n\n**Concepts for Linear Models**\n\n- Introduction to Linear Models\n- Using Random Effects in Models\n- What are Estimated Marginal Means?\n- Estimated Marginal Means for Multiple Comparisons\n- Factorial ANOVA: Main Effects, Interaction Effects, and Interaction Plots\n- p-values and R-square Values for Models\n- Accuracy and Errors for Models\n \n\n**Ordinal Tests with Cumulative Link Models**\n\n- Introduction to Cumulative Link Models (CLM) for Ordinal Data\n- Two-sample Ordinal Test with CLM\n- Two-sample Paired Ordinal Test with CLMM\n- One-way Ordinal Regression with CLM\n- One-way Repeated Ordinal Regression with CLMM\n- Two-way Ordinal Regression with CLM\n- Two-way Repeated Ordinal Regression with CLMM\n \n\n**Tests for Nominal Data**\n\n- Introduction to Tests for Nominal Variables\n- Confidence Intervals for Proportions\n- Goodness-of-Fit Tests for Nominal Variables\n- Association Tests for Nominal Variables\n- Measures of Association for Nominal Variables\n- Tests for Paired Nominal Data\n- Cochran–Mantel–Haenszel Test for 3-Dimensional Tables\n- Cochran’s Q Test for Paired Nominal Data\n- Models for Nominal Data\n \n\n**Parametric Tests**\n\n- Introduction to Parametric Tests\n- One-sample t-test\n- Two-sample t-test\n- Paired t-test\n- One-way ANOVA\n- One-way ANOVA with Blocks\n- One-way ANOVA with Random Blocks\n- Two-way ANOVA\n- Repeated Measures ANOVA\n- Correlation and Linear Regression\n- Advanced Parametric Methods\n- Transforming Data\n- Normal Scores Transformation\n \n\n**Analysis of Count Data and Percentage Data**\n\n- Regression for Count Data\n- Beta Regression for Percent and Proportion Data\n \n\n**Other Books**\n\nAn [R Companion](https://rcompanion.org/rcompanion/) for the [Handbook of Biological Statistics](https://www.biostathandbook.com/)\n\n***\n::::\n:::::\n\n\n\n## readr \n\n:::::{.my-package}\n:::{.my-package-header}\nreadr: Read Rectangular Text Data\n:::\n::::{.my-package-container}\n\n\n***\n\n::: {#pak-readr}\n\n***\n\n{**readr**}: [Read Rectangular Text Data](https://readr.tidyverse.org/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-readr-min.png){width=\"176\"}\n\nThe goal of readr is to provide a fast and friendly way to read rectangular data from delimited files, such as comma-separated values (CSV) and tab-separated values (TSV). It is designed to parse many types of data found in the wild, while providing an informative problem report when parsing leads to unexpected results. [@readr] \n\n\n:::\n\n{**readr**} supports the following formats:\n\n- read_csv(): comma-separated values (CSV)\n- read_tsv(): tab-separated values (TSV)\n- read_csv2(): semicolon-separated values with , as the decimal mark\n- read_delim(): delimited files (CSV and TSV are important special cases)\n- read_fwf(): fixed-width files\n- read_table(): whitespace-separated files\n- read_log(): web log files\n\nPackage for Reading Rectangular Data\n\n:::\n\n\n***\n\n\n\n::::\n:::::\n\n\n\n\n## readxl\n\n::: {.my-package}\n::: {.my-package-header}\nreadxl: Read Excel Files\n:::\n\n::: {.my-package-container}\n\n------------------------------------------------------------------------\n\n::: {#pak-readxl}\n\n------------------------------------------------------------------------\n\n{**readxl**}: [Read Excel Files](https://readxl.tidyverse.org/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap03/logoi/logo-readxl-min.png){width=\"176\"}\n\nThe readxl package makes it easy to get data out of Excel and into R.\nCompared to many of the existing packages (e.g. {**gdata**}, {**xlsx**},\n{**xlsReadWrite**}) {**readxl**} has no external dependencies, so it’s\neasy to install and use on all operating systems. It is designed to work\nwith *tabular* data. Works on Windows, Mac and Linux without external\ndependencies.\n:::\n\n{**readxl**} supports both the legacy `.xls` format and the modern\nxml-based `.xlsx` format. The embedded\n[libxls](https://github.com/libxls/libxls) C library is used to support\n`.xls`, which abstracts away many of the complexities of the underlying\nbinary format. To parse `.xlsx`, we use the\n[RapidXML](https://rapidxml.sourceforge.net/) C++ library.\n\n{**readxl**}: A Package to Read Excel Files\n:::\n\n:::\n:::\n\n\n\n## report\n\n:::::{.my-package}\n:::{.my-package-header}\nReport: Automated Reporting of Results and Statistical Models\n:::\n::::{.my-package-container}\n***\n\n::: {#pak-report}\n\n***\n\n{**report**}: [Automated Reporting of Results and Statistical Models](URL to package)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap02/logoi/logo-report-min.png){width=\"176\"}\n\nThe primary goal of {**report**} is to bridge the gap between R’s output and the formatted results contained in your manuscript. It automatically produces reports of models and data frames according to best practices guidelines (e.g., APA’s style), ensuring standardization and quality in results reporting.\n\n:::\n\n{**Report**} A Package for Automated Reporting of Results and Statistical Models\n:::\n\n***\n\n\n::::\n:::::\n\n## RNHANES {#sec-chap03-rnhanes}\n\n::: {.my-package}\n::: {.my-package-header}\nRNHANES: Facilitates Analysis of CDC NHANES Data\n:::\n\n::: {.my-package-container}\n\n------------------------------------------------------------------------\n\n::: {#pak-RNHANES}\n\n------------------------------------------------------------------------\n\n{**RNHANES**}: [Facilitates Analysis of CDC NHANES\nData](https://wwww.silentspring.org/RNHANES/index.html)\n\n(*There is no hexagon logo available for RNHANES*)\n\nRNHANES is an R package for accessing and analyzing <a class='glossary' title='Centers for Disease Control and Preventation (CDC) is the U.S. leading science-based, data-driven, service organization that protects the public’s health. (CDC)'>CDC</a>\n<a class='glossary' title='The National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations. (NHANES)'>NHANES</a> (National Health and Nutrition Examination\nSurvey) data that was developed by [Silent Spring\nInstitute](https://silentspring.org/).\n\n{**RNHANES**}: A Package for Facilitating Analysis of CDC NHANES Data\n:::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! {**RNHANES**} downloads data only until 2013-2014\n:::\n::::{.my-watch-out-container}\n\nThe CRAN version of {**RNHANES**} only works with data before 2015. It is said in the book that for the year 2015-2016 you could use the GitHub developer version. But this didn't work for me.\n\nThe problem is the function `RNHANES::validate_year()` that is not up-to-date. It has the valid years included as fixed strings which I see as bad programming. (One could generate these pair of years programmatically, checking with the modulo operator `%%`, subtracting from the current year 4 years, because the data has to be prepared to make it public available.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRNHANES:::validate_year\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> function (year, throw_error = TRUE) \n#> {\n#>     if (length(year) > 1) {\n#>         Map(validate_year, year, throw_error = throw_error) %>% \n#>             unlist() %>% unname() %>% return()\n#>     }\n#>     else {\n#>         valid <- grep(\"^[0-9]{4}-[0-9]{4}$\", as.character(year)) == \n#>             1\n#>         if (throw_error == TRUE && valid == FALSE) {\n#>             stop(paste0(\"Invalid year: \", year))\n#>         }\n#>         return(valid)\n#>     }\n#> }\n#> <bytecode: 0x7ff49c233e70>\n#> <environment: namespace:RNHANES>\n```\n\n\n:::\n:::\n\n\nI therefore used in @lst-chap03-get-NHANES-2018-data code to download data directly from the website. Currently I learned that there is another --- more updated --- {**nhanesA**} packages that I am going to test in chapter 6, where I need NHANES data again.\n\n::::\n:::::\n\n\n\n:::\n:::\n\n## rstatix\n\n:::::{.my-package}\n:::{.my-package-header}\nrstatix: Pipe-Friendly Framework for Basic Statistical Tests \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-rstatix}\n\n***\n\n{**rstatix**}: [Pipe-Friendly Framework for Basic Statistical Tests](https://rpkgs.datanovia.com/rstatix/) [@rstatix]\n\n\n(*There is no hexagon logo for {**rstatix**} available*)\n\nProvides a simple and intuitive pipe-friendly framework, coherent with the {**tidyverse**} design philosophy, for performing basic statistical tests, including t-test, Wilcoxon test, ANOVA, Kruskal-Wallis and correlation analyses.\n\nThe output of each test is automatically transformed into a tidy data frame to facilitate visualization.\n\nAdditional functions are available for reshaping, reordering, manipulating and visualizing correlation matrix. Functions are also included to facilitate the analysis of factorial experiments, including purely ‘within-Ss’ designs (repeated measures), purely ‘between-Ss’ designs, and mixed ‘within-and-between-Ss’ designs.\n\nIt’s also possible to compute several effect size metrics, including “eta squared” for ANOVA, “Cohen’s d” for t-test and “Cramer’s V” for the association between categorical variables. The package contains helper functions for identifying univariate and multivariate outliers, assessing normality and homogeneity of variances.\n\n{**rstatix**}: Pipe-Friendly Framework for Basic Statistical Tests\n:::\n\n***\n::::\n:::::\n\n\n\n\n\n\n\n## rvest\n\n:::::{.my-package}\n:::{.my-package-header}\nrvest: Easily Harvest (Scrape) Web Pages \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-rvest}\n\n***\n\n{**rvest**}: [Easily Harvest (Scrape) Web Pages](https://rvest.tidyverse.org/) [@rvest]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap04/logoi/logo-rvest-min.png){width=\"176\"}\n\n\nWrappers around the {**xml2**} and {**httr**} packages to make it easy to download, then manipulate, <a class='glossary' title='HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser. It defines the content and structure of web content. It is often assisted by technologies such as Cascading Style Sheets (CSS) and scripting languages such as JavaScript. Web browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for its appearance. (Wikipedia)'>HTML</a> and <a class='glossary' title='Extensible Markup Language (XML) is a markup language and file format for storing, transmitting, and reconstructing arbitrary data. It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. The World Wide Web Consortium’s XML 1.0 Specification of 1998 and several other related specifications —all of them free open standards—define XML. The design goals of XML emphasize simplicity, generality, and usability across the Internet. (Wikipedia)'>XML</a>. --- {**rvest**} helps you scrape (or harvest) data from web pages. It is designed to work with {**magrittr**} to make it easy to express common web scraping tasks, inspired by libraries like [beautiful soup](https://www.crummy.com/software/BeautifulSoup/) and [RoboBrowser](https://robobrowser.readthedocs.io/en/latest/readme.html).\n\n:::\n\nIf you’re scraping multiple pages, hadley Wickham highly recommends using {**rvest**} in concert with [{**polite**}](https://dmi3kno.github.io/polite/). The {**polite**} package ensures that you’re respecting the [robots.txt](https://en.wikipedia.org/wiki/Robots_exclusion_standard) and not hammering the site with too many requests.\n\n{**rvest**}: A Package for Easily Harvest (Scrape) Web Pages\n:::\n\nIt is important to read the introductory vignette [Web scraping 101](https://rvest.tidyverse.org/articles/rvest.html). It introduces you to the basics of web scraping with {**rvest**}. You’ll first learn the basics of HTML and how to use <a class='glossary' title='Cascading Style Sheets (CSS) is a style sheet language used for specifying the presentation and styling of a document written in a markup language such as HTML or XML (including XML dialects such as SVG, MathML or XHTML). CSS is a cornerstone technology of the World Wide Web, alongside HTML and JavaScript. CSS is designed to enable the separation of content and presentation, including layout, colors, and fonts. (Wikipedia)'>CSS</a> selectors to refer to specific elements, then you’ll learn how to use {**rvest**} functions to get data out of <a class='glossary' title='HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser. It defines the content and structure of web content. It is often assisted by technologies such as Cascading Style Sheets (CSS) and scripting languages such as JavaScript. Web browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for its appearance. (Wikipedia)'>HTML</a> and into R.\n\nA very important tool to get the appropriate CSS selector is SelectorGadget. To learn how to install and to use this tool read the [SelectorGadget help page]https://rvest.tidyverse.org/articles/selectorgadget.html) of {**rvest**}.\n\n***\n\n\n\n::::\n:::::\n\n\n## scales\n\n:::::{.my-package}\n:::{.my-package-header}\nscales: Scale Functions for Visualization\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-scales}\n\n***\n\n{**scales**}: [Scale Functions for Visualization](https://scales.r-lib.org/)\n\nGraphical scales map data to aesthetics, and provide methods for automatically determining breaks and labels for axes and legends.\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap02/logoi/logo-scales-min.png){width=\"176\"}\n\nOne of the most difficult parts of any graphics package is scaling, converting from data values to perceptual properties. The inverse of scaling, making guides (legends and axes) that can be used to read the graph, is often even harder! The scales packages provides the internal scaling infrastructure used by ggplot2, and gives you tools to override the default breaks, labels, transformations and palettes.\n\n:::\n\n{**scales**}: A Package for Scaling Functions for Visualization\n:::\n\n\n***\n::::\n:::::\n\n## scico\n\n:::::{.my-package}\n:::{.my-package-header}\nscico: Colour Palettes Based on the Scientific Colour-Maps \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-scico}\n\n***\n\n{**scico**}: [Colour Palettes Based on the Scientific Colour-Maps](https://github.com/thomasp85/scico) [@scico]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap03/logoi/logo-scico-min.png){width=\"176\"}\n\n\nColor choice in information visualization is important in order to avoid being mislead by inherent bias in the used color palette. The {**scico**} package provides access to the perceptually uniform and color-blindness friendly palettes developed by [Fabio Crameri](https://www.fabiocrameri.ch/colourmaps/) and released under the \"Scientific Color-Maps\" moniker. The package contains 39 different palettes and includes both diverging and sequential types. It uses more or less the same <a class='glossary' title='An API, or application programming interface, is a set of defined rules that enable different applications to communicate with each other. It acts as an intermediary layer that processes data transfers between systems, letting companies open their application data and functionality to external third-party developers, business partners, and internal departments within their companies. (IBM)'>API</a> as {**viridis**} and provides scales for {**ggplot2**} without requiring {**ggplot2**} to be installed.\n\n:::\n\n**Features of {scico}**\n\n- Perceptually uniform\n- Perceptually ordered\n- Color-vision-deficiency (<a class='glossary' title='Color vision deficiency (CVD) or color blindness (also spelled colour blindness) includes a wide range of causes and conditions and is actually quite complex. It’s a condition characterized by an inability or difficulty in perceiving and differentiating certain colors due to abnormalities in the three color-sensing pigments of the cones in the retina. (EnChroma)'>CVD</a>) friendly\n- Readable in black & white prints\n- All color map types & classes in all major formats\n- Citable & reproducible\n\n\n{**scico**}: A Package for Colour Palettes Based on the Scientific Colour-Maps\n:::\n\n::::\n:::::\n\n\n\n\n\n## semTools\n\n:::::{.my-package}\n:::{.my-package-header}\nsemTools:: Useful Tools for Structural Equation Modeling\n:::\n::::{.my-package-container}\n\n\n***\n\n::: {#pak-semTools}\n\n***\n\n{**semTools**}: [Useful Tools for Structural Equation Modeling](https://github.com/simsem/semTools/wiki)\n\n(For {**semTools**} there is currently no logo available.)\n\nProvides tools for structural equation modeling, many of which extend the {**lavaan**} package; for example, to pool results from multiple imputations, probe latent interactions, or test measurement invariance.\n***\n\n{**semTools**}: A Package for Useful Tools for Structural Equation Modeling\n:::\n\nThis is a very specialized package. I believe I will not use it at the moment besides the function `semTools::skew()` and `semTools::kurtosis()`.\n\n***\n\n\n::::\n:::::\n\n\n## sjlabelled\n\n:::::{.my-package}\n:::{.my-package-header}\nsjlabelled; \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-sjlabelled}\n\n***\n\n{**sjlabelled**}: [Labelled Data Utility\nFunctions](https://strengejacke.github.io/sjlabelled/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-sjlabelled-min.png){width=\"176\"}\n\nBasically, this package covers reading and writing data between other\nstatistical packages (like ‘SPSS’) and R, based on the haven and foreign\npackages; hence, this package also includes functions to make working\nwith labelled data easier. This includes easy ways to get, set or change\nvalue and variable label attributes, to convert labelled vectors into\nfactors or numeric (and vice versa), or to deal with multiple declared\nmissing values.\n:::\n\nThe prefix `sj` in {**sjlabelled**} (= in German Strenge Jacke, \"strict\njacket\") refers to other work of [Daniel\nLüdecke](https://www.uke.de/kliniken-institute/institute/medizinische-soziologie/team/profil_daniel_luedecke_ims.html),\nwho has developed many R packages. All the `sj`-packages support labelled data.\n\nsjlabelled: A Package for Labelled Data Utility Functions\n\n:::\n\n***\n\nHis packages are divided in two approaches:\n\n1.  Most packages are part pf the project\n    [EasyStats](https://easystats.github.io/easystats/), that provides\n    with 11 packages \"An R Framework for Easy Statistical Modeling,\n    Visualization, and Reporting\", similar to the {**tidyverse**}\n    collection. The {**easystats**} collection is orientated more to\n    statistics, whereas {**tidyverse**} is more directed to data\n    science.\n2.  The other line of package development supports labelled data in\n    combination with different R task like\n    -   [Data and Variable Transformation\n        Functions](https://cran.r-project.org/web/packages/sjmisc/index.html)\n        {**sjmisc**},\n    -   [Data Visualization for Statistics in Social\n        Science](https://cran.r-project.org/web/packages/sjPlot/index.html)\n        {**sjPlot**} and a\n    -   [Collection of Convenient Functions for Common Statistical\n        Computations](https://cran.r-project.org/web/packages/sjstats/index.html)\n        {**sjStats**}. -Additionally there exists {**sjtable2df**} a\n        package to [Convert 'sjPlot' HTML-Tables to R\n        'data.frame'](https://cran.r-project.org/web/packages/sjtable2df/index.html).\n\n\n\n\n\n::::\n:::::\n\n## sjPlot\n\n:::::{.my-package}\n:::{.my-package-header}\nsjPlot: Data Visualization for Statistics in Social Science \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-sjPlot}\n\n***\n\n{**sjPlot**}: [Data Visualization for Statistics in Social Science](https://strengejacke.github.io/sjPlot/) [@sjPlot]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap05/logoi/logo-sjPlot-min.png){width=\"176\"}\n\nCollection of plotting and table output functions for data visualization. Results of various statistical analyses (that are commonly used in social sciences) can be visualized using this package, including simple and cross tabulated frequencies, histograms, box plots, (generalized) linear models, mixed effects models, PCA and correlation matrices, cluster analyses, scatter plots, Likert scales, effects plots of interaction terms in regression models, constructing index or score variables and much more.\n\n:::\n\n{**sjPlot**}: Data Visualization for Statistics in Social Science\n:::\n\nThe standard plot versions are easy to create, but to adapt the resulted graph is another issue. Although {**sjPlot**} uses in the background the {**ggplot2**} package, you can’t specify changes with ggplot2 commands. I tried it and it produced two different plots. \n\nTo customize plot appearance you have to learn the many arguments of of `sjPlot:set_theme()` and `sjPlot::plot_grpfrq()`. See the documentation of the [many specialized functions](https://strengejacke.github.io/sjPlot/reference/index.html#plot-customization) to tweak the default values.\n\n\n***\n::::\n:::::\n\n\n\n\n\n\n\n## sjstats\n\n:::::{.my-package}\n:::{.my-package-header}\nsjstats: Collection of Convenient Functions for Common Statistical Computations \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-sjstats}\n\n***\n\n{**sjstats**}: [Collection of Convenient Functions for Common Statistical Computations](https://strengejacke.github.io/sjstats/) [@sjstats]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap04/logoi/logo-sjstats-min.png){width=\"176\"}\n\nCollection of convenient functions for common statistical computations, which are not directly provided by R’s base or stats packages.\n\n:::\n\nThis package aims at providing, *first*, shortcuts for statistical measures, which otherwise could only be calculated with additional effort (like Cramer’s V, Phi, or effict size statistics like Eta or Omega squared), or for which currently no functions available.\n\n*Second*, another focus lies on weighted variants of common statistical measures and tests like weighted standard error, mean, t-test, correlation, and more.\n\nThe comprised tools include:\n\n- Especially for mixed models: design effect, sample size calculation\n- Weighted statistics and tests for: mean, median, standard error, standard deviation, correlation, Chi-squared test, t-test, Mann-Whitney-U-test\n\n\n{**sjstats**}: A Collection of Convenient Functions for Common Statistical Computations\n:::\n\n\n***\n::::\n:::::\n\n\n## skimr\n\n\n\n:::::{.my-package}\n:::{.my-package-header}\nskimr: Compact and Flexible Summaries of Data\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-skimr}\n\n***\n\n{**skimr**}: [Compact and Flexible Summaries of Data](https://docs.ropensci.org/skimr/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-skimr-min.png){width=\"176\"}\n\nA simple to use summary function that can be used with pipes and displays nicely in the console. The default summary statistics may be modified by the user as can the default formatting.  Support for data frames and vectors is included, and users can implement their own skim methods for specific object types as described in a vignette. Default summaries include support for inline spark graphs. Instructions for managing these on specific operating systems are given in the [Using skimr](https://docs.ropensci.org/skimr/articles/skimr.html) vignette and the [README](https://github.com/ropensci/skimr/#skimr-).\n\n\n:::\n\nAt the moment I am just using the `skimr::skim()` function. I believe most of the many other functions for adaption are oriented to developers. But still: I need to have a closer look to this package.\n\n\nA Package for Compact and Flexible Summaries of Data\n:::\n\n***\n::::\n:::::\n\n\n## statpsych\n\n:::::{.my-package}\n:::{.my-package-header}\nstatpsych: Statistical Methods for Psychologists\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-statpsych}\n\n***\n\n{**statpsych**}: [Statistical Methods for Psychologists](URL to package)\n\n(*There is no hexagon logo for {**statpsych**} available.*)\n\n{**statpsych**} implements confidence interval and sample size methods that are especially useful in psychological research. \n\nThe methods can be applied in 1-group, 2-group, paired-samples, and multiple-group designs and to a variety of parameters including means, medians, proportions, slopes, standardized mean differences, standardized linear contrasts of means, plus several measures of correlation and association. \n\nThe confidence intervals and sample size functions are applicable to single parameters as well as differences, ratios, and linear contrasts of parameters. \n\nThe sample size functions can be used to approximate the sample size needed to estimate a parameter or function of parameters with desired confidence interval precision or to perform a variety of hypothesis tests (directional two-sided, equivalence, superiority, noninferiority) with desired power. For details, see: https://dgbonett.sites.ucsc.edu/.\n\n\n\n{**statpsych**}: A Package for Statistical Methods for Psychologists\n:::\n\n***\n::::\n:::::\n\n## stringi\n\n:::::{.my-package}\n:::{.my-package-header}\nstringi: Fast and Portable Character String Processing Facilities \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-stringi}\n\n***\n\n{**stringi**}: [Fast and Portable Character String Processing Facilities](https://stringi.gagolewski.com/) [@stringi]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-stringi-min.png){width=\"176\"}\n\n\nA collection of character string/text/natural language processing tools for pattern searching (e.g., with 'Java'-like regular expressions or the 'Unicode' collation  algorithm), random string generation, case mapping, string transliteration, concatenation, sorting, padding, wrapping, Unicode normalization, date-time formatting and parsing, and many more. \n\n:::\n\nThe {**stringi**} tools are fast, consistent, convenient, and - thanks to <a class='glossary' title='International Components for Unicode (ICU) is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications. ICU is widely portable and gives applications the same results on all platforms and between C/C++ and Java software. ICU is released under a nonrestrictive open source license that is suitable for use with both commercial software and with other open source or free software. (ICU)'>ICU</a> ([International Components for Unicode](https://icu.unicode.org/home)) - portable across all locales and platforms. Documentation about {**stringi**} is provided via its website at <https://stringi.gagolewski.com/> and the paper by Gagolewski [-@gagolewski2022].\n\n{**stringi**}: Fast and Portable Character String Processing Facilities\n:::\n\n***\n::::\n:::::\n\n\n## stringr\n\n:::::{.my-package}\n:::{.my-package-header}\nstringr: Simple, Consistent Wrappers for Common String Operations \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-stringr}\n\n***\n\n{**stringr**}: [Simple, Consistent Wrappers for Common String Operations](https://stringr.tidyverse.org) [@stringr]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-stringr-min.png){width=\"176\"}\n\nA consistent, simple and easy to use set of wrappers around the fantastic 'stringi' package. All function and argument names (and positions) are consistent, all functions deal with \"NA\"'s and zero length vectors in the same way, and the output from one function is easy to feed into the input of another.\n\n:::\n\nStrings are not glamorous, high-profile components of R, but they do play a big role in many data cleaning and preparation tasks. The {**stringr**} package provides a cohesive set of functions designed to make working with strings as easy as possible. If you’re not familiar with strings, the best place to start is the [chapter on strings](https://r4ds.hadley.nz/strings) in R for Data Science.\n\n{**stringr**} is built on top of {**stringi**}, which uses the [ICU](https://icu.unicode.org/) C library to provide fast, correct implementations of common string manipulations. {**stringr**} focusses on the most important and commonly used string manipulation functions whereas stringi provides a comprehensive set covering almost anything you can imagine. If you find that {**stringr**} is missing a function that you need, try looking in {**stringi**}. Both packages share similar conventions, so once you’ve mastered {**stringr**}, you should find {**stringi**} similarly easy to use.\n\n\n\n{**stringr**}: Simple, Consistent Wrappers for Common String Operations\n:::\n\nEven if I had not used {**stringi**} in this book I will add this package profile into the appropriate section @pak-stringi.\n\n***\n::::\n:::::\n\n\n## tableone\n\n:::::{.my-package}\n:::{.my-package-header}\ntableone: Create 'Table 1' to Describe Baseline Characteristics with or without Propensity Score Weights \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-tableone}\n\n***\n\n{**tableone**}: [Create 'Table 1' to Describe Baseline Characteristics with or without Propensity Score Weights](https://github.com/kaz-yos/tableone) [@tableone]\n\nCreates 'Table 1', i.e., description of baseline patient characteristics, which is essential in every medical research. Supports both continuous and categorical variables, as well as p-values and standardized mean differences. Weighted data are supported via the {**survey**} package.\n\n{**tableone**}: Create 'Table 1' to Describe Baseline Characteristics with or without Propensity Score Weights\n:::\n\nInstead of using {**tableone**} I will use {**gtsummry**} in conjunction with {**gt**}.\n\n***\n::::\n:::::\n\n\n## tabulizer\n\n:::::{.my-package}\n:::{.my-package-header}\ntabulizer: Bindings for 'Tabula' PDF Table Extractor Library \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-tublizer}\n\n***\n\n{**tabulizer**}: [Bindings for 'Tabula' PDF Table Extractor Library](https://docs.ropensci.org/tabulizer) [@tabulizer]\n\n(*There is no hexagon logo for {**tabulizer**} available*)\n\n\n{**tabulizer**} provides R bindings to the [Tabula java library](https://github.com/tabulapdf/tabula-java/), which can be used to computationaly extract tables from PDF documents. The {**tabulizerjars**} package <https://github.com/ropensci/tabulizerjars> provides versioned 'Java' .jar files, including all dependencies, aligned to releases of 'Tabula'.\n\ntabulizer depends on {**rJava**}, which implies a system requirement for Java. This can be frustrating, especially on Windows.\n\n{**tabulizer**}: A Package for Bindings for 'Tabula' PDF Table Extractor Library\n:::\n\n\n:::\n:::::\n\nI just noticed that {**tabulizer**} was removed from the CRAN repository. But you can still install it from the [CRAN archive](https://cran.r-project.org/src/contrib/Archive/tabulizer/) --- or even better --- from the [GitHub site](https://github.com/ropensci/tabulizer/). I have installed it several years ago (version 0.2.3) and it works smoothly.\n\nI have looked and tested alternatives, but nothing worked satisfactorily:\n\n\n[{**pdftool**}](https://docs.ropensci.org/pdftools/): A great tool to scrap text from PDFs, but not so good with tables: \"It is possible to use {**pdftools**} with some creativity to parse tables from PDF documents, which does not require Java to be installed.\" \n\nAn example how to do that is explained in [How to extract data tables from PDF in r Tutorial](How to extract data tables from PDF in r Tutorial), a video by Data Centrics Inc. Another approach can be found on [StackOverflow](https://stackoverflow.com/questions/60127375/using-the-pdf-data-function-from-the-pdftools-package-efficiently?rq=2). But both procedures are way to complex and I have to say that it does not repays the effort, especially with the small example table in the video tutorial. It would be much easier to use other tools, for instance on macOS with the app [TextSniper](https://www.textsniper.app/en) or even input the figures manually.\n\n[{**PDE**}](https://cran.r-project.org/web/packages/PDE/vignettes/PDE.html) PDF Data extractor (PDE) seems the right tool for the task because it should \"Extract Tables and Sentences from PDFs with User Interface\". I couldn't work with interactive user interface because I has many different options and I didn't have time to study them thoroughly. But I succeeded with the programming interface, although the result had some errors. Part of columns appeared in extra columns. So these errors were easy to detect and to repair.\n\nWith the following code I could extract all 13 tables from the ATF document and could also scrap the pictures in the PDF and convert them to PNGs.\n\n```\natf_tables <- PDE::PDE_pdfs2table(\n    pdfs = \"data/chap03/firearms_commerce_2019.pdf\",\n    out = \"data/chap03/test/\",\n    table.heading.words = \"Exhibit\",\n    out.table.format = \".csv (macintosh)\"\n)\n```\nIt took me about 20-30 seconds and I got the following message:\n\n> Following file is processing: 'firearms_commerce_2019.pdf'\nNo filter words chosen for analysis.\n13 table(s) found in 'firearms_commerce_2019.pdf'.\nAnalysis of 'firearms_commerce_2019.pdf' complete.\n\nMaybe the interactive UI would also work, but as I am very content with {**tabulizer**} I did not delve deeply into {**PDE**}.\n\n:::::{.my-important}\n:::{.my-important-header}\nMy recommendation: As the first choice try to install and use {**tabulizer**}. If this does not work for you, try {**PDE**}. \n\n***\n::::\n:::::\n\n\n\n\n\n## tibble \n\n:::::{.my-package}\n:::{.my-package-header}\ntibble: Simple Data Frames\n:::\n::::{.my-package-container}\n\n\n***\n\n::: {#pak-tibble}\n\n***\n\n{**tibble**}: [Simple Data Frames](https://tibble.tidyverse.org/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-tibble-min.png){width=\"176\"}\n\nA tibble, or `tbl_df`, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. Tibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced `print()` method which makes them easier to use with large datasets containing complex objects. [@tibble]\n\n:::\n\n\nPackage for Creating Simple Data Frames\n:::\n\n***\n\n\n::::\n:::::\n\n## tidyr \n\n:::::{.my-package}\n:::{.my-package-header}\ntidyr: Tidy Messy Data\n:::\n::::{.my-package-container}\n\n\n***\n\n::: {#pak-tidyr}\n\n***\n\n{**tidyr**}: [Tidy Messy Data](https://tidyr.tidyverse.org/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-tidyr-min.png){width=\"176\"}\n\nThe goal of tidyr is to help you create tidy data. Tidy data describes a standard way of storing data that is used wherever possible throughout the tidyverse. If you ensure that your data is tidy, you’ll spend less time fighting with the tools and more time working on your analysis. [@tidyr] Tidy data is data where:\n- Every column is a variable.\n- Every row is an observation.\n- Every cell is a single value.\n\n:::\n\nPackages for Tidying Messy Data\n:::\n\n\n***\n\n::::\n:::::\n\n\n\n\n## tidyselect\n\n:::::{.my-package}\n:::{.my-package-header}\ntidyselect: Select from a Set of Strings\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-tidyselect}\n\n***\n\n{**tidyselect**}: [Select from a Set of Strings](https://tidyselect.r-lib.org/index.html)\n\n{*There is no hexagon logo for {**tidyselect**} available*}\n\nThe {**tidyselect**} package is the backend of functions like `dplyr::select()` or `dplyr::pull()` as well as several {**tidyr**} verbs. It allows you to create selecting verbs that are consistent with other {**tidyverse**} packages.\n\nTo learn about the selection syntax as a user of {**dplyr**} or {**tidyr**}, read the user-friendly [?language](https://tidyselect.r-lib.org/reference/language.html) reference.\n\nTo learn how to implement tidyselect in your own functions, read [vignette(\"tidyselect\")](https://tidyselect.r-lib.org/articles/tidyselect.html).\n\nTo learn exactly how the {**tidyselect**} syntax is interpreted, read the technical description in [vignette(\"syntax\")](https://tidyselect.r-lib.org/articles/syntax.html).\n\n{**tidyselect**}: A Package to Select from a Set of Strings\n:::\n\n***\n::::\n:::::\n\n\n\n\n\n\n\n\n## tidyverse\n\n:::::{.my-package}\n:::{.my-package-header}\ntidyverse: Easily Install and Load the 'Tidyverse'\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-tidyverse}\n\n***\n\n{**tidyverse**}: [Easily Install and Load the 'Tidyverse'](https://www.tidyverse.org/)\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap01/logoi/logo-tidyverse-min.png){width=\"176\"}\n\nThe {**tidyverse**} is an opinionated [collection of R packages](https://www.tidyverse.org/packages/) designed for data science. All packages share an underlying design philosophy, grammar, and data structures [@tidyverse]. Read more about the philosophy and purpose: [The tidy tools manifesto](https://tidyverse.tidyverse.org/articles/manifesto.html) and [Welcome to the {**tidyverse**}](https://tidyverse.tidyverse.org/articles/paper.html) \n\n:::\n\nIn this book I am not going to load {**tidyverse**} with all its packages. Instead I am using the `<package>::<function>` format to access the commands.\n\n\n\nMetapackage: Easily Install and Load the 'Tidyverse'\n\n***\n\n:::\n\n::::\n:::::\n\n\n## vcd\n\n:::::{.my-package}\n:::{.my-package-header}\nvcd: Visualizing Categorical Data \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-vcd}\n\n***\n\n{**vcd**}: [Visualizing Categorical Data](https://cran.r-project.org/web/packages/vcd/index.html) [@vcd]                      \n\n(*There is no hexagon logo for {**vcd**} available*)\n\nVisualization techniques, data sets, summary and inference procedures aimed particularly at categorical data. Special emphasis is given to highly extensible grid graphics. The package was package was originally inspired by the book \"Visualizing Categorical Data\" by Michael Friendly and is now the main support package for a new book, \"Discrete Data Analysis with R\" by Michael Friendly and David Meyer [-@friendly2015].\n\n{**vcd**}: A Package for Visualizing Categorical Data\n:::\n\n***\n::::\n:::::\n\n## vcdExtra\n\n:::::{.my-package}\n:::{.my-package-header}\nvcdExtra: 'vcd' Extensions and Additions \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-vcdExtra}\n\n***\n\n{**vcdExtra**}: ['vcd' Extensions and Additions](https://friendly.github.io/vcdExtra/) [@vcdExtra]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap03/logoi/logo-vcdExtra-min.png){width=\"176\"}\n\n\nThis package provides additional data sets, documentation, and many functions designed to extend the vcd package for Visualizing Categorical Data and the {**gnm**} package for Generalized Nonlinear Models. In particular, {**vcdExtra**} extends mosaic, assoc and sieve plots from {**vcd**} to handle `stats::glm()` and `gnm::gnm()` models and adds a 3D version in `vcdExtra::mosaic3d()`.\n\n:::\n\n{**vcdExtra**} is a support package for the book Discrete Data Analysis with R (DDAR) by Michael Friendly and David Meyer [-@friendly2015]. There is also a [web site for DDAR](http://ddar.datavis.ca/) with all figures and code samples from the book. It is also used in my graduate course, [Psy 6136: Categorical Data Analysis](https://friendly.github.io/psy6136/).\n\n{**vcdExtra**}: A Package for 'vcd' Extensions and Additions\n:::\n\n***\n::::\n:::::\n\n\n\n## viridis\n\n::: {.my-package}\n::: {.my-package-header}\nviridis: Colorblind-Friendly Color Maps for R\n:::\n\n::: {.my-package-container}\n\n------------------------------------------------------------------------\n\n::: {#pak-viridis}\n\n------------------------------------------------------------------------\n\n{**viridis**}: <https://sjmgarnier.github.io/viridis/> [@viridis]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap03/logoi/logo-viridis-min.png){width=\"176\"}\n\n{**viridis**}, and its companion package {**viridisLite**} provide a\nseries of color maps that are designed to improve graph readability for\nreaders with common forms of color blindness and/or color vision\ndeficiency. The color maps are also perceptually-uniform, both in\nregular form and also when converted to black-and-white for printing.\n:::\n\n{**viridisLite**} provides the base functions for generating the color\nmaps in base R. The package is meant to be as lightweight and\ndependency-free as possible for maximum compatibility with all the R\necosystem. {**viridis**} provides additional functionalities, in\nparticular bindings for {**ggplot2**}.\n\n{**viridis**}: A Package for Colorblind-Friendly Color Maps for R\n:::\n\n:::\n:::\n\n## waffle\n\n::: {.my-package}\n::: {.my-package-header}\nwaffle: Create Waffle Chart Visualizations\n:::\n\n::: {.my-package-container}\n\n------------------------------------------------------------------------\n\n::: {#pak-waffle}\n\n------------------------------------------------------------------------\n\n{**waffle**}: [Create Waffle Chart\nVisualizations](https://cinc.rud.is/web/packages/waffle/index.html)\n\n(*There is no hexagon logo for {**waffle**} available*)\n\nSquare pie charts (a.k.a. waffle charts) can be used to communicate\nparts of a whole for categorical quantities. To emulate the percentage\nview of a pie chart, a 10x10 grid should be used with each square\nrepresenting 1% of the total.\n\nModern uses of waffle charts do not necessarily adhere to this rule and\ncan be created with a grid of any rectangular shape.\n\nBest practices suggest keeping the number of categories small, just as\nshould be done when creating pie charts.\n\nTools are provided to create waffle charts as well as stitch them\ntogether, and to use glyphs for making isotype pictograms.\n\nIt uses {**ggplot2**} and returns a `ggplot2` object.\n\n{**waffle**}: A Package for Creating Waffle Chart Visualizations\n\n\n::::\n:::::\n\n\n:::\n\n## withr\n\n:::::{.my-package}\n:::{.my-package-header}\nwithr: Run Code 'With' Temporarily Modified Global State \n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-withr}\n\n***\n\n{**withr**}: [Run Code 'With' Temporarily Modified Global State](https://withr.r-lib.org/) [@withr]\n\n::: {layout=\"[10, 30]\" layout-valign=\"center\"}\n![](img/chap03/logoi/logo-withr-min.png){width=\"176\"}\n\n\nA set of functions to run code with safely and temporarily modified global state, withr makes working with the global state, i.e. side effects, less error-prone.\n\n:::\n\nPure functions, such as the `sum()` function, are easy to understand and reason about: they always map the same input to the same output and have no other impact on the workspace. In other words, pure functions have no *side effects*: they are not affected by, nor do they affect, the global state in any way apart from the value they return.\n\nThe purpose of the {**withr**} package is to help you manage side effects in your code. You may want to run code with secret information, such as an API key, that you store as an environment variable. You may also want to run code with certain options, with a given random-seed, or with a particular working-directory.\n\nThe {**withr**} package helps you manage these situations, and more, by providing functions to modify the global state temporarily, and safely. These functions modify one of the global settings for duration of a block of code, then automatically reset it after the block is completed.\n\n{**withr**}: Run Code 'With' Temporarily Modified Global State\n:::\n\n\n***\n\n:::\n:::\n\n\n\n\n\n\n\n\n## qualvar\n\n:::::{.my-package}\n:::{.my-package-header}\nqualvar: Implements Indices of Qualitative Variation Proposed by Wilcox (1973)\n:::\n::::{.my-package-container}\n\n***\n\n::: {#pak-qualvar}\n\n***\n\n{**qualvar**}: [Implements Indices of Qualitative Variation Proposed by Wilcox (1973)](http://joelgombin.github.io/qualvar/)\n\n(*There is no hexagon logo for {**qualvar**} available*)\n\nIn 1973, Wilcox published a paper presenting various indices of qualitative variation for social scientists. The problem is to find relevant statistical indices to measure the variation in nominal-scale (i.e. qualitative or categorical) data. Please see the Wilcox paper for more details on the rationale.\n\nWilcox presents six indices that can be used to measure qualitative variation. The {**qualvar**} package implements these indices so that R users can easily use them.\n\n{**qualvar**}: A Package for Computing Indices of Qualitative Variation\n:::\n\n\n\n***\n::::\n:::::\n\n",
    "supporting": [
      "96-packages-used_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}