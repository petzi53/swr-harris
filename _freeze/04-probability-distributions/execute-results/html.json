{
  "hash": "1f07a328c249af7cde2b7b932a66f958",
  "result": {
    "engine": "knitr",
    "markdown": "# Probability distribution {#sec-chap04}\n\n\n\n\n\n\n\n\n## Achievements to unlock\n\n::: my-objectives\n::: my-objectives-header\nObjectives\n:::\n\n::: my-objectives-container\n**SwR Achievements**\n\n- **Achievement 1**: Defining and using probability distributions to infer from a sample (@sec-chap04-achievement1)\n- **Achievement 2**: Understanding the characteristics and uses of a binomial distribution of a binary variable (@sec-chap04-achievement2)\n- **Achievement 3**: Understanding the characteristics and uses of the normal distribution of a continuous variable (@sec-chap04-achievement3)\n- **Achievement 4**: Computing and interpreting z-scores to compare observations to groups (@sec-chap04-achievement4)\n- **Achievement 5**: Estimating population means from sample means using the normal distribution (@sec-chap04-achievement5)\n- **Achievement 6**: Computing and interpreting confidence intervals around means and proportions (@sec-chap04-achievement6)\n\n:::\n:::\n\n\n## The opioid overdose problem\n\nThere is an alarming increases in drug overdoses in the United States in recent years (see [County Health Rankings & Roadmaps website](https://https://www.countyhealthrankings.org/findings-and-insights/2023-county-health-rankings-national-findings-report) and [Data & Documentation](https://www.countyhealthrankings.org/health-data/methodology-and-sources/rankings-data-documentation#main)).\n\nThe <a class='glossary' title='Centers for Disease Control and Preventation (CDC) is the U.S. leading science-based, data-driven, service organization that protects the public’s health. (CDC)'>CDC</a> Wonder website has data on the underlying cause of each death in the United States. For drug deaths, the CDC WONDER data include the drug implicated in each death, if available. \n\nStates had begun to adopt policies to try to combat the opioid epidemic. Some of the state-level policy solutions to addressing the increasing number of opioid overdoses: \n\n- Imposition of quantity limits \n- Required prior authorization for opioids \n- Use of clinical criteria for prescribing opioids \n- Step therapy requirements \n- Required use of prescription drug monitoring programs.\n\nThe Kaiser Family Foundation (<a class='glossary' title='Kaiser Family Foundation (KFF) is a non-partisan organization focused on health policy. It conducts its own research, polling, journalism, and specialized public health information campaigns and its website has been heralded for having the “most up-to-date and accurate information on health policy”[4] and as a “must-read for healthcare devotees.” (Wikipdia)'>KFF</a>) keeps track of the adoption of these policies across all 50 states and the District of Columbia.\n\nTreatment programs as well as policies depend partly on the distance people have to travel to the nearest health facility. <a class='glossary' title='amfAR, the Foundation for AIDS Research, known until 2005 as the American Foundation for AIDS Research, is an international nonprofit organization dedicated to the support of AIDS research, HIV prevention, treatment education, and the advocacy of AIDS-related public policy. (Wikipedia)'>amfAR</a>, the Foundation for AIDS Research, which has an Opioid & Health Indicators Database (https://opioid.amfar.org). The data in amfAR’s database includes distance to the nearest substance abuse treatment facility that has medication assisted therapies (MAT).\n\n## Resources & Chapter Outline\n\n### Data, codebook, and R packages {#sec-chap04-data-codebook-packages}\n\n::: my-resource\n::: my-resource-header\nData, codebook, and R packages for learning about descriptive statistics\n:::\n\n::: my-resource-container\n\n**Data**\n\n1.  Download clean data sets `pdmp_2017_kff_ch4.csv` and `opioid_dist_to_facility_2017_ch4.csv` from\n    <https://edge.sagepub.com/harris1e>.\n2.  Download the county-level distance data files directly from the amfAR website (https://opioid.amfar.org/indicator/dist_MAT)\n3.  Import and clean the data for 2017 from Table 19 in the online report on the [KFF website](https://www.kff.org/report-section/implementing-coverage-and-payment-initiatives-benefits-and-pharmacy/)\n\n**Codebook**\n\nTwo options:\n\n1.  Download the codebook file `opioid_county_codebook.xlsx` from\n    <https://edge.sagepub.com/harris1e>.\n2.  Use the online version of the codebook from the amfAR Opioid & Health Indicators Database website (https://opioid.amfar.org)\n\n\n\n**Packages**\n\n1. Packages used with the book (sorted alphabetically)\n\n-   {**tidyverse**}: @pak-tidyverse (Hadley Wickham)\n\n    \n2. My additional packages (sorted alphabetically)\n\n\n\n:::\n:::\n\n### Get data\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-get-data}\n: Get data for chapter 4\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### PDMP\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-get-pdmp-book}\n: Get the cleaned PDMP data from the book `.csv` file\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell reslts='hold'}\n\n```{.r .cell-code}\n## run code only once manually ##########\n\n## get pdmp data from .csv file of the book\npdmp_2017_book <- readr::read_csv(\"data/chap04/pdmp_2017_kff_ch4.csv\")\nsave_data_file(\"chap04\", pdmp_2017_book, \"pdmp_2017_book.rds\")\n```\n:::\n\n\n***\n\n(*For this R code chunk is no output available*)\n\n::::\n:::::\n\n\n\n\n\n###### anfAR\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-amfAR}\n: Numbered R Code Title (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## run only once, manually ############\namfar_file <- \"data/chap04/opioid_dist_to_facility_2017_ch4.csv\"\n\ndist_mat <- readr::read_csv(amfar_file)\nsave_data_file(\"chap04\", dist_mat, \"dist_mat.rds\")\n```\n:::\n\n\n***\n\n(*For this R code chunk is no output available*)\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n***\n\n\n### Show raw data\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-show-data}\n: Show raw data for chapter 4\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### PDMP\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-show-pdmp-data}\n: Show data for the prescription drug monitoring programs (PDMPs)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npdmp_2017_book <- base::readRDS(\"data/chap04/pdmp_2017_book.rds\")\n\nglue::glue(\"********************* Show summary *******************\")\nbase::summary(pdmp_2017_book)\n\nglue::glue(\"\")\nglue::glue(\"****************** Show selected data ****************\")\nglance_data(pdmp_2017_book)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ********************* Show summary *******************\n#>     States          Opioid Quantity Limits Clinical Edits in Claim System\n#>  Length:51          Length:51              Length:51                     \n#>  Class :character   Class :character       Class :character              \n#>  Mode  :character   Mode  :character       Mode  :character              \n#>  Opioid Step Therapy Requirements\n#>  Length:51                       \n#>  Class :character                \n#>  Mode  :character                \n#>  Other Prior Authorization Requirements for Opioids\n#>  Length:51                                         \n#>  Class :character                                  \n#>  Mode  :character                                  \n#>  Required Use of Prescription Drug Monitoring Programs\n#>  Length:51                                            \n#>  Class :character                                     \n#>  Mode  :character                                     \n#>  Any Opioid Management Strategies In Place\n#>  Length:51                                \n#>  Class :character                         \n#>  Mode  :character                         \n#> \n#> ****************** Show selected data ****************\n#> # A tibble: 10 × 8\n#>      obs States        `Opioid Quantity Limits` `Clinical Edits in Claim System`\n#>    <int> <chr>         <chr>                    <chr>                           \n#>  1     1 Alabama       Yes                      Yes                             \n#>  2     1 Alabama       Yes                      Yes                             \n#>  3    10 Mississippi   Yes                      Yes                             \n#>  4    18 Washington    Yes                      Yes                             \n#>  5    24 Connecticut   Yes                      Yes                             \n#>  6    25 Delaware      Yes                      Yes                             \n#>  7    36 New Hampshire Yes                      Yes                             \n#>  8    37 New Mexico    Yes                      No                              \n#>  9    49 Virginia      Yes                      Yes                             \n#> 10    51 Wisconsin     Yes                      Yes                             \n#> # ℹ 4 more variables: `Opioid Step Therapy Requirements` <chr>,\n#> #   `Other Prior Authorization Requirements for Opioids` <chr>,\n#> #   `Required Use of Prescription Drug Monitoring Programs` <chr>,\n#> #   `Any Opioid Management Strategies In Place` <chr>\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n###### dist-mat\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-show-dist-mat}\n: Show the distances to nearest substance abuse facility providing medication assisted treatment (MAT)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist_mat <- base::readRDS(\"data/chap04/dist_mat.rds\")\n\nglue::glue(\"********************* Show summary *******************\")\nbase::summary(dist_mat)\n\nglue::glue(\"\")\nglue::glue(\"****************** Show selected data ****************\")\nglance_data(dist_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ********************* Show summary *******************\n#>     STATEFP         COUNTYFP          YEAR       INDICATOR        \n#>  Min.   : 1.00   Min.   :  1.0   Min.   :2017   Length:3214       \n#>  1st Qu.:19.00   1st Qu.: 35.0   1st Qu.:2017   Class :character  \n#>  Median :30.00   Median : 79.0   Median :2017   Mode  :character  \n#>  Mean   :31.25   Mean   :101.9   Mean   :2017                     \n#>  3rd Qu.:46.00   3rd Qu.:133.0   3rd Qu.:2017                     \n#>  Max.   :72.00   Max.   :840.0   Max.   :2017                     \n#>      VALUE           STATE           STATEABBREVIATION     COUNTY         \n#>  Min.   :  0.00   Length:3214        Length:3214        Length:3214       \n#>  1st Qu.:  9.25   Class :character   Class :character   Class :character  \n#>  Median : 18.17   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   : 24.04                                                           \n#>  3rd Qu.: 31.00                                                           \n#>  Max.   :414.86                                                           \n#> \n#> ****************** Show selected data ****************\n#> # A tibble: 10 × 9\n#>      obs STATEFP COUNTYFP  YEAR INDICATOR VALUE STATE   STATEABBREVIATION COUNTY\n#>    <int>   <dbl>    <dbl> <dbl> <chr>     <dbl> <chr>   <chr>             <chr> \n#>  1     1       1        1  2017 dist_MAT  23.5  Alabama AL                Autau…\n#>  2   634      17       77  2017 dist_MAT   8.73 Illino… IL                Jacks…\n#>  3  1098      21      209  2017 dist_MAT   6    Kentuc… KY                Scott…\n#>  4  1152      22       77  2017 dist_MAT  25.4  Louisi… LA                Point…\n#>  5  1177      22      127  2017 dist_MAT  36    Louisi… LA                Winn …\n#>  6  1252      26       41  2017 dist_MAT  14.6  Michig… MI                Delta…\n#>  7  2097      39      107  2017 dist_MAT  21.8  Ohio    OH                Merce…\n#>  8  2369      46       15  2017 dist_MAT  26.7  South … SD                Brule…\n#>  9  2609      48      171  2017 dist_MAT  31.8  Texas   TX                Gille…\n#> 10  3214      72      153  2017 dist_MAT  14    Puerto… PR                Yauco…\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n***\n\n### Recode data\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-recode}\n: Recode data for chapter 4\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Transform amfAR\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-code-name-a}\n: Extend amfAR data with transformed values\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist_mat_clean <- dist_mat |> \n    dplyr::mutate(square_root = sqrt(VALUE),\n                  cube_root = VALUE^(1/3),\n                  log = log(VALUE),\n                  inverse = 1/VALUE\n    )\n\nsave_data_file(\"chap04\", dist_mat_clean, \"dist_mat_clean.rds\")\n\nbase::summary(dist_mat_clean)\nglance_data(dist_mat_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>     STATEFP         COUNTYFP          YEAR       INDICATOR        \n#>  Min.   : 1.00   Min.   :  1.0   Min.   :2017   Length:3214       \n#>  1st Qu.:19.00   1st Qu.: 35.0   1st Qu.:2017   Class :character  \n#>  Median :30.00   Median : 79.0   Median :2017   Mode  :character  \n#>  Mean   :31.25   Mean   :101.9   Mean   :2017                     \n#>  3rd Qu.:46.00   3rd Qu.:133.0   3rd Qu.:2017                     \n#>  Max.   :72.00   Max.   :840.0   Max.   :2017                     \n#>      VALUE           STATE           STATEABBREVIATION     COUNTY         \n#>  Min.   :  0.00   Length:3214        Length:3214        Length:3214       \n#>  1st Qu.:  9.25   Class :character   Class :character   Class :character  \n#>  Median : 18.17   Mode  :character   Mode  :character   Mode  :character  \n#>  Mean   : 24.04                                                           \n#>  3rd Qu.: 31.00                                                           \n#>  Max.   :414.86                                                           \n#>   square_root       cube_root          log           inverse       \n#>  Min.   : 0.000   Min.   :0.000   Min.   : -Inf   Min.   :0.00241  \n#>  1st Qu.: 3.041   1st Qu.:2.099   1st Qu.:2.225   1st Qu.:0.03226  \n#>  Median : 4.263   Median :2.629   Median :2.900   Median :0.05504  \n#>  Mean   : 4.492   Mean   :2.663   Mean   : -Inf   Mean   :    Inf  \n#>  3rd Qu.: 5.568   3rd Qu.:3.141   3rd Qu.:3.434   3rd Qu.:0.10811  \n#>  Max.   :20.368   Max.   :7.458   Max.   :6.028   Max.   :    Inf  \n#> # A tibble: 10 × 13\n#>      obs STATEFP COUNTYFP  YEAR INDICATOR VALUE STATE   STATEABBREVIATION COUNTY\n#>    <int>   <dbl>    <dbl> <dbl> <chr>     <dbl> <chr>   <chr>             <chr> \n#>  1     1       1        1  2017 dist_MAT  23.5  Alabama AL                Autau…\n#>  2   634      17       77  2017 dist_MAT   8.73 Illino… IL                Jacks…\n#>  3  1098      21      209  2017 dist_MAT   6    Kentuc… KY                Scott…\n#>  4  1152      22       77  2017 dist_MAT  25.4  Louisi… LA                Point…\n#>  5  1177      22      127  2017 dist_MAT  36    Louisi… LA                Winn …\n#>  6  1252      26       41  2017 dist_MAT  14.6  Michig… MI                Delta…\n#>  7  2097      39      107  2017 dist_MAT  21.8  Ohio    OH                Merce…\n#>  8  2369      46       15  2017 dist_MAT  26.7  South … SD                Brule…\n#>  9  2609      48      171  2017 dist_MAT  31.8  Texas   TX                Gille…\n#> 10  3214      72      153  2017 dist_MAT  14    Puerto… PR                Yauco…\n#> # ℹ 4 more variables: square_root <dbl>, cube_root <dbl>, log <dbl>,\n#> #   inverse <dbl>\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n###### Rename `VALUE` in amfAR\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-rename-amfar-distance}\n: Rename amfAR `VALUE` to `distance`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist_mat_clean2 <- dist_mat |> \n    dplyr::rename(distance = VALUE) \n\nsave_data_file(\"chap04\", dist_mat_clean2, \"dist_mat_clean2.rds\")\n```\n:::\n\n\n***\n\n(*For this R code chunk is no output available*)\n\n::::\n:::::\n\n###### Prepare PDMP data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-prepare-pdmp}\n: Rename column in PDMP and recode Yes/No to 1/0\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\npdmp_2017_book <- base::readRDS(\"data/chap04/pdmp_2017_book.rds\")\n\n## recode Yes to 1 and No to 0\npdmp_2017_book_clean <- pdmp_2017_book |> \n    dplyr::rename(PDMP = 6) |> \n    dplyr::mutate(PDMP =\n          dplyr::if_else(PDMP == \"Yes\", 1, 0)\n          ) |> \n    dplyr::mutate(PDMP = as.numeric(PDMP))\n\nsave_data_file(\"chap04\", pdmp_2017_book_clean, \"pdmp_2017_book_clean.rds\")\n```\n:::\n\n\n***\n\n(*For this R code chunk is no output available*)\n\n::::\n:::::\n\n\n\n\n:::\n\n::::\n:::::\n\n***\n\n\n## Achievement 1: Probability distributions to infer from a sample {#sec-chap04-achievement1}\n\nA <a class='glossary' title='It is a way of describing all possible events and the probability of each one happening. Probability distributions are also very useful for asking questions about ranges of possible values. (BF, Chap.4) The two defining features are: (1) All values of the distribution must be real and non-negative. (2) The sum (for discrete random variables) or integral (for continuous random variables) across all possible values of the random variable must be 1. (BS, Chap.3)'>probability distribution</a> is the set of probabilities that each possible value (or range of values) of a variable occurs.\n\nProbability distributions have two characteristics:\n\n1. The probability of each real value of some variable is non-negative; it is either zero or positive. 2. The sum of the probabilities of all possible values of a variable is 1.\n\nThere are two categories of probability distributions:\n\n1. Discrete probability distributions: An example is the binomial distribution.\n2. Continuous probability distributions: An example is the normal distribution.\n\n\n## Achievement 2: Binomial distribution of a binary variable {#sec-chap04-achievement2}\n\n### Characteristics of binomial random variables\n\n:::{#bul-chap04-binomial-random-variable}\n:::::{.my-bullet-list}\n:::{.my-bullet-list-header}\nBullet List\n:::\n::::{.my-bullet-list-container}\n\n- A variable is measured in the same way n times. \n- There are only two possible values of the variable, often called “success” and “failure.” \n- Each observation is independent of the others. \n- The probability of “success” is the same for each observation. \n- The random variable is the number of successes in n measurements.\n\nThe binomial distribution is defined by two things: \n\n- **n**, which is the number of observations (e.g., coin flips, people surveyed, states selected) \n- **p**, which is the probability of success (e.g., 50% chance of heads for a coin flip, 51% chance of a state having a PDMP)\n\n::::\n:::::\nCharacteristics of binomial random variables\n:::\n\n***\n\n### dbinomial() & pbinomial()\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-binomial-distributions}\n: Statistical properties of binomial distributions\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### `dbinomial()` with exact `n`\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-comp-dbinomial-exact}\n: Compute binomial probability with exact number of success\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## exact 5 successes from 20 selections \n## with 51% probability of success \nstats::dbinom(x = 5, size = 20, prob = .51) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1.205691\n```\n\n\n:::\n:::\n\n***\n\nComputed the probability given \n\n- the number of successes (`x`), \n- the sample size (`size =`), and \n- the probability of success (`prob =`).\n\nThe probabilities are very small for scenarios of getting *exactly* 10 states with PDMPs in a sample.\n::::\n:::::\n\nThe probabilities are very small for scenarios of getting *exactly* 10 states with <a class='glossary' title='In the United States, prescription monitoring programs (PMPs) or prescription drug monitoring programs (PDMPs) are state-run programs which collect and distribute data about the prescription and dispensation of federally controlled substances and, depending on state requirements, other potentially abusable prescription drugs. PMPs are meant to help prevent adverse drug-related events such as opioid overdoses, drug diversion, and substance abuse by decreasing the amount and/or frequency of opioid prescribing, and by identifying those patients who are obtaining prescriptions from multiple providers (i.e., “doctor shopping”) or those physicians overprescribing opioids. (Wikipedia)'>PDMP</a>s in a sample. The <a class='glossary' title='A cumulative distribution function (CDF) tells us the probability that a random variable takes on a value less than or equal to x. (Statology) It sums all parts of the distribution, replacing a lot of calculus work. The CDF takes in a value and returns the probability of getting that value or lower. (BF, Chap.13) A CDF is a hypothetical model of a distribution, the ECDF models empirical (i.e. observed) data. (Statistics How To)'>cumulative distribution function</a> for the binomial distribution can determine the probability of getting some range of values, which is often more useful than finding the probability of one specific number of successes.\n\n\n###### `pbinomial()` with range\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-comp-dbinomial-range}\n: Compute binomial probability of getting some range of values\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase::options(scipen = 999)\n\n\n\n## 5 or less successes from 20 selections \n## with 51% probability of success \npbinom(q = 5, size = 20, prob = .51) * 100\n\n## 10 or more successes from 20 selections \n### with 51% probability of success \npbinom(q = 5, size = 20, prob = .51, lower.tail = FALSE) * 100\n\n\nbase::options(scipen = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1.664024\n#> [1] 98.33598\n```\n\n\n:::\n:::\n\n\n- **Exactly 5** successes with a success probability of 51% = `base::round(dbinom(x = 5, size = 20, prob = .51) * 100, 3)` : 1.206%.\n- **5 or fewer** successes with a success probability of 51% = `base::round(pbinom(q = 5, size = 20, prob = .51) * 100, 3)`: 1.664%.\n- **6 or more** successes with a success probability of 51% = `base::round(pbinom(q = 5, size = 20, prob = .51, lower.tail = FALSE) * 100, 3)`: \n98.336%.\n\n:::::{.my-important}\n:::{.my-important-header}\nFor probabilities `q and more` you have to take `q - 1` and add `lower.tail = FALSE`.\n:::\n:::::\n\n::::\n:::::\n\n###### Sample PDMPs from data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-sample-pdmp}\n: Sample 25 states from population data (n = 51)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\npdmp_2017_book <- base::readRDS(\"data/chap04/pdmp_2017_book.rds\")\n\n## set a starting value for sampling \nset.seed(seed = 10) \n\n## sample 25 states and check \npdmp_2017_book |>  \n    dplyr::select(`Required Use of Prescription Drug Monitoring Programs`) |> \n    dplyr::mutate(`Required Use of Prescription Drug Monitoring Programs` =\n          forcats::as_factor(`Required Use of Prescription Drug Monitoring Programs`)) |> \n    dplyr::slice_sample(n = 25) |> \n    base::summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Required Use of Prescription Drug Monitoring Programs\n#>  No :10                                               \n#>  Yes:15\n```\n\n\n:::\n:::\n\n***\n\nThe book features a lengthy explication of the `set.seed()` function and their revised working after R version 3.6. But this important detail is now --- several years after 3.6.0 appeared in April 2019 --- not so relevant anymore.\n\nI had to recode the character variable to a factor and I used `dplyr::slice_sample()` instead of the superseded `dplyr::sample_n()` function.\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n### Visualizing the binomial distribution\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-visualize-binomial-dist}\n: Visualizing the binomial distribution\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Distribution only\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-binomial-dist-only}\n: Binomial distribution of 20 selected states when 51% have PDMPs\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase::set.seed(42)\n\nbinomial_data <- tibble::tibble(stats::rbinom(1000, 20, .51)) |> \n    dplyr::rename(data = 1) |> \n    dplyr::mutate(my_color = \n                dplyr::if_else(data <= 5, \"purple\", \"grey\")\n    )\n\n\nbinomial_data |> \n    ggplot2::ggplot() +\n    ggplot2::aes(x = data,\n                 y = ggplot2::after_stat(count) / \n                     base::sum(count)\n                 ) +\n    ggplot2::geom_histogram(\n        color = \"black\", \n        fill = \"grey\",\n        binwidth = 1\n        ) +\n    ggplot2::theme_bw() +\n    ggplot2::scale_x_continuous(\n        breaks = base::seq(0, 20, 2)) +\n    ggplot2::labs(x = 'States with monitoring programs',\n       y = 'Probability exactly this many selected')\n```\n\n::: {.cell-output-display}\n![Probability mass function plot showing probability of number of selected states \nwith PDMPs out of 20 total selected when 51% have PDMPs overall](04-probability-distributions_files/figure-html/fig-binomial-dist-only-1.png){#fig-binomial-dist-only width=672}\n:::\n:::\n\n\n\n::::\n:::::\n\n:::::{.my-resource}\n:::{.my-resource-header}\n:::::: {#lem-chap04-econ41-lab}\nHelpful code snippet at \"ECON 41 Lab\"\n::::::\n:::\n::::{.my-resource-container}\nI got help for the code from [15 Tutorial 4: The Binomial Distribution](https://bookdown.org/gabriel_butler/ECON41Labs/tutorial-4-the-binomial-distribution.html) [@butler2019].\n\n::::\n:::::\n\n\n###### Distribution with marker\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-binomial-dist-marker}\n: Probability of 5 or fewer selected states with PDMPs out of 20 total selected \\nwhen 51% have PDMPs overall\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase::set.seed(42)\n\ncolors <- c(rep(\"purple\", 2), rep(\"grey\", 13))\n\nbinomial_data <- tibble::tibble(stats::rbinom(1000, 20, .51)) |> \n    dplyr::rename(data = 1) |> \n    dplyr::mutate(my_color = \n                dplyr::if_else(data <= 5, \"purple\", \"grey\")\n    )\n\n\nbinomial_data |> \n    ggplot2::ggplot() +\n    ggplot2::aes(x = data,\n                 y = ggplot2::after_stat(count) / \n                     base::sum(count)\n                 ) +\n    ggplot2::geom_histogram(\n        color = \"black\", \n        fill = colors,\n        binwidth = 1\n        ) +\n    ggplot2::geom_vline(xintercept = 5, \n             linewidth = 1, \n             linetype = 'dashed',\n             color = 'red') +\n    ggplot2::theme_bw() +\n    ggplot2::scale_x_continuous(breaks = base::seq(0, 20, 2)) +\n    ggplot2::labs(x = 'States with monitoring programs',\n       y = 'Probability exactly this many selected') \n```\n\n::: {.cell-output-display}\n![Probability of 5 or fewer selected states with PDMPs out of 20 total selected when 51% have PDMPs overall](04-probability-distributions_files/figure-html/fig-binomial-dist-marker-1.png){#fig-binomial-dist-marker width=672}\n:::\n:::\n\n\n::::\n:::::\n\n###### Histogram colorized\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-binomial-dist-color}\n: Probability of 5 or fewer selected states with PDMPs out of 20 total selected \\nwhen 51% have PDMPs overall\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase::set.seed(42)\n\nbinomial_data <- tibble::tibble(stats::rbinom(1000, 20, .51)) |> \n    dplyr::rename(data = 1) |> \n    dplyr::mutate(my_color = \n            dplyr::if_else(data <= 5, \"purple\", \"grey\")\n    ) |> \n    dplyr::mutate(my_color =\n            forcats::as_factor(my_color))\n\n\nbinomial_data |> \n    ggplot2::ggplot() +\n    ggplot2::aes(x = data,\n                 y = ggplot2::after_stat(count) / \n                     base::sum(count),\n                 fill = my_color\n                 ) +\n    ggplot2::geom_histogram(\n        binwidth = 1,\n        color = \"black\"\n    ) +\n    ggplot2::geom_vline(xintercept = 5, \n             linewidth = 1, \n             linetype = 'dashed',\n             color = 'red') +\n    ggplot2::theme_bw() +\n    ggplot2::scale_x_continuous(breaks = base::seq(0, 20, 2)) +\n    ggplot2::scale_fill_manual(name = \"Number of states\\nwith PDMP\",\n                               values = c(\"grey\" = \"grey\",\n                               \"purple\" = \"purple\"),\n                               labels = c(\"> 5\", \"5 or fewer\")) +\n    ggplot2::labs(x = 'States with monitoring programs',\n       y = 'Probability exactly this many selected') \n```\n\n::: {.cell-output-display}\n![Probability of 5 or fewer selected states with PDMPs out of 20 total selected when 51% have PDMPs overall](04-probability-distributions_files/figure-html/fig-binomial-dist-color-1.png){#fig-binomial-dist-color width=672}\n:::\n:::\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n***\n\n## Achievement 3: Normal distribution of a continuous variable {#sec-chap04-achievement3}\n\n### Working with normal distributions\n\nBinomial data in social sciences are only one type of data. Many data are continuous variables. Just as the shape of the binomial distribution is determined by `n` and `p`, the shape of the normal distribution for a variable in a sample is determined by `$mu$` and `$sigma$`.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-dist-mat-dist}\n: Distribution of the distances to nearest facility providing MAT\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Distances\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-dist-mat-normal}\n: Distribution of the original distance variable\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist_mat_clean <- base::readRDS(\"data/chap04/dist_mat_clean.rds\")\n\ndist_mat_clean |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = VALUE) \n    ) +\n    ggplot2::geom_histogram(\n        bins = 30,\n        fill = \"grey\",\n        color = \"black\"\n        ) +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        x = \"Distance in miles\",\n        y = \"Number of counties\"\n    )\n```\n\n::: {.cell-output-display}\n![Distribution of the distance to the nearest facility with MAT](04-probability-distributions_files/figure-html/fig-dist-mat-normal-1.png){#fig-dist-mat-normal width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\n###### Distances transformed\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-dist-mat-transformed}\n: Distribution of the distance variable transformed by various factors\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## using the extended data frame \n## with square root, cube root, inverse $ log values\n\np_cube_root <- dist_mat_clean |>\n    ggplot2::ggplot(\n        ggplot2::aes(x = cube_root)\n    ) +\n    ggplot2::geom_density(\n        color = \"black\",\n        fill = \"grey\"\n    ) +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        x = \"Cube root of miles to nearest facility\",\n        y = \"Density\"\n    )\n\n\np_square_root <- dist_mat_clean |>\n    ggplot2::ggplot(\n        ggplot2::aes(x = square_root)\n    ) +\n    ggplot2::geom_density(\n        color = \"black\",\n        fill = \"grey\"\n    ) +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        x = \"Distance in square root of miles\",\n        y = \"Density\"\n    )\n\np_inverse <- dist_mat_clean |>\n    ggplot2::ggplot(\n        ggplot2::aes(x = inverse)\n    ) +\n    ggplot2::geom_density(\n        color = \"black\",\n        fill = \"grey\"\n    ) +\n    ggplot2::theme_bw() +\n    ggplot2::xlim(0, 1) +\n    ggplot2::labs(\n        x = \"Inverse of miles to nearest facility\",\n        y = \"Density\"\n    )\n\np_log <- dist_mat_clean |>\n    ggplot2::ggplot(\n        ggplot2::aes(x = log)\n    ) +\n    ggplot2::geom_density(\n        color = \"black\",\n        fill = \"grey\"\n    ) +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        x = \"Log of miles to nearest facility\",\n        y = \"Density\"\n    )\ngridExtra::grid.arrange(grobs = list(p_cube_root,\n                                     p_square_root,\n                                     p_inverse,\n                                     p_log),\n                        ncol = 2)\n```\n\n::: {.cell-output-display}\n![Distribution of the distance variable transformed by various factors](04-probability-distributions_files/figure-html/fig-dist-mat-transformed-1.png){#fig-dist-mat-transformed width=672}\n:::\n:::\n\n\n***\n\nThe best result of these transformation was with cube root.\n\nI tried to write a function for these four graphs, but it was not easy to pass the dataframe and column to the function. I finally succeeded with passing the column name as character string and using `[[` inside the function to select the column. (See [StackOverflow](https://stackoverflow.com/a/36015931/7322615)) But I gave up with `xlim()` parameter for the inverse transformation.\n::::\n:::::\n\n###### Mean and sd\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-mean-sd-distance-transformed}\n: Mean and standard deviation for cube root of mile transformation\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ndist_mat_clean |> \n    dplyr::summarize(mean = mean(cube_root),\n                  sd = sd(cube_root))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 2\n#>    mean    sd\n#>   <dbl> <dbl>\n#> 1  2.66 0.792\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n###### Probability distribution\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-prob-distance-dist}\n: Probability density function for a variable with a mean of 2.66 and a standard deviation of .79\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbase::set.seed(42)\nnormal_data <- tibble::tibble(stats::rnorm(\n    n = 1e3, \n    mean = 2.66, \n    sd = .79)) |> \n    dplyr::rename(data = 1) \n\nnormal_data |> \n    ggplot2::ggplot() +\n    ggplot2::aes(x = data,\n                 y = ggplot2::after_stat(count) / \n                     base::sum(count)\n                 ) +\n    ggplot2::geom_density() +\n    ggplot2::theme_bw() +\n    ggplot2::labs(x = 'Cube root of miles to the nearest facility with MAT',\n       y = 'Probability density')\n```\n\n::: {.cell-output-display}\n![Probability density function for a variable with a mean of 2.66 and a standard deviation of .79](04-probability-distributions_files/figure-html/fig-prob-distance-dist-1.png){#fig-prob-distance-dist width=672}\n:::\n:::\n\n***\n\nIn this plot I draw the probability density function with randomly generated data. The above curve will smooth out when I will take a bigger sample (for instance 1e5 instead 1e3).\n::::\n:::::\n\n###### with shaded area\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-prob-shaded}\n: Probability density function of the cube root transformation for 64 miles distance to a treatment facility\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_data |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = data)\n    ) +\n    ggplot2::stat_function(\n        fun = dnorm, \n        n = 1e3, \n        args = list(mean = 2.66, \n                    sd = .79),\n        linewidth = .5) +\n    ggplot2::geom_area(stat = 'function',\n            fun = dnorm,\n            fill = 'blue',\n            args = list(mean = 2.66, \n                    sd = .79),\n            xlim = c(4, 6),\n            alpha = 0.3) +\n    ggplot2::theme_bw() +\n    ggplot2::labs(x = 'Cube root of miles to the nearest facility with MAT',\n       y = 'Probability density')\n```\n\n::: {.cell-output-display}\n![Probability density function of the cube root transformation for 64 miles distance to a treatment facility](04-probability-distributions_files/figure-html/fig-chap04-prob-shaded-1.png){#fig-chap04-prob-shaded width=672}\n:::\n:::\n\n***\n\nFor this plot I have used the `dnorm()` function. Therefore this normal distribution curve is smooth.\n\nThe shaded area is the probability for counties that are $4^3 = 64$ miles from a facility that provides medical assisted treatment (MAT) \n::::\n:::::\n\n###### Compute shaded area\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-comp-shaded-area}\n: Compute shaded area: Percentage of counties where the nearest facility with MAT is 64 miles or more far away\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::pnorm(4, 2.66, .79, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.04492377\n```\n\n\n:::\n:::\n\n***\n\nIf you want to calculate the right part of the distribution then you need to change the default value from `lower.tail = TRUE` to `lower.tail = FALSE`.\n::::\n:::::\n\n4.49% of observations were in the shaded part of this distribution and therefore had a value for the distance variable of 4 or greater. Reversing the transformation, this indicated that residents of 4.49% of counties have to travel 43 or 64 miles or more to get to the nearest substance abuse facility providing medication-assisted treatment.\n\n:::\n\n::::\n:::::\n\n### Check understanding\n\nShows shading for the part of the distribution that is less than 2. Estimate (without computing the answer) the percentage of counties in the shaded area.\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\n:::::: {#exr-chap04-check-achievement3}\n: Achievement 3: Check understanding\n::::::\n:::\n::::{.my-exercise-container}\n\n::: {.panel-tabset}\n\n###### Show shaded graph\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-achievement3-graph}\n: Shows shading for the part of the distribution that is less than 2\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ndist_mat_clean |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = cube_root)\n    ) +\n    ggplot2::stat_function(\n            fun = dnorm, \n            n = 1e3, \n            args = list(mean = 2.66,\n                        sd = .79),\n            linewidth = .5) +\n    ggplot2::geom_area(stat = 'function',\n            fun = dnorm,\n            fill = 'blue',\n            args = list(mean = 2.66, \n                    sd = .79),\n            xlim = c(0, 2),\n            alpha = 0.3) +\n    ggplot2::theme_bw() +\n    ggplot2::labs(x = 'Cube root of miles to the nearest facility with MAT',\n       y = 'Probability density')\n```\n\n::: {.cell-output-display}\n![Probability density function for a variable with a mean of 2.66 and a standard deviation of .79 with the shaded area for counties that are 16 miles or less from the nearest facility with MAT](04-probability-distributions_files/figure-html/fig-chap04-achievement3-graph-1.png){#fig-chap04-achievement3-graph width=672}\n:::\n:::\n\n\n::::\n:::::\n\n###### Computation shaded area\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-achievement3-computation}\n: Compute area of the shading for the part of the distribution that is 8 miles or less from the nearest facility with MAT\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::pnorm(2, mean = 2.66, sd = .79)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.2017342\n```\n\n\n:::\n:::\n\n***\n\nAbout 20% of the counties are 8 miles or less from the nearest facility with MAT. My estimation of the shaded area without computation would have been much more (about 30-35%).\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n\n## Achievement 4: z-scores {#sec-chap04-achievement4}\n\n:::::{.my-important}\n:::{.my-important-header}\nValues of normally distributed variables\n:::\n::::{.my-important-container}\nRegardless of what the mean and standard deviation are, a normally distributed variable has approximately \n\n- 68% of values within one standard deviation of the mean \n- 95% of values within two standard deviations of the mean \n- 99.7% of values within three standard deviations of the mean\n\nThese characteristics of the normal distribution can be used to describe and compare how far individual observations are from a mean value.\n\n::::\n:::::\n\n### Defining z-score\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap04-z-score}\n: Z-Score formula\n::::::\n:::\n::::{.my-theorem-container}\n$$\nz_{i} = \\frac{x_{i} - m_{x}}{s_{x}}\n$$ {#eq-chap04-z-score}\n\nThe <a class='glossary' title='A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (StatisticsHowTo)'>z-score</a> for an observation is the number of standard deviations from the mean.\n\n::::\n:::::\n\n### z-score calculation & interpretation\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-calc-z-scores}\n: Calculation and interpretation of z-scores\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Example 1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-z-score1}\n: Z-score for a county with residents who have to travel 50 miles to the nearest facility\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncube_miles <- 50^(1/3)\nmean = 2.66\nsd = .79\n\n(cube_miles - mean) / sd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1.296242\n```\n\n\n:::\n:::\n\n***\n\nThis example county is farther than the mean away from the nearest facility with MAT.\n::::\n:::::\n\n\n###### Example 2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-code-name-b}\n: Z-score for a county with residents who have to travel 10 miles to the nearest facility\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncube_miles <- 10^(1/3)\nmean = 2.66\nsd = .79\n\n(cube_miles - mean) / sd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] -0.6399561\n```\n\n\n:::\n:::\n\n***\n\nThis example county is less than the mean away from the nearest facility with MAT.\n\n::::\n:::::\n\n###### Achievement 4\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-z-score3}\n: Z-score for a county where you have to drive 15 miles to the nearest facility with MAT.\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ncube_miles <- 15^(1/3)\nmean = 2.66\nsd = .79\n\n(cube_miles - mean) / sd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] -0.2453012\n```\n\n\n:::\n:::\n\n\nThis example county is less than the mean away from the nearest facility with MAT. (The mean of our transformed data is $2.66^3$ miles = 18.821096).\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n## Achievement 5: Estimating population means {#sec-chap04-achievement5}\n\n### Samples and populations\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-sample-and-population}\n: Estimating population means\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Summarize all\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-summarize-all-distances}\n: Summarize distances from the <a class='glossary' title='amfAR, the Foundation for AIDS Research, known until 2005 as the American Foundation for AIDS Research, is an international nonprofit organization dedicated to the support of AIDS research, HIV prevention, treatment education, and the advocacy of AIDS-related public policy. (Wikipedia)'>amfAR</a> database \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## load data with renamed `VALUE` column to `distance`\ndist_mat_clean2 <- base::readRDS(\"data/chap04/dist_mat_clean2.rds\")\n\ndist_mat_clean2 |> \n    dplyr::summarize(\n        mean_distance = base::mean(distance),\n        sd_distance = stats::sd(distance),\n        n = dplyr::n()\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 3\n#>   mean_distance sd_distance     n\n#>           <dbl>       <dbl> <int>\n#> 1          24.0        22.7  3214\n```\n\n\n:::\n:::\n\n\n***\n\nThese are the value for the population of (almost all) counties of the US (n = 3214). We are going now to get a sample of 500 counties to see how near we will come with the sample summaries to mean and sd of the population .\n::::\n:::::\n\n\n###### Summarize sample\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-summarize-sample-distances}\n: Draw a sample of 500 counties and compute the summaries\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(seed = 1945)\ndist_mat_clean2 |> \n    dplyr::slice_sample(n = 500, replace = TRUE) |> \n    dplyr::summarize(\n    mean_distance = base::mean(distance),\n    sd_distance = stats::sd(distance),\n    n = dplyr::n()\n    )\n\nset.seed(seed = 48)\ndist_mat_clean2 |> \n    dplyr::slice_sample(n = 500, replace = TRUE) |> \n    dplyr::summarize(\n    mean_distance = base::mean(distance),\n    sd_distance = stats::sd(distance),\n    n = dplyr::n()\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 3\n#>   mean_distance sd_distance     n\n#>           <dbl>       <dbl> <int>\n#> 1          24.4        23.8   500\n#> # A tibble: 1 × 3\n#>   mean_distance sd_distance     n\n#>           <dbl>       <dbl> <int>\n#> 1          23.5        20.1   500\n```\n\n\n:::\n:::\n\n\n***\n\nOne sample is somewhat higher, the other a little lower than the population mean.\n::::\n:::::\n\n###### Sample of 20 samples\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-sample-of-20-samples}\n: Examining a sample of 20 samples from a population\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## get 20 samples \n## each sample has 500 counties \n## put samples in a data frame with each sample \n## having a unique id called \"sample_num\"\n\nbase::set.seed(111)\ndist_mat_sample_20 <- \n    dplyr::bind_rows(\n        base::replicate(n = 20, dist_mat_clean2 |> \n                        dplyr::slice_sample(n = 500, replace = TRUE),\n                        simplify = FALSE),\n        .id = \"sample_num\")\n\n## find the mean for each sample \ndist_mat_sample_20_means <- dist_mat_sample_20 |> \n    dplyr::group_by(sample_num) |> \n    dplyr::summarize(\n        mean_distance = mean(x = distance, na.rm = TRUE))\n\ndist_mat_sample_20_means\n\n## find the mean of the 20 sample means\ndist_mat_sample_20_means |> \n    dplyr::summarize(mean_20_means = mean(mean_distance))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 20 × 2\n#>    sample_num mean_distance\n#>    <chr>              <dbl>\n#>  1 1                   24.2\n#>  2 10                  22.0\n#>  3 11                  23.9\n#>  4 12                  23.8\n#>  5 13                  23.1\n#>  6 14                  23.0\n#>  7 15                  22.6\n#>  8 16                  24.4\n#>  9 17                  24.4\n#> 10 18                  24.0\n#> 11 19                  23.7\n#> 12 2                   24.2\n#> 13 20                  23.1\n#> 14 3                   23.9\n#> 15 4                   24.4\n#> 16 5                   24.7\n#> 17 6                   22.8\n#> 18 7                   24.2\n#> 19 8                   23.9\n#> 20 9                   24.2\n#> # A tibble: 1 × 1\n#>   mean_20_means\n#>           <dbl>\n#> 1          23.7\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n###### Sample 100 samples\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-sample-100-samples}\n: Examining a sample of 100 samples from a population\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## get 100 samples \n## each sample has 500 counties \n## put samples in a data frame with each sample \n## having a unique id called \"sample_num\"\n\nbase::set.seed(143)\ndist_mat_sample_100 <- \n    dplyr::bind_rows(\n        base::replicate(n = 100, dist_mat_clean2 |> \n                        dplyr::slice_sample(n = 500, replace = TRUE),\n                        simplify = FALSE),\n        .id = \"sample_num\")\n\n## find the mean for each sample \ndist_mat_sample_100_means <- dist_mat_sample_100 |> \n    dplyr::group_by(sample_num) |> \n    dplyr::summarize(\n        mean_distance = mean(x = distance, na.rm = TRUE))\n\ndist_mat_sample_100_means\n\n## find the mean of the 100 sample means\ndist_mat_sample_100_means |> \n    dplyr::summarize(mean_100_means = mean(mean_distance))\n\ndist_mat_sample_100_means |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = mean_distance)\n    ) +\n    ggplot2::geom_histogram(\n        bins = 30,\n        color = \"black\",\n        fill = \"grey\") +\n    ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![](04-probability-distributions_files/figure-html/sample-100-samples-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 100 × 2\n#>    sample_num mean_distance\n#>    <chr>              <dbl>\n#>  1 1                   22.9\n#>  2 10                  23.3\n#>  3 100                 23.8\n#>  4 11                  23.2\n#>  5 12                  24.9\n#>  6 13                  23.8\n#>  7 14                  24.8\n#>  8 15                  23.5\n#>  9 16                  25.5\n#> 10 17                  24.1\n#> # ℹ 90 more rows\n#> # A tibble: 1 × 1\n#>   mean_100_means\n#>            <dbl>\n#> 1           24.0\n```\n\n\n:::\n:::\n\n\n\n***\n\nEven if the mean of the 100 sample means is already very near from the population value (23.9591026 versus 24.0427722, difference = -0.0836696) the sampling distribution is still far from a nice normal distribution. This will change if we are going to generate 1000 sample means.\n::::\n:::::\n\n###### Sample 1000 samples\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-sample-1000-distribution}\n: Plot the sample distribution of 1000 samples\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#lst-sample-distributions}\n\n::: {.cell}\n\n```{.r .cell-code}\n## get 1000 samples \n## each sample has 500 counties \n## put samples in a data frame with each sample \n## having a unique id called \"sample_num\"\n\nbase::set.seed(159)\ndist_mat_sample_1000 <- \n    dplyr::bind_rows(\n        base::replicate(n = 1000, dist_mat_clean2 |> \n                        dplyr::slice_sample(n = 500, replace = TRUE),\n                        simplify = FALSE),\n        .id = \"sample_num\")\n\n## find the mean for each sample \ndist_mat_sample_1000_means <- dist_mat_sample_1000 |> \n    dplyr::group_by(sample_num) |> \n    dplyr::summarize(\n        mean_distance = mean(x = distance, na.rm = TRUE))\n\ndist_mat_sample_1000_means\n\n## find the mean of the 100 sample means\ndist_mat_sample_1000_means |> \n    dplyr::summarize(mean_1000_means = mean(mean_distance))\n\ndist_mat_sample_1000_means |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = mean_distance)\n    ) +\n    ggplot2::geom_histogram(\n        bins = 30,\n        color = \"black\",\n        fill = \"grey\") +\n    ggplot2::theme_bw()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1,000 × 2\n#>    sample_num mean_distance\n#>    <chr>              <dbl>\n#>  1 1                   24.8\n#>  2 10                  24.1\n#>  3 100                 23.8\n#>  4 1000                24.5\n#>  5 101                 22.3\n#>  6 102                 24.4\n#>  7 103                 23.7\n#>  8 104                 24.2\n#>  9 105                 23.0\n#> 10 106                 24.6\n#> # ℹ 990 more rows\n#> # A tibble: 1 × 1\n#>   mean_1000_means\n#>             <dbl>\n#> 1            24.0\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Sample distribution of 1000 samples](04-probability-distributions_files/figure-html/fig-sample-1000-distribution-1.png){#fig-sample-1000-distribution width=672}\n:::\n:::\n\n\nSample distribution of 1000 samples\n:::\n\n***\n\nTaking a lot of large samples and graphing their means results in a <a class='glossary' title='A sampling distribution is the distribution of summary statistics, like means, from repeated samples taken from a population. (SwR, Glossary)'>sampling distribution</a> that looks like a normal distribution, and, more importantly, the mean of the sample means is nearly the same as the population mean (24.0360017 versus 24.0427722, difference = -0.0067706).\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n### Central Limit Theorem {#sec-chap04-clt}\n\nThe fact that the mean of the sample distribution of many samples approximates the population mean is called <a class='glossary' title='A foundational idea in inferential statistics that shows the mean of a sampling distribution of a variable will be a close approximation to the mean of the variable in the population, regardless of whether the variable is normally distributed. The Central Limit Theorem demonstrates why samples can be used to infer information about the population. (SwR, Glossary)'>Central Limit Theorem</a>. It holds true for continuous variables that both are and are not normally distributed.\n\nAnother characteristic of the Central Limit Theorem is that the standard deviation of the sample means can be estimated using the population standard deviation and the size of the samples that make up the distribution:\n\n$$\ns_{sample\\space distribution} = \\frac{\\sigma}{\\sqrt{n}}\n$$ {#eq-chap04-sd-sample-means}\n\nIf we want to calculate the standard deviation of the population we cannot use the `stats::sd()`. The reason is that `stats::sd()` uses the <a class='glossary' title='Bessel’s correction is the use of n − 1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance. It also partially corrects the bias in the estimation of the population standard deviation. However, the correction often increases the mean squared error in these estimations. This technique is named after Friedrich Bessel. (Wikipedia)'>Bessel’s correction</a> for samples which is not correct for the standard deviation of the population.\n\nInstead to apply the rather complex procedure in the book, I recommend to use the `sd_pop()` function from the {**sjstats**} package (see @pak-sjstats).\n\n### Standard deviation (sd)\n\nThe standard deviation of the sampling distribution shows how much we expect sample means to vary from the population mean.\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-estimated-sd}\n: Compute estimated standard deviation of the sampling distributions\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## compute parameters for population\ndist_mat_clean2  |>  \n    tidyr::drop_na(distance) |> # not necessary - no NAs\n    dplyr::summarize(n = dplyr::n(), \n                     pop.var = sjstats::var_pop(distance),\n                     pop.sd = sjstats::sd_pop(distance),\n                     samp_dist_est = pop.sd / base::sqrt(x = 500)\n    )\n\n## computing the sample dist standard deviation \n## directly from the 1000 sample means\n\nsd(x = dist_mat_sample_1000_means$mean_distance, \n   na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 4\n#>       n pop.var pop.sd samp_dist_est\n#>   <int>   <dbl>  <dbl>         <dbl>\n#> 1  3214    514.   22.7          1.01\n#> [1] 1.04966\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n### Standard error (se)\n\nSince it is unusual to have the entire population for computing the population <a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a>, and it is also unusual to have a large number of samples from one population, a close approximation to this value is called the <a class='glossary' title='The standard error (SE) of a statistic is the standard deviation of its [sampling distribution]. If the statistic is the sample mean, it is called the standard error of the mean (SEM). (Wikipedia) The standard error is a measure of variability that estimates how much variability there is in a population based on the variability in the sample and the size of the sample. (SwR, Glossary)'>standard error</a> of the mean (often referred to simply as the “standard error”). The standard error is computed by dividing the standard deviation of a variable by the square root of the sample size.\n\n$$\nse = \\frac{s}{\\sqrt{n}}\n$$ {#eq-chap04-se}\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-compute-se}\n: Compute standard error of the mean\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## mean, sd, se for first sample of 500 counties\nset.seed(seed = 1945)\ndist_mat_clean2 |> \n    dplyr::slice_sample(n = 500, replace = TRUE) |> \n    dplyr::summarize(\n    mean_distance = base::mean(distance),\n    sd_distance = stats::sd(distance),\n    se_distance = stats::sd(x = distance) /\n        base::sqrt(x = base::length(x = distance)),\n    n = dplyr::n()\n    )\n\nset.seed(seed = 48)\ndist_mat_clean2 |> \n    dplyr::slice_sample(n = 500, replace = TRUE) |> \n    dplyr::summarize(\n    mean_distance = base::mean(distance),\n    se_distance = stats::sd(x = distance) /\n        base::sqrt(x = base::length(x = distance)),\n    sd_distance = stats::sd(distance),\n    n = dplyr::n()\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 4\n#>   mean_distance sd_distance se_distance     n\n#>           <dbl>       <dbl>       <dbl> <int>\n#> 1          24.4        23.8        1.06   500\n#> # A tibble: 1 × 4\n#>   mean_distance se_distance sd_distance     n\n#>           <dbl>       <dbl>       <dbl> <int>\n#> 1          23.5       0.898        20.1   500\n```\n\n\n:::\n:::\n\n***\n\nBoth of the standard error (se) values are close to the sampling distribution standard deviation of 1.05, but they are not exactly the same. The first sample standard error of 1.06 was a little above and the second sample standard error of .90 was a little below.\n\n::::\n:::::\n\n::: {.callout-note #nte-chap04-se}\n**Summary**\n\n- The standard deviation of the sampling distribution is 1.05. \n- The standard error from the first sample is 1.06. \n- The standard error from the second sample is 0.90.\n\nMost of the time researchers have a single sample and so the only feasible way to determine the <a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a> of the <a class='glossary' title='A sampling distribution is the distribution of summary statistics, like means, from repeated samples taken from a population. (SwR, Glossary)'>sampling distribution</a> is by computing the <a class='glossary' title='The standard error (SE) of a statistic is the standard deviation of its [sampling distribution]. If the statistic is the sample mean, it is called the standard error of the mean (SEM). (Wikipedia) The standard error is a measure of variability that estimates how much variability there is in a population based on the variability in the sample and the size of the sample. (SwR, Glossary)'>standard error</a> of the single sample. This value tends to be a good estimate of the standard deviation of sample means.\n\n**The sample standard error is a good estimate of the sampling distribution standard deviation!**\n:::\n\n:::::{.my-important}\n:::{.my-important-header}\nDifference between standard deviation and standard error\n:::\n::::{.my-important-container}\n\nThe <a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a> is a measure of the variability in the sample, while the <a class='glossary' title='The standard error (SE) of a statistic is the standard deviation of its [sampling distribution]. If the statistic is the sample mean, it is called the standard error of the mean (SEM). (Wikipedia) The standard error is a measure of variability that estimates how much variability there is in a population based on the variability in the sample and the size of the sample. (SwR, Glossary)'>standard error</a> is an estimate of how closely the sample represents the population.\n::::\n:::::\n\n## Achievement 6: Confidence intervals {#sec-chap04-achievement6}\n\n### Introduction\n\n95% <a class='glossary' title='A range of values, calculated from the sample observations, that is believed, with a particular probability, to contain the true parameter value. (Cambridge Dictionary of Statistics, 4th ed., p.98)'>confidence interval</a> (CIs) show the range where the population value would likely be 95 times if the study were conducted 100 times.\n\n**The 95% interval idea summarized:**\n\n- About 95% of values lie within two <a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviations</a> of the mean for a variable that is normally distributed. \n- The <a class='glossary' title='The standard error (SE) of a statistic is the standard deviation of its [sampling distribution]. If the statistic is the sample mean, it is called the standard error of the mean (SEM). (Wikipedia) The standard error is a measure of variability that estimates how much variability there is in a population based on the variability in the sample and the size of the sample. (SwR, Glossary)'>standard error</a> of a sample is a good estimate of the standard deviation of the <a class='glossary' title='A sampling distribution is the distribution of summary statistics, like means, from repeated samples taken from a population. (SwR, Glossary)'>sampling distribution</a>, which is normally distributed. \n- The mean of the sampling distribution is a good estimate of the population mean. \n- So, most sample means will be within two standard errors (or more exact 1.96) of the population mean.\n- The number of standard deviations some observation is away from the mean is called a <a class='glossary' title='A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (StatisticsHowTo)'>z-score</a>.\n\nIn the following example I am going to replicate Figure 4.20 and 4.21 of the book. For these two graphs there exist no demonstration how to use R code to produce the figures in the book.\n\n### Working with 95% CIs\n\n#### Compute and plot stats: mean, sd, se and CIs\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-working-with.ci}\n: Working with 95% confidence intervals\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Compute 1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-compute-ci1}\n: Compute with a sample of 500 counties CI together with mean, sd, and se \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## get the population mean\ndist_mat_clean2 <- base::readRDS(\"data/chap04/dist_mat_clean2.rds\")\nmean_dist_pop <- dist_mat_clean2 |> \n    dplyr::summarize(mean_pop = mean(distance)) |> \n    dplyr::pull()\n\n## mean, sd, se and 95% ci for first sample of 500 counties\nset.seed(seed = 1945)\ndist_mat_sample1 <- dist_mat_clean2 |> \n    dplyr::slice_sample(n = 500, replace = TRUE)\n\ndist_mat_param_sample1 <-  dist_mat_sample1 |> \n    dplyr::summarize(\n    mean_distance = base::mean(distance),\n    sd_distance = stats::sd(distance),\n    se_distance = stats::sd(distance) /\n        base::sqrt(x = base::length(distance)),\n    lower_ci_distance = mean_distance - 1.96 * se_distance, \n    upper_ci_distance = mean_distance + 1.96 * se_distance\n    )\n```\n:::\n\n***\n\n::: {.callout-tip}\nThe mean distance in miles to the nearest substance abuse treatment facility with MAT in a sample of 500 counties is 24.4; the true or population mean distance in miles to a facility likely lies between 22.32 and 26.49 (m = 24.4; 95% CI = 22.32 – 26.49).\n:::\n\nIn this special case we have the population mean also available: So we can compare: The population mean = 24.04, e.g., it lies withing the 95 CI!\n\n::::\n:::::\n\n###### Compute 2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-compute-ci2}\n: Compute with another sample of 500 counties CI together with mean, sd, and se \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist_mat_clean2 <-  base::readRDS(\"data/chap04/dist_mat_clean2.rds\")\n\n## mean, sd, se and 95% ci for first sample of 500 counties\nset.seed(seed = 48)\ndist_mat_sample2 <- dist_mat_clean2 |> \n    dplyr::slice_sample(n = 500, replace = TRUE)\n\ndist_mat_param_sample2 <-  dist_mat_sample2 |> \n    dplyr::summarize(\n    mean_distance = base::mean(distance),\n    sd_distance = stats::sd(distance),\n    se_distance = stats::sd(distance) /\n        base::sqrt(x = base::length(distance)),\n    lower_ci_distance = mean_distance - 1.96 * se_distance, \n    upper_ci_distance = mean_distance + 1.96 * se_distance\n    )\n```\n:::\n\n\n***\n\n::: {.callout-tip}\nThe mean distance in miles to the nearest substance abuse treatment facility with MAT in a sample of 500 counties is 23.5; the true or population mean distance in miles to a facility likely lies between 21.74 and 25.26 (m = 23.5; 95% CI = 21.74 – 25.26).\n\n:::\n\nIn this special case we have the population mean also available: So we can compare: The population mean = 24.04, e.g., it lies withing the 95 CI!\n\n::::\n:::::\n\n\n###### Plot 1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-plot-ci1}\n: Plot CI of a sample 0f 500 counties compared with population mean\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## create data frame for the 4 different vertical lines\nvlines <- tibble::tibble(labels = c(\n        \"Lower CI\", \"Upper CI\", \n        \"Sample Mean\", \"Population Mean\"\n        ),\n    xintercepts = c(\n        dist_mat_param_sample1$lower_ci_distance,\n        dist_mat_param_sample1$upper_ci_distance,\n        dist_mat_param_sample1$mean_distance,\n        mean_dist_pop\n        ),\n    colors = c(\"coral\", \"blue4\", \"seagreen\", \"yellow\"),\n    linetypes = c(\"solid\", \"solid\", \"dotted\", \"dashed\" )\n     )\n\n## plot with scale & legend ##############\ndist_mat_sample1 |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = distance)\n        ) +\n    ggplot2::geom_histogram(\n        bins = 30,\n        fill = \"grey\",\n        color = \"black\"\n        ) + \n    \n    ## add all vertical lines via data frame\n    ggplot2::geom_vline(\n        data = vlines,\n        ggplot2::aes(\n            xintercept = xintercepts,\n            color = colors, # color order is alphabetically\n            linetype = linetypes)\n    ) +\n    \n    ## change / prepare legend\n    ggplot2::scale_color_identity(        \n        name = \"Parameter\",\n        labels = vlines$labels, \n        guide = \"legend\",\n        breaks = c(\"coral\", \"blue4\", \"seagreen\", \"yellow\")\n        ) +\n    \n    ## prevent second legend for line type\n    ## `guide = \"none\"` as default not necessary\n    ggplot2::scale_linetype_identity(guide = \"none\") + \n    ggplot2::theme_bw() +\n    \n    ## make legend bigger so that the lines are better visible\n    ## and position legend on top with a border around\n    ggplot2::guides(color = \n        ggplot2::guide_legend(override.aes = base::list(size = 8))) +\n    ggplot2::theme(\n        legend.position = \"top\",\n        legend.background = \n           ## color = legend border,\n           ## fill would be background, here not used\n           ggplot2::element_rect(color = \"black\")\n        ) +       \n    ggplot2::labs(\n        x = \"Distance in miles\",\n        y = \"Number of counties\"\n    )\n```\n\n::: {.cell-output-display}\n![Distribution of the distance to the nearest facility with MAT with a 95% cofindence interval and compared to the population mean](04-probability-distributions_files/figure-html/fig-plot-ci1-1.png){#fig-plot-ci1 width=672}\n:::\n:::\n\n***\nThe 95% interval is very small. The population mean is inside the sample CI, very near and therefore almost overlapping the sample mean.\n\n- The population mean = 24.0427722\n- The sample mean = 24.40444\n- The difference = -0.3616678\n\n::::\n:::::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nOrder of legend labels\n:::\n::::{.my-watch-out-container}\nThe standard for the order of the legend label is alphabetically. For instance I could manage my desired order with `colors = c(\"blue4\", \"coral\", \"seagreen\", \"yellow\")` in the `vlines` data frame.\n\nBut I wanted to learn how to re-order the colors if they are not sorted. So I have changed my color code to `colors = c(\"coral\", \"blue4\", \"seagreen\", \"yellow\")` in the `vlines` data frame. The first two colors are exchanged and not alphabetically sorted anymore.\n\nIt turned out that in this case I need to add the `breaks` argument with the correct order of colors inside my scale specification. So I have added `breaks = c(\"coral\", \"blue4\", \"seagreen\", \"yellow\")` into `ggplot2::scale_color_identity()`.\n\nBTW: `ggplot2::scale_color_identity()` was necessary because I have passed the color names to be used for the vertical lines directly from the data frame (and not via a color palette). Calling `scale_color_indentity()` tells ggplot2 that it doesn’t need to create a new color scale in that situation.\n\n\n::::\n:::::\n\n\n###### Plot 2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-plot-ci2}\n: Plot CI of another sample 0f 500 counties compared with population mean\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## plot with scale & legend ##############\ndist_mat_sample2 |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = distance)\n        ) +\n    ggplot2::geom_histogram(\n        bins = 30,\n        fill = \"grey\",\n        color = \"black\"\n        ) + \n    \n    ## add all vertical lines via data frame\n    ggplot2::geom_vline(\n        data = vlines,\n        ggplot2::aes(\n            xintercept = xintercepts,\n            color = colors, # color order is alphabetically\n            linetype = linetypes)\n    ) +\n    \n    ## change / prepare legend\n    ggplot2::scale_color_identity(        \n        name = \"Parameter\",\n        labels = vlines$labels, \n        guide = \"legend\",\n        breaks = c(\"coral\", \"blue4\", \"seagreen\", \"yellow\")\n        ) +\n    \n    ## prevent second legend for line type\n    ## `guide = \"none\"` as default not necessary\n    ggplot2::scale_linetype_identity(guide = \"none\") + \n    ggplot2::theme_bw() +\n    \n    ## make legend bigger so that the lines are better visible\n    ## and position legend on top with a border around\n    ggplot2::guides(color = \n        ggplot2::guide_legend(override.aes = base::list(size = 8))) +\n    ggplot2::theme(\n        legend.position = \"top\",\n        legend.background = \n           ## color = legend border,\n           ## fill would be background, here not used\n           ggplot2::element_rect(color = \"black\")\n        ) +       \n    ggplot2::labs(\n        x = \"Distance in miles\",\n        y = \"Number of counties\"\n    )\n```\n\n::: {.cell-output-display}\n![Distribution of the distance to the nearest facility with MAT with a 95% cofindence interval and compared to the population mean](04-probability-distributions_files/figure-html/fig-plot-ci2-1.png){#fig-plot-ci2 width=672}\n:::\n:::\n\n***\n\n\nThe 95% interval is very small. The population mean is inside the sample CI, very near and therefore almost overlapping the sample mean.\n\n- The population mean = 24.0427722\n- The sample mean = 23.49652\n- The difference = 0.5462522\n\n::::\n:::::\n\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nOrder of legend labels\n:::\n::::{.my-watch-out-container}\nThe standard for the order of the legend label is alphabetically. For instance I could manage my desired order with `colors = c(\"blue4\", \"coral\", \"seagreen\", \"yellow\")` in the `vlines` data frame.\n\nBut I wanted to learn how to re-order the colors if they are not sorted. So I have changed my color code to `colors = c(\"coral\", \"blue4\", \"seagreen\", \"yellow\")` in the `vlines` data frame. The first two colors are exchanged and not alphabetically sorted anymore.\n\nIt turned out that in this case I need to add the `breaks` argument with the correct order of colors inside my scale specification. So I have added `breaks = c(\"coral\", \"blue4\", \"seagreen\", \"yellow\")` into `ggplot2::scale_color_identity()`.\n\nBTW: `ggplot2::scale_color_identity()` was necessary because I have passed the color names to be used for the vertical lines directly from the data frame (and not via a color palette). Calling `scale_color_indentity()` tells ggplot2 that it doesn’t need to create a new color scale in that situation.\n\n::::\n:::::\n\n\n\n\n\n:::\n\n::::\n:::::\n\n#### Population mean & sample CIs for continuous variable\n\nWe have seen that the population mean of both samples is inside the 95% confidence intervals. But lets get more calculation and see if 5% of the samples --- per definition --- really fall outside the CIs.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-ci-samples-with-mean}\n: Check how often the population mean falls outside the sample CIs\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n20 Sample stats\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-sample-stats-20}\n: Means and 95% confidence intervals of 20 samples\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_stats_20 <- dist_mat_sample_20 |> \n    dplyr::group_by(sample_num) |> \n    dplyr::summarize(mean_20 = mean(distance),\n                     sd_20 = sd(distance),\n                     se_20 = sd_20 / \n                         base::sqrt(dplyr::n()),\n                     ci_lower_20 = mean_20 - 2 * se_20,\n                     ci_upper_20 = mean_20 + 2 * se_20\n                     )\nsample_stats_20\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 20 × 6\n#>    sample_num mean_20 sd_20 se_20 ci_lower_20 ci_upper_20\n#>    <chr>        <dbl> <dbl> <dbl>       <dbl>       <dbl>\n#>  1 1             24.2  24.5 1.09         22.0        26.4\n#>  2 10            22.0  17.1 0.765        20.4        23.5\n#>  3 11            23.9  20.0 0.895        22.1        25.7\n#>  4 12            23.8  20.7 0.924        21.9        25.6\n#>  5 13            23.1  20.8 0.931        21.2        24.9\n#>  6 14            23.0  22.3 0.999        21.0        25.0\n#>  7 15            22.6  19.7 0.880        20.8        24.3\n#>  8 16            24.4  21.9 0.979        22.4        26.4\n#>  9 17            24.4  21.6 0.965        22.5        26.4\n#> 10 18            24.0  22.9 1.02         21.9        26.0\n#> 11 19            23.7  21.4 0.958        21.8        25.7\n#> 12 2             24.2  19.6 0.878        22.4        25.9\n#> 13 20            23.1  21.4 0.957        21.2        25.0\n#> 14 3             23.9  22.4 1.00         21.9        25.9\n#> 15 4             24.4  21.7 0.971        22.5        26.4\n#> 16 5             24.7  22.6 1.01         22.7        26.7\n#> 17 6             22.8  19.1 0.853        21.1        24.5\n#> 18 7             24.2  23.5 1.05         22.1        26.3\n#> 19 8             23.9  21.9 0.978        22.0        25.9\n#> 20 9             24.2  22.4 1.00         22.2        26.2\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n###### 20 samples graph\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-ci-sample-stats-20}\n: Visualizing position of population mean in relation to 95% confidence intervals of 20 samples\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_stats_20 |> \n    dplyr::group_by(sample_num) |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = sample_num,\n                     y = mean_20\n        )\n    ) +\n    ggplot2::geom_errorbar(\n        ggplot2::aes(\n            ymin = ci_lower_20, \n            ymax = ci_upper_20,\n            linetype = \"95% CI\\nof sample mean\"\n        )\n    ) +\n    ggplot2::geom_point(\n        ggplot2::aes(\n            x = sample_num,\n            y = mean_20,\n            size = \"Sample mean\"\n        )\n    ) +\n    ggplot2::geom_hline(\n        ggplot2::aes(\n            yintercept = mean_dist_pop,\n            color = \"darkred\"\n        ),\n        linewidth = 1.5\n    ) +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        x = \"Sample\",\n        y = \"Mean distance to treatment facility (95% CI)\"\n    ) +\n    ggplot2::scale_color_discrete(\n        name = \"\",\n        labels = \"Population mean\"\n    ) +\n    ggplot2::scale_linetype_manual(\n        name = \"\",\n        values = c(1, NULL) \n    ) +\n    ggplot2::scale_size_manual(\n        name = \"\",\n        values = 4\n    ) +\n    ggplot2::theme(\n        legend.position = \"top\"\n    ) \n```\n\n::: {.cell-output-display}\n![Means and 95% confidence intervals of miles to the nearest substance abuse treatment facility with MAT from 20 samples of counties in the United States](04-probability-distributions_files/figure-html/fig-ci-sample-stats-20-1.png){#fig-ci-sample-stats-20 width=672}\n:::\n:::\n\n***\n\nOne confidence interval did not contain the population mean. This is 5% of 20 sample, which corresponds exactly to the definition of the 95% CI!\n::::\n:::::\n\n\n###### 100 samples graph\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-ci-sample-stats-100}\n: Visualizing position of population mean in relation to 95% confidence intervals of 100 samples\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_stats_100 <- dist_mat_sample_100 |> \n    dplyr::group_by(sample_num) |> \n    dplyr::summarize(mean_100 = mean(distance),\n                     sd_100 = sd(distance),\n                     se_100 = sd_100 / \n                         base::sqrt(dplyr::n()),\n                     ci_lower_100 = mean_100 - 2 * se_100,\n                     ci_upper_100 = mean_100 + 2 * se_100\n                     )\n\nsample_stats_100 |> \n    dplyr::group_by(sample_num) |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = sample_num,\n                     y = mean_100\n        )\n    ) +\n    ggplot2::geom_errorbar(\n        ggplot2::aes(\n            ymin = ci_lower_100, \n            ymax = ci_upper_100,\n            linetype = \"95% CI\\nof sample mean\"\n        )\n    ) +\n    ggplot2::geom_point(\n        ggplot2::aes(\n            x = sample_num,\n            y = mean_100,\n            size = \"Sample mean\"\n        )\n    ) +\n    ggplot2::geom_hline(\n        ggplot2::aes(\n            yintercept = mean_dist_pop,\n            color = \"darkred\"\n        ),\n        linewidth = .5\n    ) +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        x = \"Sample\",\n        y = \"Mean distance to treatment facility (95% CI)\"\n    ) +\n    ggplot2::scale_color_discrete(\n        name = \"\",\n        labels = \"Population mean\"\n    ) +\n    ggplot2::scale_linetype_manual(\n        name = \"\",\n        values = c(1, NULL) \n    ) +\n    ggplot2::scale_size_manual(\n        name = \"\",\n        labels = \"Sample mean\",\n        values = 1\n    ) +\n    ggplot2::scale_x_discrete(\n        breaks = NULL\n    ) +\n    ggplot2::theme(\n        legend.position = \"top\"\n    ) \n```\n\n::: {.cell-output-display}\n![Means and 95% confidence intervals of miles to the nearest substance abuse treatment facility with MAT from 100 samples of counties in the United States](04-probability-distributions_files/figure-html/fig-ci-sample-stats-100-1.png){#fig-ci-sample-stats-100 width=672}\n:::\n:::\n\n***\n\nThis time four confidence intervals that did contain the population mean. This is within the tolerance limit of the 95% CI: With 100 samples 5 would be allowed not to contain the population mean.\n\nForm the graph it is difficult to find those CIs that do not contain the population mean. In the next tab I am trying to colorize those intervals. \n\n::::\n:::::\n\n::: {.callout-note #nte-chap04-x-axis-removed}\nIn the book the scale of the x-axis was removed with `ggplot2::theme(axis.text.x = ggplot2::element_blank())`. I have used `ggplot2::scale_x_discrete(breaks = NULL)` with the same effect. \n:::\n\n###### 100 samples graph colored\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-ci-sample-stats-100-colored}\n: Visualizing position of population mean in relation to 95% confidence intervals of 100 samples, colorizing those CIs that do not include the population mean\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_stats_100 <- dist_mat_sample_100 |> \n    dplyr::group_by(sample_num) |> \n    dplyr::summarize(mean_100 = mean(distance),\n                     sd_100 = sd(distance),\n                     se_100 = sd_100 / \n                         base::sqrt(dplyr::n()),\n                     ci_lower_100 = mean_100 - 2 * se_100,\n                     ci_upper_100 = mean_100 + 2 * se_100\n                     )\n\nsample_stats_100 |> \n    dplyr::group_by(sample_num) |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = sample_num,\n                     y = mean_100\n        )\n    ) +\n    ggplot2::geom_errorbar(\n        ggplot2::aes(\n            ymin = ci_lower_100, \n            ymax = ci_upper_100,\n            linetype = \"95% CI\\nof sample mean\"\n        ),\n        color = dplyr::if_else(\n            mean_dist_pop >= sample_stats_100$ci_lower_100 & \n            mean_dist_pop <= sample_stats_100$ci_upper_100,\n            \"black\",\n            \"orange\"\n        )\n    ) +\n    ggplot2::geom_point(\n        ggplot2::aes(\n            x = sample_num,\n            y = mean_100,\n            size = \"Sample mean\"\n        )\n    ) +\n    ggplot2::geom_hline(\n        ggplot2::aes(\n            yintercept = mean_dist_pop,\n            color = \"darkred\"\n        ),\n        linewidth = .5\n    ) +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        x = \"Sample\",\n        y = \"Mean distance to treatment facility (95% CI)\"\n    ) +\n    ggplot2::scale_color_discrete(\n        name = \"\",\n        labels = \"Population mean\"\n    ) +\n    ggplot2::scale_linetype_manual(\n        name = \"\",\n        values = c(1, NULL) \n    ) +\n    ggplot2::scale_size_manual(\n        name = \"\",\n        labels = \"Sample mean\",\n        values = 1\n    ) +\n    ggplot2::scale_x_discrete(\n        breaks = NULL\n    ) +\n    ggplot2::theme(\n        legend.position = \"top\"\n    ) \n```\n\n::: {.cell-output-display}\n![Means and 95% confidence intervals of miles to the nearest substance abuse treatment facility with MAT from 100 samples of counties in the United States](04-probability-distributions_files/figure-html/fig-ci-sample-stats-100-colored-1.png){#fig-ci-sample-stats-100-colored width=672}\n:::\n:::\n\n***\n\nNow you can better see that 4 CIs do not include the population mean. The graph is a reproduction of book’s Figure 4.24 which is not accompanied with the appropriate R code.\n\n::::\n:::::\n\n::: {.callout-note-chap04-x-axis-removed2}\nIn the book the scale of the x-axis was removed with `ggplot2::theme(axis.text.x = ggplot2::element_blank())`. I have used `ggplot2::scale_x_discrete(breaks = NULL)` with the same effect. \n:::\n\n\n\n:::\n\n::::\n:::::\n\n#### Population mean & sample CIs for binomial variable\n\nGiven that the sampling distribution is normally distributed, 95% of sample means would be within two standard deviations of the mean of the means. This is also valid for binomial distributions, e.g. there are also confidence interval around the proportion of successes for a binary variable.\n\nBut there are two points to observe:\n\n1. For variables that have only two values (e.g., Yes and No, success and failure, 1 and 0), the mean of the variable is the same as the percentage of the group of interest. (The mean of a binary variable is typically abbreviated as `p` for proportion rather than `m` for mean.)\n2. For any given sample, then, the 95% confidence interval for the mean (which is the percentage in the category of interest) can be computed using the same formula of $m + (1.96 × se)$ and $m – (1.96 × se)$. **But there is a difference in the calculation of the standard error!**\n\n\n\n:::::{.my-remark}\n:::{.my-remark-header}\nHow to calculate the standard error for binomial distributions?\n:::\n::::{.my-remark-container}\nInstead of the formula for standard error for continuous variables (see @eq-chap04-se) the standard error for binomial distribution is:\n\n$$\n\\sqrt\\frac{p (1 - p)}{n}\n$$ {#eq-chap04-se-binomial}\n::::\n:::::\n\nThe essence of this difference: Instead of computing the CI via the <a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a> you have to calculate the CIs directly from mean and <a class='glossary' title='The standard error (SE) of a statistic is the standard deviation of its [sampling distribution]. If the statistic is the sample mean, it is called the standard error of the mean (SEM). (Wikipedia) The standard error is a measure of variability that estimates how much variability there is in a population based on the variability in the sample and the size of the sample. (SwR, Glossary)'>standard error</a>.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-ci-binomial}\n: Population mean & sample CIs for binomial variable\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Sample PDMP\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-prop-sample-pdmp}\n: Get 100 samples of 30 states for PDMPs\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\npdmp_2017_book_clean <- base::readRDS(\"data/chap04/pdmp_2017_book_clean.rds\")\n\n## find the mean of pdmp\npdmp_mean_2017 <- pdmp_2017_book_clean |> \n    dplyr::summarize(p = base::mean(PDMP)) |> \n    dplyr::pull()\n\n\n## get 100 samples: each sample has 30 states \n## put samples in a data frame with each sample having \n## a unique id called sample_num\n\nbase::set.seed(143)\npdmp_2017_book_samples <- \n    dplyr::bind_rows(\n        base::replicate(n = 100, \n            pdmp_2017_book_clean |> \n                        dplyr::slice_sample(n = 30, replace = TRUE),\n                        simplify = FALSE),\n        .id = \"sample_num\")  \n\n## find the mean for each sample\npdmp_2017_book_samples_states <-  pdmp_2017_book_samples |> \n    dplyr::group_by(sample_num) |> \n    dplyr::summarize(pdmp_p = base::mean(PDMP))\n    \npdmp_mean_2017 \npdmp_2017_book_samples_states\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.627451\n#> # A tibble: 100 × 2\n#>    sample_num pdmp_p\n#>    <chr>       <dbl>\n#>  1 1           0.6  \n#>  2 10          0.633\n#>  3 100         0.5  \n#>  4 11          0.633\n#>  5 12          0.467\n#>  6 13          0.567\n#>  7 14          0.767\n#>  8 15          0.433\n#>  9 16          0.6  \n#> 10 17          0.667\n#> # ℹ 90 more rows\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n###### Histogram samples PDMP \n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-graph-ci-pdmp}\n: Histogram of 100 samples of states with PDMPs (2017)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\npdmp_2017_book_samples_states |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = pdmp_p)\n    ) +\n    ggplot2::geom_histogram(\n        bins = 10,\n        color = \"black\",\n        fill = \"grey\") +\n    ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Histogram of 100 samples of states with PDMPs (2017)](04-probability-distributions_files/figure-html/fig-histo-bdmp-100-samples-1.png){#fig-histo-bdmp-100-samples width=672}\n:::\n:::\n\n***\n\nThe group looks normally distributed and it would even look more normally distributed with more samples. Given that the sampling distribution is normally distributed, 95% of sample means would be within 1.96 standard deviations of the mean of the means. \n\n::::\n:::::\n\n###### 100 samples binomial\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-ci-sample-binomial-100-colored}\n: Visualizing position of population mean in relation to 95% confidence intervals of 100 samples of the binomial distribution of the BDMPs, colorizing those CIs that do not include the population mean\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_binomial_100 <- pdmp_2017_book_samples |> \n    dplyr::group_by(sample_num) |>\n    dplyr::summarize(mean_100 = base::mean(PDMP),\n                     se_100 = base::sqrt(mean_100 * (1 - mean_100) / \n                              dplyr::n()),\n                     ci_lower_100 = mean_100 - 2 * se_100,\n                     ci_upper_100 = mean_100 + 2 * se_100\n                     )\n\nsample_binomial_100 |> \n    dplyr::group_by(sample_num) |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = sample_num,\n                     y = mean_100\n        )\n    ) +\n    ggplot2::geom_errorbar(\n        ggplot2::aes(\n            ymin = ci_lower_100, \n            ymax = ci_upper_100,\n            linetype = \"95% CI\\nof sample mean\"\n        ),\n        color = dplyr::if_else(\n            pdmp_mean_2017 >= sample_binomial_100$ci_lower_100 & \n            pdmp_mean_2017 <= sample_binomial_100$ci_upper_100,\n            \"black\",\n            \"orange\"\n        )\n    ) +\n    ggplot2::geom_point(\n        ggplot2::aes(\n            x = sample_num,\n            y = mean_100,\n            size = \"Sample mean\"\n        )\n    ) +\n    ggplot2::geom_hline(\n        ggplot2::aes(\n            yintercept = pdmp_mean_2017,\n            color = \"darkred\"\n        ),\n        linewidth = .5\n    ) +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        x = \"Sample\",\n        y = \"Mean distance to treatment facility (95% CI)\"\n    ) +\n    ggplot2::scale_color_discrete(\n        name = \"\",\n        labels = \"Population mean\"\n    ) +\n    ggplot2::scale_linetype_manual(\n        name = \"\",\n        values = c(1, NULL) \n    ) +\n    ggplot2::scale_size_manual(\n        name = \"\",\n        labels = \"Sample mean\",\n        values = 1\n    ) +\n    ggplot2::scale_x_discrete(\n        breaks = NULL\n    ) +\n    ggplot2::theme(\n        legend.position = \"top\"\n    ) \n```\n\n::: {.cell-output-display}\n![Mean and 95% CI for proportion of states with PDMPs from 100 samples of 30 states from a population where 62.75% of states have PDMPs](04-probability-distributions_files/figure-html/fig-ci-sample-binomial-100-colored-1.png){#fig-ci-sample-binomial-100-colored width=672}\n:::\n:::\n\n***\n\nWe see that 2 CIs do not include the population mean. The graph is a reproduction of book’s Figure 4.26 which is not accompanied with the appropriate R code.\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n#### Other confidence intervals\n\nThe three most common intervals have the following z-scores: \n\n***\n:::{#bul-three-cis}\n:::::{.my-bullet-list}\n:::{.my-bullet-list-header}\nBullet List\n:::\n::::{.my-bullet-list-container}\n\n- 90% confidence interval z-score = 1.645 \n- 95% confidence interval z-score = 1.96 \n- 99% confidence interval z-score = 2.576\n\n::::\n:::::\nThe three most common intervals with its z-scores\n:::\n***\n\nConfidence intervals for small samples, usually defined as samples with fewer than 30 observations [@field2012], use a <a class='glossary' title='The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (Statistics How-To)'>t-statistic</a> instead of a <a class='glossary' title='A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (StatisticsHowTo)'>z-score</a> in computing <a class='glossary' title='A range of values, calculated from the sample observations, that is believed, with a particular probability, to contain the true parameter value. (Cambridge Dictionary of Statistics, 4th ed., p.98)'>confidence interval</a> for means and in other types of analyses.\n\n> The t-statistic is from the t-distribution and, like the z-score, it measures the distance from the mean. However, the t-statistic does this using the standard deviation of the sampling distribution, also known as the standard error, rather than the standard deviation of the sample.\n\n$$\n\\begin{align*}\nt = \\frac{m}{\\frac{s}{\\sqrt{n}}} \\\\\nm = \\text{sample mean for a variable} \\\\\ns = \\text{sample standard deviation for the same variable} \\\\\nn = \\text{sample size} \\\\\nnote = \\text{the denominator for t is} \\frac{s}{\\sqrt(n)}\\text{This is the standard error!}\n\\end{align*}\n$$ {#eq-chap04-t-statistic}\n\nThe main practical difference between the two is that the t-statistic works better when samples are small; once samples are very large (n > 1,000), the two values will be virtually identical. (See @sec-chap06 for more about the t-statistics.)\n\n## Experiments\n\n### Get PDMP data\n\n\n\n:::::{.my-experiment}\n:::{.my-experiment-header}\n:::::: {#def-chap04-get-pdmp-data}\n: Get Prescription Drug Monitory Program (PMDP) data \n::::::\n:::\n::::{.my-experiment-container}\n\n::: {.panel-tabset}\n\n###### book\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-pdmp-book}\n: Get the cleaned PDMP data from the book `.csv` file\n::::::\n:::\n::::{.my-r-code-container}\n\n\n\n::: {.cell}\n\n```{#lst-chap04-pdmp-book .r .cell-code  lst-cap=\"Get the cleaned PDMP data from the book `.csv` file\"}\n## run code only once manually ##########\n\n## get pdmp data from books .csv\npdmp_2017_book <- readr::read_csv(\"data/chap04/pdmp_2017_kff_ch4.csv\")\nsave_data_file(\"chap04\", pdmp_2017_book, \"pdmp_2017_book.rds\")\n```\n:::\n\n\n\n***\n\n(*For this R code chunk is no output available*)\n\n::::\n:::::\n\n\n###### tabulizer\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-pdmp-tabulizer}\n: Get PDMP data with {**tabulizer**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n\n::: {.cell}\n\n```{#lst-chap04-pdmp-tabulizer .r .cell-code  lst-cap=\"Get PDMP data with {**tabulizer**}\"}\n## run only once (manually) ##########\n\n## get pdmp table via tabulizer\npdmp_2017_temp <- tabulizer::extract_tables(\n    \"data/chap04/PDMPs-2017.pdf\")\npdmp_2017_tabulizer <- pdmp_2017_temp[[1]]\n\nsave_data_file(\"chap04\", pdmp_2017_tabulizer, \"pdmp_2017_tabulizer.rds\")\n```\n:::\n\n\n***\n(*For this R code chunk is no output available*)\n\n\n::::\n:::::\n\n###### Clipboard\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-pdmp-clipboard}\n: Get PDMP data via clipboard\n::::::\n:::\n::::{.my-r-code-container}\n\n\n\n::: {.cell}\n\n```{#lst-chap04-pdmp-clipboard .r .cell-code  lst-cap=\"Get PDMP data via clipboard\"}\n## run code only once manually ###########\n\n## readr::read_delim(\"clipboard\") # Windows\n\npdmp_2017_clipboard1 <- readr::read_table(pipe(\"pbpaste\")) # normal copy & paste\npdmp_2017_clipboard2 <- readr::read_table(pipe(\"pbpaste\")) # TextSniper\nsave_data_file(\"chap04\", pdmp_2017_clipboard1, \"pdmp_2017_clipboard1.rds\")\nsave_data_file(\"chap04\", pdmp_2017_clipboard2, \"pdmp_2017_clipboard2.rds\")\n```\n:::\n\n\n***\n\n(*For this R code chunk is no output available*)\n\nWith this approach I have selected the table data and copied it into the clipboard. Be aware that here are two different functions for Windows and macOS.\n\n::::\n:::::\n\n\n###### rvest \n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-pdmp-rvest}\n: Get the PDMP data with {**rvest**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n\n::: {.cell}\n\n```{#lst-chap04-pdmp-rvest .r .cell-code  lst-cap=\"Get the PDMP data with {**rvest**}\"}\n## run code only once manually\n\n## 1. check if web scrapping is allowed\nurl <- paste0(\"https://www.kff.org/report-section/\",\n  \"implementing-coverage-and-payment-initiatives-benefits-and-pharmacy/\")\nrobotstxt::paths_allowed((url))\n\n## 2. get the whole KFF page\npdmp_2017_page <- rvest::read_html(url)\nsave_data_file(\"chap04\", pdmp_2017_page, \"pdmp_2017_page.rds\")\n\n## 3. extract PDMP table\npdmp_2017_rvest <- pdmp_2017_page |> \n    rvest::html_nodes(\"table\") |> \n    purrr::pluck(10) |> \n    rvest::html_table()\n\nsave_data_file(\"chap04\", pdmp_2017_rvest, \"pdmp_2017_rvest.rds\")\n```\n:::\n\n\n\n***\n\n(*For this R code chunk is no output available*)\n\n::::\n:::::\n\n\n\n:::\n\n::::\n:::::\n\nIn @def-chap04-get-pdmp-data I have data imported in four different ways: \n\n1. @lst-chap04-pdmp-book: This is the reference data frame, imported form the `.csv` file of the book.\n2. @lst-chap04-pdmp-tabulizer: The package {**tabulizer**} worked fine, but the PDF table 19 (a) did not separate several columns with a vertical line and (b) did not put entries if there was no PDMP in place, but left the place entry. {**tabulizer**} could therefore not detect which entries belong to which column.\n3. @lst-chap04-pdmp-clipboard: Even if the web page has dividing vertical lines for all columns, the same problem (namely empty) prevents a correct data transfer.\n4. @lst-chap04-pdmp-rvest: This is the best option of my experiment: After confirming that web scraping is allowed, I scrapped all tables from the <a class='glossary' title='Kaiser Family Foundation (KFF) is a non-partisan organization focused on health policy. It conducts its own research, polling, journalism, and specialized public health information campaigns and its website has been heralded for having the “most up-to-date and accurate information on health policy”[4] and as a “must-read for healthcare devotees.” (Wikipdia)'>KFF</a> web page, because there was no unique selector for table 19 available. I received 10 tables. That was strange because on the web page I could only visible detect seven tables. But luckily the last one was the table I was interested.\n\nConclusion: If I would not have the data to work with, I would take for further recoding the table the data imported by web scparing with {**rvest**}.\n\n\n\n\n## Exercises (empty)\n\n\n\n## Glossary\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> amfAR </td>\n   <td style=\"text-align:left;\"> amfAR, the Foundation for AIDS Research, known until 2005 as the American Foundation for AIDS Research, is an international nonprofit organization dedicated to the support of AIDS research, HIV prevention, treatment education, and the advocacy of AIDS-related public policy. (&lt;a href=\"https://en.wikipedia.org/wiki/AmfAR\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Bessel’s Correction </td>\n   <td style=\"text-align:left;\"> Bessel's correction is the use of n − 1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance. It also partially corrects the bias in the estimation of the population standard deviation. However, the correction often increases the mean squared error in these estimations. This technique is named after Friedrich Bessel. (&lt;a href=\"https://en.wikipedia.org/wiki/Bessel%27s_correction\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> CDC </td>\n   <td style=\"text-align:left;\"> Centers for Disease Control and Preventation (CDC) is the U.S. leading science-based, data-driven, service organization that protects the public’s health. (&lt;a href=\"https://www.cdc.gov/about/\"&gt;CDC&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Central Limit Theorem </td>\n   <td style=\"text-align:left;\"> A foundational idea in inferential statistics that shows the mean of a sampling distribution of a variable will be a close approximation to the mean of the variable in the population, regardless of whether the variable is normally distributed. The Central Limit Theorem demonstrates why samples can be used to infer information about the population. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Confidence Interval </td>\n   <td style=\"text-align:left;\"> A range of values, calculated from the sample observations, that is believed, with a particular probability, to contain the true parameter value. (Cambridge Dictionary of Statistics, 4th ed., p.98) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Cumulative Distribution Function </td>\n   <td style=\"text-align:left;\"> A cumulative distribution function (CDF) tells us the probability that a random variable takes on a value less than or equal to x. (&lt;a href=\"https://www.statology.org/cdf-vs-pdf/\"&gt;Statology&lt;/a&gt;) It sums all parts of the distribution, replacing a lot of calculus work. The CDF takes in a value and returns the probability of getting that value or lower. (BF, Chap.13) A CDF is a hypothetical model of a distribution, the ECDF models empirical (i.e. observed) data. (&lt;a href=\"https://www.statisticshowto.com/empirical-distribution-function/\"&gt;Statistics How To&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> KFF </td>\n   <td style=\"text-align:left;\"> Kaiser Family Foundation (KFF) is a non-partisan organization focused on health policy. It conducts its own research, polling, journalism, and specialized public health information campaigns and its website has been heralded for having the \"most up-to-date and accurate information on health policy\"[4] and as a \"must-read for healthcare devotees.\" (&lt;a href=\"https://en.wikipedia.org/wiki/Kaiser_Family_Foundation\"&gt;Wikipdia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PDMP </td>\n   <td style=\"text-align:left;\"> In the United States, prescription monitoring programs (PMPs) or prescription drug monitoring programs (PDMPs) are state-run programs which collect and distribute data about the prescription and dispensation of federally controlled substances and, depending on state requirements, other potentially abusable prescription drugs. PMPs are meant to help prevent adverse drug-related events such as opioid overdoses, drug diversion, and substance abuse by decreasing the amount and/or frequency of opioid prescribing, and by identifying those patients who are obtaining prescriptions from multiple providers (i.e., \"doctor shopping\") or those physicians overprescribing opioids. (&lt;a href=\"https://en.wikipedia.org/wiki/Prescription_monitoring_program\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Probability Distribution </td>\n   <td style=\"text-align:left;\"> It is a way of describing all possible events and the probability of each one happening. Probability distributions are also very useful for asking questions about ranges of possible values. (BF, Chap.4) The two defining features are: (1) All values of the distribution must be real and non-negative. (2) The sum (for discrete random variables) or integral (for continuous random variables) across all possible values of the random variable must be 1. (BS, Chap.3) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Sampling Distribution </td>\n   <td style=\"text-align:left;\"> A sampling distribution is the distribution of summary statistics, like means, from repeated samples taken from a population. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standard Deviation </td>\n   <td style=\"text-align:left;\"> The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter $\\sigma$ (sigma), for the population standard deviation, or the Latin letter $s$ for the sample standard deviation. ([Wikipedia] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standard Error </td>\n   <td style=\"text-align:left;\"> The standard error (SE) of a statistic is the standard deviation of its [sampling distribution]. If the statistic is the sample mean, it is called the standard error of the mean (SEM). (&lt;a href=\"https://en.wikipedia.org/wiki/Standard_error\"&gt;Wikipedia&lt;/a&gt;) The standard error is a measure of variability that estimates how much variability there is in a population based on the variability in the sample and the size of the sample. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> T-Statistic </td>\n   <td style=\"text-align:left;\"> The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (&lt;a href=\"https://www.statisticshowto.com/t-statistic/\"&gt;Statistics How-To&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Z-score </td>\n   <td style=\"text-align:left;\"> A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (&lt;a href=\"https://www.statisticshowto.com/probability-and-statistics/z-score/#Whatisazscore\"&gt;StatisticsHowTo&lt;/a&gt;) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n## Session Info {.unnumbered}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\nSession Info\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.3.3 (2024-02-29)\n#>  os       macOS Sonoma 14.4.1\n#>  system   x86_64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2024-04-13\n#>  pandoc   3.1.12.3 @ /usr/local/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package      * version    date (UTC) lib source\n#>  backports      1.4.1      2021-12-13 [1] CRAN (R 4.3.0)\n#>  bayestestR     0.13.2     2024-02-12 [1] CRAN (R 4.3.2)\n#>  boot           1.3-30     2024-02-26 [2] CRAN (R 4.3.2)\n#>  broom          1.0.5      2023-06-09 [1] CRAN (R 4.3.0)\n#>  cli            3.6.2      2023-12-11 [1] CRAN (R 4.3.0)\n#>  coda           0.19-4.1   2024-01-31 [1] CRAN (R 4.3.2)\n#>  codetools      0.2-20     2024-03-31 [1] CRAN (R 4.3.3)\n#>  colorspace     2.1-1      2024-01-03 [1] R-Forge (R 4.3.2)\n#>  commonmark     1.9.1      2024-01-30 [1] CRAN (R 4.3.2)\n#>  crayon         1.5.2      2022-09-29 [1] CRAN (R 4.3.0)\n#>  curl           5.2.1      2024-03-01 [1] CRAN (R 4.3.2)\n#>  digest         0.6.35     2024-03-11 [1] CRAN (R 4.3.2)\n#>  dplyr          1.1.4      2023-11-17 [1] CRAN (R 4.3.0)\n#>  emmeans        1.10.0     2024-01-23 [1] CRAN (R 4.3.2)\n#>  estimability   1.5        2024-02-20 [1] CRAN (R 4.3.2)\n#>  evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.0)\n#>  fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.0)\n#>  farver         2.1.1      2022-07-06 [1] CRAN (R 4.3.0)\n#>  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#>  forcats        1.0.0      2023-01-29 [1] CRAN (R 4.3.0)\n#>  generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#>  ggplot2        3.5.0      2024-02-23 [1] CRAN (R 4.3.2)\n#>  glossary     * 1.0.0.9000 2023-08-12 [1] Github (debruine/glossary@819e329)\n#>  glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.0)\n#>  gtable         0.3.4      2023-08-21 [1] CRAN (R 4.3.0)\n#>  here           1.0.1      2020-12-13 [1] CRAN (R 4.3.0)\n#>  highr          0.10       2022-12-22 [1] CRAN (R 4.3.0)\n#>  htmltools      0.5.8      2024-03-25 [1] CRAN (R 4.3.2)\n#>  htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.3.0)\n#>  insight        0.19.10    2024-03-22 [1] CRAN (R 4.3.3)\n#>  jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.0)\n#>  kableExtra     1.4.0      2024-01-24 [1] CRAN (R 4.3.2)\n#>  knitr          1.45       2023-10-30 [1] CRAN (R 4.3.0)\n#>  labeling       0.4.3      2023-08-29 [1] CRAN (R 4.3.0)\n#>  lattice        0.22-6     2024-03-20 [2] CRAN (R 4.3.2)\n#>  lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.3.0)\n#>  lme4           1.1-35.2   2024-03-28 [1] CRAN (R 4.3.2)\n#>  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#>  markdown       1.12       2023-12-06 [1] CRAN (R 4.3.0)\n#>  MASS           7.3-60.0.1 2024-01-13 [2] CRAN (R 4.3.3)\n#>  Matrix         1.6-5      2024-01-11 [1] CRAN (R 4.3.0)\n#>  minqa          1.2.6      2023-09-11 [1] CRAN (R 4.3.0)\n#>  modelr         0.1.11     2023-03-22 [1] CRAN (R 4.3.0)\n#>  multcomp       1.4-25     2023-06-20 [1] CRAN (R 4.3.0)\n#>  munsell        0.5.0      2018-06-12 [1] CRAN (R 4.3.0)\n#>  mvtnorm        1.2-4      2023-11-27 [1] CRAN (R 4.3.2)\n#>  nlme           3.1-164    2023-11-27 [1] CRAN (R 4.3.2)\n#>  nloptr         2.0.3      2022-05-26 [1] CRAN (R 4.3.0)\n#>  performance    0.11.0     2024-03-22 [1] CRAN (R 4.3.2)\n#>  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#>  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#>  purrr          1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n#>  R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#>  Rcpp           1.0.12     2024-01-09 [1] CRAN (R 4.3.0)\n#>  rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.0)\n#>  rmarkdown      2.26       2024-03-05 [1] CRAN (R 4.3.2)\n#>  rprojroot      2.0.4      2023-11-05 [1] CRAN (R 4.3.0)\n#>  rstudioapi     0.16.0     2024-03-24 [1] CRAN (R 4.3.2)\n#>  rversions      2.1.2      2022-08-31 [1] CRAN (R 4.3.0)\n#>  sandwich       3.1-0      2023-12-11 [1] CRAN (R 4.3.0)\n#>  scales         1.3.0      2023-11-28 [1] CRAN (R 4.3.2)\n#>  sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n#>  sjlabelled     1.2.0      2022-04-10 [1] CRAN (R 4.3.0)\n#>  sjmisc         2.8.9      2021-12-03 [1] CRAN (R 4.3.0)\n#>  sjstats        0.18.2     2022-11-19 [1] CRAN (R 4.3.0)\n#>  stringi        1.8.3      2023-12-11 [1] CRAN (R 4.3.0)\n#>  stringr        1.5.1      2023-11-14 [1] CRAN (R 4.3.0)\n#>  survival       3.5-8      2024-02-14 [2] CRAN (R 4.3.3)\n#>  svglite        2.1.3      2023-12-08 [1] CRAN (R 4.3.0)\n#>  systemfonts    1.0.6      2024-03-07 [1] CRAN (R 4.3.2)\n#>  TH.data        1.1-2      2023-04-17 [1] CRAN (R 4.3.0)\n#>  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#>  tidyr          1.3.1      2024-01-24 [1] CRAN (R 4.3.2)\n#>  tidyselect     1.2.1      2024-03-11 [1] CRAN (R 4.3.2)\n#>  utf8           1.2.4      2023-10-22 [1] CRAN (R 4.3.0)\n#>  vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n#>  viridisLite    0.4.2      2023-05-02 [1] CRAN (R 4.3.0)\n#>  withr          3.0.0      2024-01-16 [1] CRAN (R 4.3.0)\n#>  xfun           0.43       2024-03-25 [1] CRAN (R 4.3.2)\n#>  xml2           1.3.6      2023-12-04 [1] CRAN (R 4.3.0)\n#>  xtable         1.8-4      2019-04-21 [1] CRAN (R 4.3.0)\n#>  yaml           2.3.8      2023-12-11 [1] CRAN (R 4.3.0)\n#>  zoo            1.8-12     2023-04-13 [1] CRAN (R 4.3.0)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.3-x86_64/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n\n\n::::\n:::::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}