{
  "hash": "8e64c74dcd3e492b7c007f2a9546d008",
  "result": {
    "engine": "knitr",
    "markdown": "# Linear regression {#sec-chap09}\n\n\n\n\n\n::: {.callout-note #nte-chap09-differenct-chapter-structure style=\"color: blue;\"}\n### Different chapter structure, especially in <a class='glossary' title='Explorative Data Analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. (Wikipedia)'>EDA</a> (@sec-chap09-achievement1)\n\nAs I have already read some books on linear regression I will not follow exactly the text in this chapter. I will leave out those passages that are not new for me and where I feel confident. Other passages I will only summarize to have content to consult whenever I would need it.\n:::\n\n\n\n\n\n## Achievements to unlock\n\n\n::: {#obj-chap09}\n::: {.my-objectives}\n::: {.my-objectives-header}\nObjectives for chapter 09\n:::\n\n::: {.my-objectives-container}\n**SwR Achievements**\n\n- **Achievement 1**: Using exploratory data analysis to learn about the data before developing a linear regression model (@sec-chap09-achievement1)\n- **Achievement 2**: Exploring the statistical model for a line (@sec-chap09-achievement2)\n- **Achievement 3**: Computing the slope and intercept in a simple linear regression (@sec-chap09-achievement3)\n- **Achievement 4**: Slope interpretation and significance ($b_{1}$, p-value, CI) (@sec-chap09-achievement4)\n- **Achievement 5**: Model significance and model fit (@sec-chap09-achievement5)\n- **Achievement 6**: Checking assumptions and conducting diagnostics (@sec-chap09-achievement6)\n- **Achievement 7**: Adding variables to the model and using transformation (@sec-chap09-achievement7)\n\n:::\n:::\n\nAchievements for chapter 09\n:::\n\n\n## The needle exchange examination\n\nSome infectious diseases like HIV and Hepatitis C are on the rise again with young people in non-urban areas having the highest increases and needle-sharing being a major factor. Clean needles are distributed by syringe services programs (SSPs), which can also provide a number of other related services including overdose prevention, referrals for substance use treatment, and infectious disease testing. But even there are programs in place --- which is not allowed legally in some US states! --- some people have to travel long distances for health services, especially for services that are specialized, such as needle exchanges.\n\nIn discussing the possible quenstion one could analyse it turned out that for some questions critical data are missing: \n\n- There is a distance-to-syringe-services-program variable among the health services data sources of <a class='glossary' title='amfAR, the Foundation for AIDS Research, known until 2005 as the American Foundation for AIDS Research, is an international nonprofit organization dedicated to the support of AIDS research, HIV prevention, treatment education, and the advocacy of AIDS-related public policy. (Wikipedia)'>amfAR</a> (https://opioid.amfar.org/). \n- Many of the interesting variables were not available for much of the nation, and many of them were only at the state level.\n\nGiven these limitations, the book focuses whether the distance to a syringe program could be explained by \n\n- whether a county is urban or rural, \n- what percentage of the county residents have insurance (as a measure of both access to health care and socioeconomic status [SES]), \n- HIV prevalence, \n- and the number of people with opioid prescriptions.\n\nAs there is no variable for rural or urban status in the amfAR database, the book will tale a variable from the U.S. Department of Agriculture Economic Research Services website (https://www.ers.usda.gov/data-products/county-typology-codes/) that classifies all counties as metro or non-metro.\n\n## Resources & Chapter Outline\n\n### Data, codebook, and R packages {#sec-chap04-data-codebook-packages}\n\n::: {.my-resource}\n::: {.my-resource-header}\n:::::: {#lem-chap09-resources}\n: Data, codebook, and R packages for learning about descriptive statistics\n::::::\n:::\n\n::: {.my-resource-container}\n\n**Data**\n\nTwo options for accessing the data:\n\n1. Download the clean data set `dist_ssp_amfar_ch9.csv` from https://edge.sagepub.com/harris1e.\n2. Follow the instructions in Box 9.1 to import, merge, and clean the data from multiple files or from the original online source \n\n\n\n**Codebook**\n\nTwo options for accessing the codebook: \n\n1. Download the codebook file `opioid_county_codebook.xlsx` from https://edge.sagepub.com/harris1e.\n2. Use the online codebook from the amfAR Opioid & Health Indicators Database website (https://opioid.amfar.org/)\n\n\n**Packages**\n\n1. Packages used with the book (sorted alphabetically)\n\n-   {**tidyverse**}: @pak-tidyverse (Hadley Wickham)\n-   {**tableone**}: @pak-tableone (Kazuki Yoshida) \n-   {**lmtest**}: @pak-lmtest (Achim Zeileis) \n-   {**broom**}, @pak-broom (David Robinson and Alex Hayes)\n-   {**car**}, @pak-car (John Fox)\n\n    \n2. My additional packages (sorted alphabetically)\n\n- {**gt**}: @pak-gt (Richard Iannone)\n- {**gtsummary**}: @pak-gtsummary (Daniel D. Sjoberg)\n\n:::\n:::\n\n\n### Get, recode and show data\n\nI will use the data file provided by the book because I am feeling quite confident with reading and recoding the original data. But I will change the columns names so that the variable names conform to the [tidyverse style guide](https://style.tidyverse.org/).\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap09-data}\n: Data for chapter 9\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Get & recode\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-get-recode-data}\n: Get & recode data for chapter 9\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## run only once (manually)\ndistance_ssp <- readr::read_csv(\n    \"data/chap09/dist_ssp_amfar_ch9.csv\",\n    show_col_types = FALSE)\n\ndistance_ssp_clean <- distance_ssp |> \n    dplyr::rename(\n        state = \"STATEABBREVIATION\",\n        dist_ssp = \"dist_SSP\",\n        hiv_prevalence = \"HIVprevalence\",\n        opioid_rate = \"opioid_RxRate\",\n        no_insurance = \"pctunins\"\n    ) |> \n    dplyr::mutate(\n        state = forcats::as_factor(state),\n        metro = forcats::as_factor(metro)\n    ) |> \n    dplyr::mutate(\n        hiv_prevalence = dplyr::na_if(\n            x = hiv_prevalence,\n            y = -1\n        )\n    )\n\nsave_data_file(\"chap09\", distance_ssp_clean, \"distance_ssp_clean.rds\")\n```\n:::\n\n\n(*For this R code chunk is no output available*)\n\n::::\n:::::\n\n\n###### Show data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-show-data}\n: Show data for chapter 9\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {#tbl-chap09-show-data .cell tbl-cap='Descriptive statistics for data of chapter 9'}\n\n```{.r .cell-code}\ndistance_ssp_clean <- base::readRDS(\"data/chap09/distance_ssp_clean.rds\")\n\nskimr::skim(distance_ssp_clean)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |                   |\n|:------------------------|:------------------|\n|Name                     |distance_ssp_clean |\n|Number of rows           |500                |\n|Number of columns        |7                  |\n|_______________________  |                   |\n|Column type frequency:   |                   |\n|character                |1                  |\n|factor                   |2                  |\n|numeric                  |4                  |\n|________________________ |                   |\n|Group variables          |None               |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|county        |         0|             1|  10|  28|     0|      406|          0|\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                     |\n|:-------------|---------:|-------------:|:-------|--------:|:------------------------------|\n|state         |         0|             1|FALSE   |       49|TX: 50, GA: 30, NC: 21, TN: 21 |\n|metro         |         0|             1|FALSE   |        2|non: 274, met: 226             |\n\n\n**Variable type: numeric**\n\n|skim_variable  | n_missing| complete_rate|   mean|     sd|   p0|   p25|    p50|    p75|   p100|hist  |\n|:--------------|---------:|-------------:|------:|------:|----:|-----:|------:|------:|------:|:-----|\n|dist_ssp       |         0|          1.00| 107.74|  94.23|  0.0| 35.12|  75.94| 163.83|  510.0|▇▃▂▁▁ |\n|hiv_prevalence |        70|          0.86| 192.89| 213.35| 14.4| 72.30| 118.95| 227.48| 2150.7|▇▁▁▁▁ |\n|opioid_rate    |         0|          1.00|  68.33|  36.81|  0.2| 45.12|  62.40|  89.95|  345.1|▇▆▁▁▁ |\n|no_insurance   |         0|          1.00|  12.18|   4.97|  3.0|  8.60|  11.70|  15.00|   35.9|▅▇▂▁▁ |\n\n\n:::\n:::\n\n\n::::\n:::::\n\n***\n\n::: {#bul-chap09-codebook}\n:::::{.my-bullet-list}\n:::{.my-bullet-list-header}\nBullet List\n:::\n::::{.my-bullet-list-container}\n- **county**: the county name \n- **state**: the two-letter abbreviation for the state the county is in \n- **dist_ssp**: distance in miles to the nearest syringe services program \n- **hiv_prevalence**: people age 13 and older living with diagnosed HIV per 100,000\n- **opioid_rate**: number of opioid prescriptions per 100 people \n- **no_insurance**:percentage of the civilian non-institutionalized population with no health insurance coverage \n- **metro**: county is either non-metro, which includes open countryside, rural towns, or smaller cities with up to 49,999 people, or metro\n::::\n:::::\nCodebook: Explanation of variables used in Chapter 9\n:::\n\n***\n\n###### metro\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-metro}\n: Summary of `metro` variable\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#lst-chap09-metro}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistance_ssp_clean |> \n    dplyr::group_by(metro) |> \n    skimr::skim()\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |                             |\n|:------------------------|:----------------------------|\n|Name                     |dplyr::group_by(distance_... |\n|Number of rows           |500                          |\n|Number of columns        |7                            |\n|_______________________  |                             |\n|Column type frequency:   |                             |\n|character                |1                            |\n|factor                   |1                            |\n|numeric                  |4                            |\n|________________________ |                             |\n|Group variables          |metro                        |\n\n\n**Variable type: character**\n\n|skim_variable |metro     | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|:---------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|county        |metro     |         0|             1|  10|  28|     0|      201|          0|\n|county        |non-metro |         0|             1|  10|  25|     0|      241|          0|\n\n\n**Variable type: factor**\n\n|skim_variable |metro     | n_missing| complete_rate|ordered | n_unique|top_counts                     |\n|:-------------|:---------|---------:|-------------:|:-------|--------:|:------------------------------|\n|state         |metro     |         0|             1|FALSE   |       46|TX: 20, GA: 15, VA: 12, NC: 11 |\n|state         |non-metro |         0|             1|FALSE   |       42|TX: 30, KS: 18, GA: 15, TN: 13 |\n\n\n**Variable type: numeric**\n\n|skim_variable  |metro     | n_missing| complete_rate|   mean|     sd|   p0|   p25|    p50|    p75|   p100|hist  |\n|:--------------|:---------|---------:|-------------:|------:|------:|----:|-----:|------:|------:|------:|:-----|\n|dist_ssp       |metro     |         0|          1.00|  85.00|  88.00|  0.0| 20.31|  50.37| 123.96|  436.0|▇▂▁▁▁ |\n|dist_ssp       |non-metro |         0|          1.00| 126.50|  95.21|  6.0| 50.61|  96.05| 181.96|  510.0|▇▅▂▁▁ |\n|hiv_prevalence |metro     |        18|          0.92| 245.59| 262.55| 14.4| 89.30| 162.60| 316.35| 2150.7|▇▁▁▁▁ |\n|hiv_prevalence |non-metro |        52|          0.81| 143.51| 136.86| 22.6| 63.65|  96.60| 159.18|  904.3|▇▂▁▁▁ |\n|opioid_rate    |metro     |         0|          1.00|  65.44|  28.65|  4.7| 45.55|  60.10|  87.43|  143.6|▂▇▅▃▁ |\n|opioid_rate    |non-metro |         0|          1.00|  70.71|  42.28|  0.2| 44.60|  65.60|  91.97|  345.1|▇▆▁▁▁ |\n|no_insurance   |metro     |         0|          1.00|  11.17|   4.42|  3.0|  8.22|  10.80|  14.05|   30.2|▅▇▅▁▁ |\n|no_insurance   |non-metro |         0|          1.00|  13.02|   5.25|  3.3|  9.17|  12.15|  15.88|   35.9|▅▇▃▁▁ |\n\n\n:::\n:::\n\nSummary of `metro` variable\n:::\n\n***\n\nFor the exploratory data analysis I need more details about the association between the distance to the next SSP separated for people living in metro and non-metro areas. See \n\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Do missing values have a pattern?\n:::\n::::{.my-watch-out-container}\nWe know from @tbl-chap09-show-data that the variable `hiv_prevalence` has many missing values. In all the forthcoming analyses we will remove those 70 `NAs` and work with complete cases. 70 NA’s in a sample of 500 is with 14% a big proportion from the available data. The question arises: Is there a reason why there are so many missing values? Could it be that this reason is distorting our analysis?\n\nMost of the time I have provided code that suppresses these warnings. This is a dangerous enterprise as it could bias results and conclusions without knowledge of the researcher. I think that a more prudent approach would need an analysis of the missing values. I do not know how to do this yet, but with {**naniar**} (@pak-naniar) there is a package for exploring missing data structures. Its website and package has [several vignettes](https://naniar.njtierney.com/) to learn its functions and there is also an scientific article about the package [@tierney2023].\n\nExploring missing data structures is in the book no planned achievement, therefore it is here enough to to get rid of the NA’s and to follow the books outline. But I am planning coming back to this issue and learn how to address missing data structures appropriately.\n\n::::\n:::::\n\n\n## Achievement 1: Explorative Data Analysis {#sec-chap09-achievement1}\n\n### Introduction\n\nInstead following linearly the chapter I will try to compute my own <a class='glossary' title='Explorative Data Analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. (Wikipedia)'>EDA</a>. I will try three different method:\n\n1. Manufacturing the data and graphs myself. Writing own functions and using {**tidyverse**} packages to provide summary plots and statistics.\n2. Trying out the `graphics::pairs()` function.\n3. Experimenting with {**GGally**}, an extension package to {**ggplot2**} where one part (`GGally::ggpairs()`) is the equivalent to the base R `graphics::pairs()` function.\n\n### Steps for EDA\n\nI will apply the following steps:\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-chap09-eda-steps}\n: Some useful steps to explore data for regression analysis\n::::::\n:::\n::::{.my-procedure-container}\nOrder and completeness of the following tasks is not mandatory.\n\n1. **Browse the data**: \n    - **RStudio Data Explorer**: I am always using the data explorer in RStudio to get my first impression of the data. Although this step is not reproducible it forms my frame of mind what EDA steps I should follow and if there are issues I need especially to care about. \n    - **Skim data**: Look at the data with `skimr::skim()` to get a holistic view of the data: names, data types, missing values, ordered (categorical) minimum, maximum, mean, sd, distribution (numerical).\n    - **Read the codebook:** It is important to understand what the different variables mean.\n    - **Check structure:** Examine with `utils::str()` if the dataset has special structures, e.g. labelled data, attributes etc.\n    - **Glimpse actual data**: To get a feeling about data types and actual values use `dplyr::glimpse()`.\n    - **Glance at example rows**: As an alternative of `utils::head()` / `utils::tails()` get random row examples including first and last row of the dataset with my own function `glance_data()`.\n2. **Check normality assumption**:\n    - **Draw histograms of numeric variables**: To get a better idea I have these histogram overlaid with the theoretical normal distribution and the density curve of the current data. The difference between these two curves gives a better impression if normality is met or not.\n    - **Draw Q-Q plots of numeric variables**: <a class='glossary' title='A quantile-quantile plot is a visualization of data using probabilities to show how closely a variable follows a normal distribution. (SwR, Glossary) This plot is made up of points below which a certain percentage of the observations fall. On the x-axis are normally distributed values with a mean of 0 and a standard deviation of 1. On the y-axis are the observations from the data. If the data are normally distributed, the values will form a diagonal line through the graph. (SwR, chapter 6)'>Q-Q plots</a> gives even a more detailed picture if normality is met.\n    - **Compute normality tests**: If your data has less than 5.000 rows then use the <a class='glossary' title='The Shapiro-Wilk test is a statistical test to determine or confirm whether a variable has a normal distribution; it is sensitive to small deviations from normality and not useful for sample sizes above 5,000 because it will nearly always find non-normality. (SwR, Glossary)'>Shapiro-Wilk test</a>, otherwise the <a class='glossary' title='The Anderson-Darling Goodness of Fit Test (AD-Test) is a measure of how well your data fits a specified distribution. It’s commonly used as a test for normality. (Statistics How-To)'> Anderson-Darling test</a>. \n3. **Check homogeneity assumption**: If the normality assumption is not met, then test if the homogeneity of variance assumption between groups is met with <a class='glossary' title='Levene’s test is a statistical test to determine whether observed data meet the homogeneity of variances assumption; Levene’s test is used to test this assumption for t-tests and analysis of variance. (SwR, Glossary)'>Levene’s test</a> or with the more robust <a class='glossary' title='The Fligner-Killeen test is a non-parametric test for homogeneity of group variances based on ranks. It is useful when the data is non-normal or when there are outliers. (Real Statistics Using Excel)'>Fligner-Killeen’s test</a>. In the following steps use always median instead of mean and do not compute the <a class='glossary' title='Pearson’s r is a statistic that indicates the strength and direction of the relationship between two numeric variables that meet certain assumptions. (SwR, Glossary)'>Pearson’s r</a> but the <a class='glossary' title='Spearman’s rho a statistical test used to examine the strength, direction, and significance of the relationship between two numeric variables when they do not meet the assumptions for [Pearson]’s r. (SwR, Glossary)'>Spearman’s rho</a> coefficient. \n4. **Compute correlation coefficient**: Apply either Pearson’s r or the Spearman’s rho coefficient. There are function like `graphics::pairs()` or `GGally::ggpairs()` (@pak-GGally) that provide a graphical and statistical representation of all combinations of bivariate relationships.\n5. **Explore categorical data with box plots or violin plots**: Box plots work well between a numerical and categorical variable. You could also overlaid the data and violin plots to maximize the information in one single graph.\n::::\n:::::\n\n### Executing EDA for chapter 9\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap09-eda}\n: Explorative Data Analysis for chapter 9\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n\n###### tableone\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-eda-tableone}\n: Descriptive statics with the {**tableone**} package\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#tbl-eda-tableone .cell tbl-cap='Descriptive statics with the \\'tableone\\' package'}\n\n```{.r .cell-code}\ntableone::CreateTableOne(data = distance_ssp_clean,\n                         vars = c('dist_ssp', 'hiv_prevalence',\n                                  'opioid_rate', 'no_insurance',\n                                  'metro'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                             \n#>                              Overall        \n#>   n                             500         \n#>   dist_ssp (mean (SD))       107.74 (94.23) \n#>   hiv_prevalence (mean (SD)) 192.89 (213.35)\n#>   opioid_rate (mean (SD))     68.33 (36.81) \n#>   no_insurance (mean (SD))    12.18 (4.97)  \n#>   metro = non-metro (%)         274 (54.8)\n```\n\n\n:::\n:::\n\n***\n\n`skimr::skim()` with @tbl-chap09-show-data is a much better alternative! The second version of {**tableone**} in the book with the median instead of the mean is not necessary because it is in `skimr::skim()` integrated.\n::::\n:::::\n\n\n###### Histograms\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-eda-histograms}\n: Histograms of numeric variables\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist_distance <- my_hist_dnorm(\n    df = distance_ssp_clean,\n    v = distance_ssp_clean$dist_ssp,\n    n_bins = 30,\n    x_label = \"Nearest syringe services program in miles\"\n) \n\nhist_hiv <- my_hist_dnorm(\n    df = distance_ssp_clean,\n    v = distance_ssp_clean$hiv_prevalence,\n    n_bins = 30,\n    x_label = \"People with diagnosed HIV per 100,000\"\n) \n\nhist_opioid <- my_hist_dnorm(\n    df = distance_ssp_clean,\n    v = distance_ssp_clean$opioid_rate,\n    n_bins = 30,\n    x_label = \"Opioid prescriptions per 100 people\"\n)\n\nhist_insurance <- my_hist_dnorm(\n    df = distance_ssp_clean,\n    v = distance_ssp_clean$no_insurance,\n    n_bins = 30,\n    x_label = \"Percentage with no health insurance coverage\"\n)\n\ngridExtra::grid.arrange(\n   hist_distance, hist_hiv, hist_opioid, hist_insurance, nrow = 2\n)\n```\n\n::: {.cell-output-display}\n![Histograms for numeric variables of chapter 9](09-linear-regression_files/figure-html/fig-eda-histograms-1.png){#fig-eda-histograms width=672}\n:::\n:::\n\n\n***\nI developed a function where I can overlay the theoretical normal distribution and the density of the current data. The difference between the two curves gives an indication if we have a normal distribution.\n\nFrom our data we see that the biggest difference is between SPP distance and HIV prevalence. This right skewed distribution could also be detected from other indicator already present in the `skimr::skim()`view of @tbl-chap09-show-data:\n- The small histogram on the right is the most right skewed distribution.\n- The standard deviation of `hiv_prevalence` is the only one, that is bigger than the mean of the variable. \n- There is a huge difference between mean and the median (p50) where the mean is much bigger than the median (= right skewed distribution), e.g. there is a long tail to the right as can also be seen in the tiny histogram.\n\nAside from `hiv_prevalence` the variable `distance_ssp` is almost equally right skewed. The situation seems better for the rest of the numeric variables. But let's manufacture <a class='glossary' title='A quantile-quantile plot is a visualization of data using probabilities to show how closely a variable follows a normal distribution. (SwR, Glossary) This plot is made up of points below which a certain percentage of the observations fall. On the x-axis are normally distributed values with a mean of 0 and a standard deviation of 1. On the y-axis are the observations from the data. If the data are normally distributed, the values will form a diagonal line through the graph. (SwR, chapter 6)'>Q-Q plots</a> for all of them to see more in detail if they are normally distributed or not.\n\n\n\n\n\n::::\n:::::\n\n\n###### Q-Q plots\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-eda-qq-plots}\n: Q-Q plots of numeric variables\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqq_distance <- my_qq_plot(\n    df = distance_ssp_clean,\n    v  = distance_ssp_clean$dist_ssp,\n    col_qq = \"Distance to SSP\"\n)\n\nqq_hiv <- my_qq_plot(\n    df = distance_ssp_clean,\n    v  = distance_ssp_clean$hiv_prevalence,\n    col_qq = \"HIV diagnosed\"\n)\n\nqq_opioid <- my_qq_plot(\n    df = distance_ssp_clean,\n    v  = distance_ssp_clean$opioid_rate,\n    col_qq = \"Opioid prescriptions\"\n)\n\nqq_insurance <- my_qq_plot(\n    df = distance_ssp_clean,\n    v  = distance_ssp_clean$no_insurance,\n    col_qq = \"Health insurance\"\n)\n\n\ngridExtra::grid.arrange(\n   qq_distance, qq_hiv, qq_opioid, qq_insurance, nrow = 2\n)\n```\n\n::: {.cell-output-display}\n![Q-Q plots for numeric variables of chapter 9](09-linear-regression_files/figure-html/fig-eda-qq-plots-1.png){#fig-eda-qq-plots width=672}\n:::\n:::\n\n\n***\nIt turned out that all four numeric variables are not normally distributed. Some of them looked in the histograms quite OK, because the differences to the normal distribution on the lower and upper end of the data compensate each other.\n\nTesting normality with <a class='glossary' title='The Shapiro-Wilk test is a statistical test to determine or confirm whether a variable has a normal distribution; it is sensitive to small deviations from normality and not useful for sample sizes above 5,000 because it will nearly always find non-normality. (SwR, Glossary)'>Shapiro-Wilk</a> or <a class='glossary' title='The Anderson-Darling Goodness of Fit Test (AD-Test) is a measure of how well your data fits a specified distribution. It’s commonly used as a test for normality. (Statistics How-To)'>Anderson-Darling</a> test will show that they are definitely not normally distributed.\n\n::::\n:::::\n\n###### Normality\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-eda-test-normality}\n: Normality checking with Shapiro-Wilk & Anderson-Darling tests\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#tbl-eda-test-normality .cell tbl-cap='Testing normality with Shapiro-Wilk & Anderson-Darling tests'}\n\n```{.r .cell-code}\ndist_test <-  stats::shapiro.test(distance_ssp_clean$dist_ssp)\nhiv_test <-  stats::shapiro.test(distance_ssp_clean$hiv_prevalence)\nopioid_test <- stats::shapiro.test(distance_ssp_clean$opioid_rate)\ninsurance_test <- stats::shapiro.test(distance_ssp_clean$no_insurance)\n\ndist_test2 <-  nortest::ad.test(distance_ssp_clean$dist_ssp)\nhiv_test2 <-  nortest::ad.test(distance_ssp_clean$hiv_prevalence)\nopioid_test2 <- nortest::ad.test(distance_ssp_clean$opioid_rate)\ninsurance_test2 <- nortest::ad.test(distance_ssp_clean$no_insurance)\n\n\nnormality_test <- \n    dplyr::bind_rows(\n        broom:::glance.htest(dist_test),\n        broom:::glance.htest(hiv_test),\n        broom:::glance.htest(opioid_test),\n        broom:::glance.htest(insurance_test),\n        broom:::glance.htest(dist_test2),\n        broom:::glance.htest(hiv_test2),\n        broom:::glance.htest(opioid_test2),\n        broom:::glance.htest(insurance_test2)\n    ) |> \n    dplyr::bind_cols(\n        variable = c(\"dist_ssp\", \"hiv_prevalence\",\n                     \"opioid_rate\", \"no_insurance\",\n                     \"dist_ssp\", \"hiv_prevalence\",\n                     \"opioid_rate\", \"no_insurance\")\n    ) |> \n    dplyr::relocate(variable)\n\nnormality_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 8 × 4\n#>   variable       statistic  p.value method                         \n#>   <chr>              <dbl>    <dbl> <chr>                          \n#> 1 dist_ssp           0.874 1.10e-19 Shapiro-Wilk normality test    \n#> 2 hiv_prevalence     0.649 8.72e-29 Shapiro-Wilk normality test    \n#> 3 opioid_rate        0.938 1.55e-13 Shapiro-Wilk normality test    \n#> 4 no_insurance       0.946 1.85e-12 Shapiro-Wilk normality test    \n#> 5 dist_ssp          18.0   3.7 e-24 Anderson-Darling normality test\n#> 6 hiv_prevalence    37.0   3.7 e-24 Anderson-Darling normality test\n#> 7 opioid_rate        2.68  9.14e- 7 Anderson-Darling normality test\n#> 8 no_insurance       3.43  1.39e- 8 Anderson-Darling normality test\n```\n\n\n:::\n:::\n\n\n***\n\nThe <a class='glossary' title='The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary)'>p-values</a> from both tests are for all four variables very small, e.g. statistically significant. Therefore we have to reject the Null that they are normally distributed.\n\n\n::::\n:::::\n\n\n::: {.callout-tip}\nIt turned out that all four variable are not normally distributed. We can't therefore not use <a class='glossary' title='Pearson’s r is a statistic that indicates the strength and direction of the relationship between two numeric variables that meet certain assumptions. (SwR, Glossary)'>Pearson’s r coefficient</a>. \n:::\n\nBefore we are going to use <a class='glossary' title='Spearman’s rho a statistical test used to examine the strength, direction, and significance of the relationship between two numeric variables when they do not meet the assumptions for [Pearson]’s r. (SwR, Glossary)'>Spearman’s rho</a> let's check the homogeneity of variance assumption (<a class='glossary' title='Homoscedasticity is [homogeneity of variances], contrast is [Heteroscedasticity]. Homoscedasticity is an assumption of correlation and linear regression that requires that the variance of y be constant across all the values of x; visually, this assumption would show points along a fit line between x and y being evenly spread on either side of the line for the full range of the relationship. (SwR, Glossary)'>homoscedasticity</a>) with a scatterplot with `lm` and `loess` curve  and using <a class='glossary' title='Levene’s test is a statistical test to determine whether observed data meet the homogeneity of variances assumption; Levene’s test is used to test this assumption for t-tests and analysis of variance. (SwR, Glossary)'>Levene’s Test</a> and the <a class='glossary' title='The Fligner-Killeen test is a non-parametric test for homogeneity of group variances based on ranks. It is useful when the data is non-normal or when there are outliers. (Real Statistics Using Excel)'>Fligner-Killeen’s test</a>.\n\n###### Scatterplots\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-eda-scatterplots}\n: Scatterplots of numeric variables\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nscatter_dist_hiv <-  my_scatter(\n    df = distance_ssp_clean,\n    v =  distance_ssp_clean$hiv_prevalence,\n    w =  distance_ssp_clean$dist_ssp,\n    x_label = \"HIV prevalence\",\n    y_label = \"Distance to SSP\"\n)\n\nscatter_dist_opioid <-  my_scatter(\n    df = distance_ssp_clean,\n    v =  distance_ssp_clean$opioid_rate,\n    w =  distance_ssp_clean$dist_ssp,\n    x_label = \"Opioid rate\",\n    y_label = \"Distance to SSP\"\n)\n\nscatter_dist_insurance <-  my_scatter(\n    df = distance_ssp_clean,\n    v =  distance_ssp_clean$no_insurance,\n    w =  distance_ssp_clean$dist_ssp,\n    x_label = \"No insurance\",\n    y_label = \"Distance to SSP\"\n)\n\ngridExtra::grid.arrange(\n   scatter_dist_hiv, scatter_dist_opioid, scatter_dist_insurance, nrow = 3\n)\n```\n\n::: {.cell-output-display}\n![Scatterplots of numeric variables](09-linear-regression_files/figure-html/fig-eda-scatterplots-1.png){#fig-eda-scatterplots width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\n\n\n\n###### Homogeneity\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-eda-test-homogeneity}\n: Testing homogeneity of variances with Levene’s and Fligner-Killeen’s test\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#tbl-chap09-eda-test-homogeneity .cell tbl-cap='Homogeneity of variances tested with Levene’s and Fligner-Killeen’s test'}\n\n```{.r .cell-code}\nhiv_test <-  stats::fligner.test(\n    distance_ssp_clean$dist_ssp,\n    distance_ssp_clean$hiv_prevalence\n    )\nopioid_test <- stats::fligner.test(\n    distance_ssp_clean$dist_ssp,\n    distance_ssp_clean$opioid_rate\n    )\ninsurance_test <- stats::fligner.test(\n    distance_ssp_clean$dist_ssp,\n    distance_ssp_clean$no_insurance\n    )\n\nhiv_test2 <-  car::leveneTest(\n    distance_ssp_clean$dist_ssp,\n    distance_ssp_clean$hiv_prevalence\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning in leveneTest.default(distance_ssp_clean$dist_ssp,\n#> distance_ssp_clean$hiv_prevalence): distance_ssp_clean$hiv_prevalence coerced\n#> to factor.\n```\n\n\n:::\n\n```{.r .cell-code}\nopioid_test2 <- car::leveneTest(\n    distance_ssp_clean$dist_ssp,\n    distance_ssp_clean$opioid_rate\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning in leveneTest.default(distance_ssp_clean$dist_ssp,\n#> distance_ssp_clean$opioid_rate): distance_ssp_clean$opioid_rate coerced to\n#> factor.\n```\n\n\n:::\n\n```{.r .cell-code}\ninsurance_test2 <- car::leveneTest(\n    distance_ssp_clean$dist_ssp,\n    distance_ssp_clean$no_insurance\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning in leveneTest.default(distance_ssp_clean$dist_ssp,\n#> distance_ssp_clean$no_insurance): distance_ssp_clean$no_insurance coerced to\n#> factor.\n```\n\n\n:::\n\n```{.r .cell-code}\nhomogeneity_test <- \n    dplyr::bind_rows(\n        broom::tidy(hiv_test2),\n        broom::tidy(opioid_test2),\n        broom::tidy(insurance_test2)\n    ) |> \n    dplyr::mutate(method = \"Levene's Test for Homogeneity of Variance\") |> \n    dplyr::bind_rows(\n        broom:::glance.htest(hiv_test),\n        broom:::glance.htest(opioid_test),\n        broom:::glance.htest(insurance_test),\n    ) |> \n    dplyr::bind_cols(\n        variable = c(\"dist_hiv\",\n                     \"dist_opioid\", \n                     \"dist_insurance\",\n                     \"dist_hiv\",\n                     \"dist_opioid\", \n                     \"dist_insurance\"\n                 )\n    ) |> \n    dplyr::relocate(variable)\n\nhomogeneity_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 6 × 7\n#>   variable       statistic p.value    df df.residual method            parameter\n#>   <chr>              <dbl>   <dbl> <int>       <int> <chr>                 <dbl>\n#> 1 dist_hiv           0.493  0.999    395          34 Levene's Test fo…        NA\n#> 2 dist_opioid        0.893  0.770    406          93 Levene's Test fo…        NA\n#> 3 dist_insurance     0.750  0.983    171         328 Levene's Test fo…        NA\n#> 4 dist_hiv         408.     0.312     NA          NA Fligner-Killeen …       395\n#> 5 dist_opioid      451.     0.0609    NA          NA Fligner-Killeen …       406\n#> 6 dist_insurance   170.     0.502     NA          NA Fligner-Killeen …       171\n```\n\n\n:::\n:::\n\n***\n\nAll p-values are higher than the threshold of .05 and are therefore not statistically significant. The Null must not rejected, the homogeneity of variance assumption for all variables is met.\n::::\n:::::\n\n###### Correlations\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-eda-cor}\n: Correlations for numeric variables of chapter 9\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#tbl-chap09-cor .cell tbl-cap='Correlations for numeric variables of chapter 9'}\n\n```{.r .cell-code}\ncor_pearson <- distance_ssp_clean |> \n    dplyr::summarize(\n        hiv_cor = stats::cor(\n        x = dist_ssp,\n        y = hiv_prevalence,\n        use = \"complete.obs\",\n        method = \"pearson\"\n        ),\n        opioid_cor = stats::cor(\n        x = dist_ssp,\n        y = opioid_rate,\n        use = \"complete.obs\",\n        method = \"pearson\"\n        ),\n        insurance_cor = stats::cor(\n        x = dist_ssp,\n        y = no_insurance,\n        use = \"complete.obs\",\n        method = \"pearson\"\n        ),\n        `n (sample)` = dplyr::n()\n    )\n\ncor_spearman <- distance_ssp_clean |> \n    dplyr::summarize(\n        hiv_cor = stats::cor(\n        x = dist_ssp,\n        y = hiv_prevalence,\n        use = \"complete.obs\",\n        method = \"spearman\"\n        ),\n        opioid_cor = stats::cor(\n        x = dist_ssp,\n        y = opioid_rate,\n        use = \"complete.obs\",\n        method = \"spearman\"\n        ),\n        insurance_cor = stats::cor(\n        x = dist_ssp,\n        y = no_insurance,\n        use = \"complete.obs\",\n        method = \"spearman\"\n        ),\n        `n (sample)` = dplyr::n()\n    )\n\ncor_kendall <- distance_ssp_clean |> \n    dplyr::summarize(\n        hiv_cor = stats::cor(\n        x = dist_ssp,\n        y = hiv_prevalence,\n        use = \"complete.obs\",\n        method = \"kendall\"\n        ),\n        opioid_cor = stats::cor(\n        x = dist_ssp,\n        y = opioid_rate,\n        use = \"complete.obs\",\n        method = \"kendall\"\n        ),\n        insurance_cor = stats::cor(\n        x = dist_ssp,\n        y = no_insurance,\n        use = \"complete.obs\",\n        method = \"kendall\"\n        ),\n        `n (sample)` = dplyr::n()\n    )\n\ncor_chap09 <- dplyr::bind_rows(cor_pearson, cor_spearman, cor_kendall)\ncor_chap09 <- dplyr::bind_cols(\n    method = c(\"Pearson\", \"Spearman\", \"Kendall\"), cor_chap09)\ncor_chap09\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 3 × 5\n#>   method   hiv_cor opioid_cor insurance_cor `n (sample)`\n#>   <chr>      <dbl>      <dbl>         <dbl>        <int>\n#> 1 Pearson  -0.0240   -0.0998          0.413          500\n#> 2 Spearman  0.0855   -0.0164          0.345          500\n#> 3 Kendall   0.0600   -0.00918         0.234          500\n```\n\n\n:::\n:::\n\n***\n\nHere I have computed for a comparison all three correlation coefficients of the nearest distance to the next <a class='glossary' title='SSP stands for syringe services program (SwR)'>SSP</a> with the numeric variabeles of the dataset. \n\n- Pearson’s $r$ is not allowed for all of the three variables, because our data didn't meet the normality assumption. \n- Using Spearman’s $\\rho$ or Kendall’s $\\tau$ instead of Pearson’s $r$ results in big differences. For instance: the correlation of distance to the next SSP and HIV prevalence reverses it direction.\n- Kendall’s tau $\\tau$ is more conservative (smaller) than Spearman’s rho and it is also preferred in most scenarios. (Kendall’s tau is not mentioned in the book. Maybe the reason is --- as far as I understand -- that Spearman’s  is the most widely used correlation coefficient?)\n\n***\n\nI want to confirm my internet research with the following quotes:\n\n**First quote**\n\n> In the normal case, Kendall correlation is more robust and efficient than Spearman correlation. It means that Kendall correlation is preferred when there are small samples or some outliers. ([Pearson vs Spearman vs Kendall](https://datascience.stackexchange.com/questions/64260/pearson-vs-spearman-vs-kendall)) [@pluviophile2019].\n\n**Second quote**\n\n> Kendall’s Tau: usually smaller values than Spearman’s rho correlation. Calculations based on concordant and discordant pairs. Insensitive to error. P values are more accurate with smaller sample sizes.\n>\n> Spearman’s rho: usually have larger values than Kendall’s Tau.  Calculations based on deviations.  Much more sensitive to error and discrepancies in data.\n>\n> The main advantages of using Kendall’s tau are as follows:\n>\n> - The distribution of Kendall’s tau has better statistical properties.\n> - The interpretation of Kendall’s tau in terms of the probabilities of observing the agreeable (concordant) and non-agreeable (discordant) pairs is very direct.\n> - In most of the situations, the interpretations of Kendall’s tau and Spearman’s rank correlation coefficient are very similar and thus invariably lead to the same inferences. ([Kendall’s Tau and Spearman’s Rank Correlation Coefficient](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/kendalls-tau-and-spearmans-rank-correlation-coefficient/)) [@statisticssolutionsn.d] \n\n**Third quote**\n\n> - Kendall Tau-b is more accurate for small sample sizes with strong correlations.\n> - Spearman’s rho is preferred for weak correlations in small datasets.\n> - In large samples, Kendall Tau-b’s reliability surpasses Spearman’s rho.\n> - Kendall’s Tau is a robust estimator against outliers and non-normality.\n> - Overall, Kendall Tau-b outperforms Spearman for most statistical scenarios [Kendall Tau-b vs Spearman: Which Correlation Coefficient Wins?](https://statisticseasily.com/kendall-tau-b-vs-spearman/) [@learnstatisticseasily2024]\n\n:::::{.my-resource}\n:::{.my-resource-header}\n:::::: {#lem-chap09-corr-coefficient}\nUnderstanding the different correlation coefficients\n::::::\n:::\n::::{.my-resource-container}\n- [Kendall Tau-b vs Spearman: Which Correlation Coefficient Wins?](https://statisticseasily.com/kendall-tau-b-vs-spearman/): This important article wxplains the decisive factors in choosing the proper non-parametric correlation coefficient (Kendall Tau-b vs Spearman) for your data analysis. [@learnstatisticseasily2024]\n- [Pearson, Spearman and Kendall correlation coefficients by hand](https://statsandr.com/blog/pearson-spearman-kendall-correlation-by-hand/#introduction): This articles illustrates how to compute the Pearson, Spearman and Kendall correlation coefficients by hand and under two different scenarios (i.e., with and without ties). [@soetewey2023]\n- [Chapter 22: Correlation Types and When to Use Them](https://ademos.people.uic.edu/Chapter22.html): This chapter of [@demos2024] covers the strengths, weaknesses, and when or when not to use three common types of correlations (Pearson, Spearman, and Kendall). It’s part statistics refresher, part R tutorial. [@sarmenton.d]\n\n::::\n:::::\n\n\n\n\n\n\n::::\n:::::\n\n###### metro\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-eda-metro}\n: Distance in miles to nearest syringe programs by metro or non-metro status for a sample of 500 counties\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#tbl-chap09-eda-violin-boxplot .cell tbl-cap='Distance in miles to nearest syringe programs by metro or non-metro status for a sample of 500 counties'}\n\n```{.r .cell-code}\ndistance_ssp_clean |>\n    dplyr::group_by(metro) |>\n    dplyr::summarize(mean.dist = base::mean(dist_ssp),\n                     median.dist = stats::median(dist_ssp),\n                     min.dist = base::min(dist_ssp),\n                     max.dist = base::max(dist_ssp)\n                     )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 5\n#>   metro     mean.dist median.dist min.dist max.dist\n#>   <fct>         <dbl>       <dbl>    <dbl>    <dbl>\n#> 1 metro          85.0        50.4        0      436\n#> 2 non-metro     126.         96.0        6      510\n```\n\n\n:::\n:::\n\n***\n\nThe big difference between mean and median reflects a right skewed distribution. There are some people living extremely far from the next <a class='glossary' title='SSP stands for syringe services program (SwR)'>SSP</a> both in non-metro *and* metro areas. \n\nIt is no surprise that the distance for people living in a non-metro area is much longer than for people in big city. But what certainly surprised me, is that even in big cities half of people live more than 50 miles form the next SSP.\n\n\n::::\n:::::\n\n###### Violin\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-eda-violin-plot}\n: Distance in miles to nearest syringe programs by metro or non-metro status for a sample of 500 counties\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ndistance_ssp_clean |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          x = metro, \n          y = dist_ssp, \n          fill = metro\n          )\n      ) +\n  ggplot2::geom_violin(\n      ggplot2::aes(\n          color = metro\n          ), \n      fill = \"white\", \n      alpha = .8\n      ) +\n  ggplot2::geom_boxplot(\n      ggplot2::aes(\n          fill = metro, \n          color = metro\n          ), \n      width = .2, \n      alpha = .3\n      ) +\n  ggplot2::geom_jitter(\n      ggplot2::aes(\n          color = metro\n          ), \n      alpha = .4\n      ) +\n  ggplot2::labs(\n      x = \"Type of county\",\n      y = \"Miles to syringe program\"\n      ) +\n  ggplot2::scale_fill_manual(\n      values = c(\"#78A678\", \"#7463AC\"), \n      guide = \"none\") +\n  ggplot2::scale_color_manual(\n      values = c(\"#78A678\", \"#7463AC\"), \n      guide = \"none\") +\n  ggplot2::coord_flip()\n```\n\n::: {.cell-output-display}\n![Distance in miles to nearest syringe programs by metro or non-metro status for a sample of 500 counties](09-linear-regression_files/figure-html/fig-chap09-eda-violin-boxplot-1.png){#fig-chap09-eda-violin-boxplot width=672}\n:::\n:::\n\n***\n\n\n\n\n::::\n:::::\n\n###### pairs\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-plot-pairs}\n: Scatterplots of variable pairs from dataset of chapter 9\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nlt_purple <- t_col(\"purple3\", perc = 50, name = \"lt.purple\")\n\npanel.hist <- function(x, ...)\n{\n    usr <- par(\"usr\")\n    par(usr = c(usr[1:2], 0, 1.5) )\n    h <- hist(x, plot = FALSE)\n    breaks <- h$breaks; nB <- length(breaks)\n    y <- h$counts; y <- y/max(y)\n    rect(breaks[-nB], 0, breaks[-1], y, col = \"grey80\", ...)\n}\n\n\ngraphics::pairs(distance_ssp_clean[3:6], \n                pch = 23, \n                panel = panel.smooth,\n                cex = 1.5, \n                bg = lt_purple, \n                horOdd = TRUE,\n                diag.panel = panel.hist, \n                cex.labels = 2, \n                font.labels = 2,\n                gap = 0\n                )\n```\n\n::: {.cell-output-display}\n![Scatterplots of variable pairs from dataset of chapter 9](09-linear-regression_files/figure-html/fig-plot-pairs-1.png){#fig-plot-pairs width=960}\n:::\n:::\n\n\n::::\n:::::\n\n###### metro\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-plot-pairs-metro}\n: Distance to SSP with different numeric variable and metro/nonmetro location\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\npanel.hist <- function(x, ...) {\n    usr <- par(\"usr\")\n    par(usr = c(usr[1:2], 0, 1.5) )\n    h <- hist(x, plot = FALSE)\n    breaks <- h$breaks; nB <- length(breaks)\n    y <- h$counts; y <- y/max(y)\n    rect(breaks[-nB], 0, breaks[-1], y, col = \"grey80\", ...)\n}\n\ngraphics::pairs(distance_ssp_clean[3:6], \n                main = \"Distance to SSP -- Metro (red) & Nonmetro (blue)\",\n                panel = panel.smooth,\n                horOdd = TRUE,\n                diag.panel = panel.hist, \n                pch = 21, \n                gap = 0,\n                bg = c(\"red\", \"blue\")[unclass(distance_ssp_clean$metro)])\n```\n\n::: {.cell-output-display}\n![Distance to SSP -- Metro (red) & Nonmetro (blue)](09-linear-regression_files/figure-html/fig-plot-pairs-metro-1.png){#fig-plot-pairs-metro width=960}\n:::\n:::\n\n\n::::\n:::::\n\n###### ggpairs\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-plot-ggpairs}\n: Scatterplots of variable pairs from dataset of chapter 9\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nGGally::ggpairs(distance_ssp_clean,\n                columns = 3:7)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\n#> Removed 70 rows containing missing values\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Removed 70 rows containing missing values or values outside the scale range\n#> (`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Removed 70 rows containing non-finite outside the scale range\n#> (`stat_density()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\n#> Removed 70 rows containing missing values\n\n#> Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\n#> Removed 70 rows containing missing values\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Removed 70 rows containing non-finite outside the scale range\n#> (`stat_boxplot()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Removed 70 rows containing missing values or values outside the scale range\n#> (`geom_point()`).\n#> Removed 70 rows containing missing values or values outside the scale range\n#> (`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Removed 70 rows containing non-finite outside the scale range\n#> (`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Scatterplots of variable pairs from dataset of chapter 9](09-linear-regression_files/figure-html/fig-plot-ggpairs-1.png){#fig-plot-ggpairs width=960}\n:::\n:::\n\n\n::::\n:::::\n\n\n\n\n:::\n\n::::\n:::::\n\n## Achivement 2: Exploring line model {#sec-chap09-achievement2}\n\n### Introduction\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap09-line-model}\n: Equation for linear model\n::::::\n:::\n::::{.my-theorem-container}\n\n\n$$\n\\begin{align*}\ny = &m_{x}+b \\\\\ny = &b_{0}+b_{1}x \\\\\ny = &c+b_{1}x\n\\end{align*}\n$$ {#eq-chap09-linear-model}\n\n***\n\n- $m, b_{1}$: <a class='glossary' title='The slope is the increase in the dependent variable when the independent variable increases with one unit and all other independent variables remain the same. (Intercept, Slope in Regression)'>slope</a> of the line \n- $b, b_{0}, c$: y-<a class='glossary' title='The intercept is the value of the dependent variables if all independent variables have the value zero. (Intercept, Slope in Regression)'>intercept</a> of the line, or the value of y when x = 0\n- $x, y$: the coordinates of each point along the line\n\nSometimes $b^*$ is used. This means that the variable had been standardized, or transformed into z-scores, before the regression model was estimated.\n\n\n::::\n:::::\n\nAn example of a linear equation would be $y = 3 + 2x$.\n\n::: {.callout-important #imp-variable-names-linear-equation}\n\n## Variable names and the difference between deterministic and stochastic\n\n- The y variable on the left-hand side of the equation is called the dependent or outcome variable. \n- The x variable(s) on the right-hand side of the equation is/are called the independent or predictor variable(s).\n\n***\n\n- A deterministic equation, or model, has one precise value for y for each value of x. Some equation in physics are deterministic, e.g., $e = mc^2$.\n- In a stochastic equation, or model, you cannot predict or explain something exactly. Most of the time, there is some variability that cannot be fully explained or predicted. This unexplained variability is represented by an error term that is added to the equation. Relationships measured in social science are typically stochastic.\n\n:::\n\n@eq-chap09-linear-model can be re-written with these terms:\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap09-linear-model}\n: Equation of a linear model (rewritten)\n::::::\n:::\n::::{.my-theorem-container}\n\n$$\n\\begin{align*}\noutcome = &b_{0} + b_{1} \\times predictor \\\\\noutcome = &b_{0} + b_{1} \\times predictor + error \\\\\n\\end{align*}\n$$ {#eq-chap09-lm-rewritten}\n\n::::\n:::::\n\n### Plotting an example\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap09-example-lm}\n: Example of a linear model\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### deterministic\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-example-lm-water-weeks}\n: Example of a deterministic linear model with gallons of water needed as an outcome and weeks as a predictor \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make a vector called weeks that has the values 1 through 12 in it\nweeks <- 1:12\n\n# use the regression model to make a vector called gallons with\n# weeks as the values\ngallons <- 3 + 2 * weeks\n\n# make a data frame of weeks and gallons\nwater <- data.frame(weeks, gallons)\n\n# Make a plot (Figure 9.9)\nwater |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          x = weeks, \n          y = gallons\n          )\n      ) +\n  ggplot2::geom_line(\n      ggplot2::aes(\n          linetype = \"Linear model\\ngallons=3+2*weeks\"\n          ), \n      color = \"gray60\", \n      linewidth = 1\n      ) + \n  ggplot2::geom_point(\n      ggplot2::aes(\n          color = \"Observation\"\n          ), \n      size = 4, \n      alpha = .6\n      ) +\n  ggplot2::labs(\n      x = \"Weeks\", \n      y = \"Gallons of water needed\"\n      ) +\n  ggplot2::scale_linetype_manual(values = 2, name = \"\") +\n  ggplot2::scale_color_manual(values = \"#7463AC\", name = \"\")\n```\n\n::: {.cell-output-display}\n![Example of a linear model with gallons of water needed as an outcome and weeks as a predictor](09-linear-regression_files/figure-html/fig-example-lm-water-weeks-1.png){#fig-example-lm-water-weeks width=672}\n:::\n:::\n\n***\n\nThere is nothing new in this code chunk, therefore I have just taken the code from the book only adapted with changes resulting from newer versions of {**ggplot2**} (e.g., `linewidth` instead of `size`). \n\nIt is important to know that the graph does not use the calculation of a linear model with `ggplot2::geom_smooth()` but merely uses `ggplot2::geom_line` to connect the points. We are using #eq-chap09-linear-model, e.g. a <a class='glossary' title='A deterministic equation, or model, has one precise value for y for each value of x. (SwR, Chap09)'>deterministic</a> formula.\n\n\n::::\n:::::\n\n\n###### stochastic\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-example-lm-water-weeks-with-errors}\n: Example of a stochastic linear model with gallons of water needed as an outcome and weeks as a predictor \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make a vector called weeks that has the values 1 through 12 in it\nweeks <- 1:12\n\n# use the regression model to make a vector called gallons with\n# weeks as the values \n# but this time with simulated residuals\n\nset.seed(1001) # for reproducibility\ngallons <- 3 + (2 * weeks) + rnorm(12, 0, 2.5)\n\n# make a data frame of weeks and gallons\nwater <- data.frame(weeks, gallons)\n\n# calculate the residuals from the linear model\nres <- base::summary(\n    stats::lm(gallons ~ weeks, data = water)\n    )$residuals\n\n\nwater |> \n      ggplot2::ggplot(\n          ggplot2::aes(\n                x = weeks,\n                y = gallons\n          )\n      ) +\n      ggplot2::geom_point(\n            ggplot2::aes(\n                color = \"Observation\"\n          ), \n          size = 4, \n          alpha = .6\n      ) +\n      ggplot2::geom_smooth(\n          formula = y ~ x,\n          method = \"lm\",\n          se = FALSE,\n          ggplot2::aes(\n                linetype = \"Linear model\\ngallons=3+2*weeks\"\n          ), \n          color = \"gray60\", \n          linewidth = 1\n      ) +\n      ggplot2::geom_segment(\n          ggplot2::aes(\n              x = weeks,\n              y = gallons,\n              xend = weeks,\n              yend = gallons - res\n          )\n      )  +\n      ggplot2::labs(\n          x = \"Weeks\", \n          y = \"Gallons of water needed\"\n          ) +\n      ggplot2::scale_linetype_manual(values = 2, name = \"\") +\n      ggplot2::scale_color_manual(values = \"#7463AC\", name = \"\")\n```\n\n::: {.cell-output-display}\n![Example of a linear model with gallons of water needed as an outcome and weeks as a predictor with deviations (errors)](09-linear-regression_files/figure-html/fig-example-lm-water-weeks-with-errors-1.png){#fig-example-lm-water-weeks-with-errors width=672}\n:::\n:::\n\n***\n\nThis is my replication of book’s Figure 9.10, where no R example code is available. I am proud to state that I did this graph without looking ahead or to read the tutorial by Jackson [-@jackson2016] that is recommended later in the book. To draw this graph I had to take three new actions:\n\n1. I had to simulate with `stats::rnorm()` residuals to change from @eq-chap09-linear-model to the second line of @eq-chap09-lm-rewritten.\n2. I had to calculate a linear model to get the residuals with `base::summary(stats::lm())`.\n3. I had this time to compute the line for the linear model with `ggplot2::geom_smooth()`.\n\nWithout these three code addition, I wouldn’t have been able to draw the vertical lines from the observations to the line of the linear model. \n\n\n\n\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n\n\nAlthough I managed to create @fig-example-lm-water-weeks-with-errors myself I mixed up in the explaining text the concepts of errors and residuals.\n\n::: {.callout-important #imp-errors-residuals}\n## Errors vs. Residuals\n\nErrors and residuals are two closely related and easily confused measures:\n\n- The error of an observation is the deviation of the observed value from the true value of a quantity of interest (for example, a population mean). \n- The residual is the difference between the observed value and the estimated value of the quantity of interest (for example, a sample mean).\n:::\n\n\n## Achievement 3: Slope and Intercept {#sec-chap09-achievement3}\n\n### Introduction\n\nA <a class='glossary' title='Simple does not mean easy; instead, it is the term used for a statistical model used to predict or explain a continuous outcome by a single predictor. (SwR, Glossary, Chap09)'>simple linear regression</a> model could be used to examine the relationship between the percentage of people without health insurance and the distance to a syringe program for a county.\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap09-formula-distance-insurance}\n: Regression of people without health insurance and the distance to SSP\n::::::\n:::\n::::{.my-theorem-container}\n$$\ndistance = b_{0} + b_{1} \\times \\text{no\\_insurance}\n$$ {#eq-chap09-distance-insurance-regression}\n::::\n:::::\n\n### Computing the slope\n\nThe slope formula in @eq-chap09-slope is adding up the product of differences between the observed values and mean value of percentage uninsured (`no_insurance`) and the observed values and mean value of distance to syringe program (`dist_ssp`) for each of the 500 counties. This value is divided by the summed squared differences between the observed and mean values of `no_insurance` for each county.\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap09-slope}\n: Computing the slop\n::::::\n:::\n::::{.my-theorem-container}\n$$\nb_{1} = \\frac{\\sum_{i = 1}^n (x_{i}-m_{x})(y_{i}-m_{y})}{\\sum_{i = 1}^n (x_{i}-m_{x})^2}\n$$ {#eq-chap09-slope}\n\n***\n\n- $i$: individual observation, in this case a county\n- $n$: sample size, in this case 500\n- $x_{i}$: mean value of `no_insurance` for the sample\n- $y_{i}$: value of `dist_ssp` for $i$\n- $m_{y}$: mean value of `dist_ssp` for the sample\n- $\\sum$: symbol for the sum\n- $b_{i}$: slope\n\n::::\n:::::\n\n### Computing the intercept\n\nOnce the slope is computed, the intercept can be computed by putting the slope and the values of $m_{x}$ and $m_{y}$ into the equation for the line with x and y replaced by $m_{x}$ and $m_{y}$, $m_{y} = b_{0} + b_{1} times m_{x}$, and solving it for $b_{0}$, which is the y-intercept. Because this method of computing the slope and intercept relies on the squared differences and works to minimize the residuals overall, it is often called ordinary least squares or <a class='glossary' title='Ordinary least square regression (OLS) is a method of estimating a linear regression model that finds the regression line by minimizing the squared differences between each data point and the regression line. (Swr; Glossary)'>OLS</a> regression.\n\n### Estimating the linear regression model with R\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-est-lm-r}\n: Estimating the linear regression model of people without health insurance and the distance to SSP using R\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-est-lm-r}\n\n::: {.cell}\n\n```{.r .cell-code}\n# linear regression of distance to syringe program by percent uninsured\n\nbase::summary(\n    stats::lm(\n        formula = dist_ssp ~ no_insurance,\n        data = distance_ssp_clean, \n        na.action = na.exclude\n        )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> stats::lm(formula = dist_ssp ~ no_insurance, data = distance_ssp_clean, \n#>     na.action = na.exclude)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -217.71  -60.86  -21.61   47.73  290.77 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   12.4798    10.1757   1.226    0.221    \n#> no_insurance   7.8190     0.7734  10.110   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 85.91 on 498 degrees of freedom\n#> Multiple R-squared:  0.1703,\tAdjusted R-squared:  0.1686 \n#> F-statistic: 102.2 on 1 and 498 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nLinear regression of distance to syringe program by percent uninsured\n:::\n***\n\n- <a class='glossary' title='The intercept is the value of the dependent variables if all independent variables have the value zero. (Intercept, Slope in Regression)'>Intercept</a>: The $y$-intercept of 12.48 is the $y$-value when $x$ is zero. The model predicts that a county with 0% of people being uninsured would have a distance to the nearest syringe program of 12.48 miles.\n- <a class='glossary' title='The slope is the increase in the dependent variable when the independent variable increases with one unit and all other independent variables remain the same. (Intercept, Slope in Regression)'>Slope</a>: The slope of 7.82 is the change in $y$ for every one-unit change in $x$. If the percent uninsured goes up by 1% in a county, the distance in miles to a syringe program would change by 7.82 miles.\n\n***\n\n$$\n\\begin{align*}\ndistance = 12.48 + 7.82 \\times \\text{no\\_insurance} \\\\\ndistance = 12.48 + 7.82 \\times 10 = 90.68\n\\end{align*}\n$$\nBased on the linear regression model, a county with 10% of people uninsured would be 90.68 miles from the nearest syringe program.\n::::\n:::::\n\n### Understanding residuals\n\nIn @fig-example-lm-water-weeks-with-errors I have already graphed a demonstration how the residuals relate to the regression line. The regression line minimizes the residual differences between the values predicted by the regression line and the observed values. \n\nThis is how <a class='glossary' title='Ordinary least square regression (OLS) is a method of estimating a linear regression model that finds the regression line by minimizing the squared differences between each data point and the regression line. (Swr; Glossary)'>OLS</a> works. OLS minimizes those distances captured in @fig-example-lm-water-weeks-with-errors by the solid vertical lines: It minimizes the <a class='glossary' title='Residuals are the differences between the observed values and the predicted values. (SwR, Glossary)'>residuals</a>.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap05-regression-with-residuals}\n: Regression with residuals between percentage without health insurance and distance to nearest SSP\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#tbl-regression-with-residuals .cell tbl-cap='Relationship between percentage without health insurance and distance to nearest syringe program in 500 counties with residuals'}\n\n```{.r .cell-code}\n# calculate the linear model\nlm_dist  <-  stats::lm(\n        formula = dist_ssp ~ no_insurance, \n        data = distance_ssp_clean\n        )\n\ndf_lm <- distance_ssp_clean |>\n    dplyr::mutate(\n        lm_fitted_values = lm_dist$fitted.values\n    ) |> \n    dplyr::mutate(\n        lm_residuals = lm_dist$residuals\n    )\n\ndf_lm |> \n      ggplot2::ggplot(\n          ggplot2::aes(\n                y = dist_ssp,\n                x = no_insurance\n          )\n      ) +\n      ggplot2::geom_smooth(\n          formula = y ~ x,\n          method = \"lm\",\n          se = FALSE,\n          ggplot2::aes(\n                linetype = \"Linear model\"\n          ), \n          color = \"gray60\", \n          linewidth = 1\n      ) +\n      ggplot2::geom_point(\n            ggplot2::aes(\n                color = \"Observation\"\n          ), \n          size = 1, \n          alpha = .6\n      ) +\n      ggplot2::geom_segment(\n          ggplot2::aes(\n              x = no_insurance,\n              y = dist_ssp,\n              xend = no_insurance,\n              yend = lm_fitted_values - lm_residuals\n          ),\n          color = \"grey\",\n          alpha = .6\n      )  +\n      ggplot2::labs(\n          y = \"Distance to nearest SSP facility\", \n          x = \"Percentage of people without health insurance \"\n          ) +\n      ggplot2::scale_linetype_manual(values = 2, name = \"\") +\n      ggplot2::scale_color_manual(values = \"#7463AC\", name = \"\")\n```\n\n::: {.cell-output-display}\n![](09-linear-regression_files/figure-html/tbl-regression-with-residuals-1.png){width=672}\n:::\n:::\n\n***\n\nThis is the replication of Figure 9.12, where no example code is available. After I had calculated the linear model I needed the position on the regression line (`lm_fitted_values` in my case) and the values of the residuals (`lm_residuals`). I couldn't use `base::summary(stats::lm())` because `fitted.values` are calculated only for the `lm` object (in my case `lm_dist`), which also has the residuals computed and included. \n::::\n:::::\n\n## Achievement 4: Slope interpretation and significance {#sec-chap09-achievement4}\n\n### Interpreting statistical significance of the slope\n\nThe output of @lst-est-lm-r for the linear model included a <a class='glossary' title='The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary)'>p-value</a> for the <a class='glossary' title='The slope is the increase in the dependent variable when the independent variable increases with one unit and all other independent variables remain the same. (Intercept, Slope in Regression)'>slope</a> (<2e16) and a p-value for the <a class='glossary' title='The intercept is the value of the dependent variables if all independent variables have the value zero. (Intercept, Slope in Regression)'>intercept</a> (0.221). \nThe statistical significance of the slope in linear regression is tested using a <a class='glossary' title='Wald test is the statistical test for comparing the value of the coefficient in linear or logistic regression to the hypothesized value of zero; the form is similar to a one-sample t-test, although some Wald tests use a t-statistic and others use a z-statistic as the test statistic. (SwR, Glossary)'>Wald test</a>, which is like a <a class='glossary' title='One-sample t-test, also known as the single-parameter t-test or single-sample t-test, is an inferential statistical test comparing the mean of a numeric variable to a population or hypothesized mean. (SwR, Glossary)'>one-sample t-test</a> where the hypothesized value of the slope is zero. To get the p-value from the regression model of distance to syringe program, the slope of 12.48 was compared to a hypothesized value of zero using the Wald test.\n\n\n#### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: {.callout-note}\n- **H0**: The slope of the line is equal to zero.\n- **HA**: The slope of the line is not equal to zero.\n:::\n\n#### NHST Step 2\n\nCompute the test statistic. \n\nThe test statistic for the Wald test in <a class='glossary' title='Ordinary least square regression (OLS) is a method of estimating a linear regression model that finds the regression line by minimizing the squared differences between each data point and the regression line. (Swr; Glossary)'>OLS</a> regression is the <a class='glossary' title='The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (Statistics How-To)'>t-statistic</a>.\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap09-wald-test}\n: Formula for the for the Wald test in OLS regression\n::::::\n:::\n::::{.my-theorem-container}\n$$\n\\begin{align*}\nt = &\\frac{b_{1}-0}{se_{b_{1}}} \\\\\nt = &\\frac{7.8190-0}{0.7734} = 10.11\n\\end{align*}\n$$ {#eq-chap09-wald-test}\n\n***\n\nNote that the formula is the same as the formula for the one-sample t-test from @eq-chap06-t-statistic, but with the slope of the regression model instead of the mean. The t-statistic, that was computed manually in @eq-chap09-wald-test can also be found in the model output of @lst-est-lm-r.\n\n::::\n:::::\n\n#### NHST Step 3\n\nReview and interpret the test statistics: \nCalculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\nThe p-value of the slope in @lst-est-lm-r is < 2e-16.\n\n#### NHST Step 4\n\nConclude and write report.\n\nThe p-value is < 0.01 and therefore the null hypothesis is rejected in favor of the alternate hypothesis that the slope is not equal to zero.\n\n::: {.callout #rep-chap09-1 style=\"color: darkgreen;\"}\n## Interpretation of the linear regression model (first draft)\n\nThe percentage of uninsured residents in a county is a statistically significant predictor of the distance to the nearest syringe program (b = 7.82; p < .05) in our sample. For every 1% increase in uninsured residents in a county, the predicted distance to the nearest syringe program increases by 7.82 miles.\n:::\n\n### Computing confidence intervals\n\n`stats::confint()` computes the confidence interval for the intercept and the slope.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-confint}\n: Confidence interval for regression parameters\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#tbl-confint .cell tbl-cap='Computing confidence interval for regression parameters'}\n\n```{.r .cell-code}\nstats::confint(\n        stats::lm(\n            formula = dist_ssp ~ no_insurance,\n            data = distance_ssp_clean, \n            na.action = na.exclude\n            )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                  2.5 %    97.5 %\n#> (Intercept)  -7.512773 32.472391\n#> no_insurance  6.299493  9.338435\n```\n\n\n:::\n:::\n\n***\n\nThe intercept is often reported but not interpreted because it does not usually contribute much to answering the research question.\n\n::: {.callout #rep-chap09-2 style=\"color: darkgreen;\"}\n## Interpretation of the linear regression model (second draft)\n\nThe percentage of uninsured residents in a county is a statistically significant predictor of the distance to the nearest syringe program (b = 7.82; p < .05). For every 1% increase in uninsured residents in a county, the predicted distance to the nearest syringe program increases by 7.82 miles. The value of the slope in the sample is 7.82, and the value of the slope is likely between 6.30 and 9.34 in the population that the sample came from (95% CI: 6.30–9.34). With every 1% increase in uninsured residents in a county, the nearest syringe program is between 6.30 and 9.34 more miles away. These results suggest that counties with a larger percentage of uninsured are farther from this resource, which may exacerbate existing health disparities.\n:::\n\n\n::::\n:::::\n\n### Making predcition\n\nPredicted values of $y$ are called `y-hat` and denoted $\\hat{y}$. The `stats::predict()` function can be used to find the predicted values for all observations, or for a specific value of the independent variable.\n\n::: {.callout-important #imp-ID}\n## `stats::predict()` as a generic function \n\n`stats::predict()` is a generic function for predictions from the results of various model fitting functions. The function invokes particular methods which depend on the class of the first argument. \n\nIn our case we have to consult the help page of `stats::predict.lm()`. R knows which method to apply so just using `stats::predict()` is enough to invoke the correct computation.\n\nMost prediction methods which are similar to those for linear models have an argument `newdata` specifying the first place to look for explanatory variables to be used for prediction. \n:::\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap09-prediction}\n: Using the model to make prediction\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Predict value\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-predict-one-value}\n: Predict distance for a county where 10% of people are uninsured\n::::::\n:::\n::::{.my-r-code-container}\n\n\n\n\n\n::: {.cell}\n\n```{#lst-chap09-predict-one-value .r .cell-code  lst-cap=\"Predict distance for a county where 10% of people are uninsured\"}\nstats::predict.lm(\n        stats::lm(\n            formula = dist_ssp ~ no_insurance,\n            data = distance_ssp_clean, \n            na.action = na.exclude\n            ),\n        newdata = data.frame(no_insurance = 10),\n        interval = \"confidence\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>        fit      lwr      upr\n#> 1 90.66945 82.42356 98.91534\n```\n\n\n:::\n:::\n\n\n\n\n***\n\nThe predicted distance to a syringe program from a county with 10% of people uninsured is 90.67 miles with a confidence interval for the prediction (sometimes called a prediction interval) of 82.42 to 98.92 miles.\n\n::::\n:::::\n\n\n###### Predict all\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap09-predict-all-values}\n: Find predictions for all observed values\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {#tbl-predict-all-values .cell tbl-cap='Predicted values for all observed x'}\n\n```{.r .cell-code}\npred_all <- tibble::as_tibble(\n    stats::predict.lm(\n        stats::lm(\n            formula = dist_ssp ~ no_insurance,\n            data = distance_ssp_clean, \n            na.action = na.exclude\n            ),\n        interval = \"confidence\"\n    )\n)\n\nglance_data(pred_all)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 10 × 4\n#>      obs   fit   lwr   upr\n#>    <int> <dbl> <dbl> <dbl>\n#>  1     1  52.4  39.2  65.5\n#>  2    49 245.  218.  273. \n#>  3    74  93.0  84.9 101. \n#>  4   122  78.9  69.5  88.3\n#>  5   146  87.5  79.0  96.1\n#>  6   153  75.0  65.2  84.9\n#>  7   228  78.2  68.7  87.6\n#>  8   321  82.1  73.0  91.1\n#>  9   485 160.  148.  173. \n#> 10   500  56.3  43.7  68.8\n```\n\n\n:::\n:::\n\n***\n\nThis is the same code as in @lst-chap09-predict-one-value but without the `newdata` line.\nSee @tbl-predict-all-values\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n***\n\n## Achievement 5: Model fit {#sec-chap09-achievement5}\n\n## Exercises (empty)\n\n## Glossary\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> amfAR </td>\n   <td style=\"text-align:left;\"> amfAR, the Foundation for AIDS Research, known until 2005 as the American Foundation for AIDS Research, is an international nonprofit organization dedicated to the support of AIDS research, HIV prevention, treatment education, and the advocacy of AIDS-related public policy. (&lt;a href=\"https://en.wikipedia.org/wiki/AmfAR\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Anderson-Darling </td>\n   <td style=\"text-align:left;\"> The Anderson-Darling Goodness of Fit Test (AD-Test) is a measure of how well your data fits a specified distribution. It’s commonly used as a test for normality. (&lt;a href=\"https://www.statisticshowto.com/anderson-darling-test/\"&gt;Statistics How-To&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Deterministic </td>\n   <td style=\"text-align:left;\"> A deterministic equation, or model, has one precise value for y for each value of x. (SwR, Chap09) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ExDA </td>\n   <td style=\"text-align:left;\"> Explorative Data Analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. (&lt;a href=\"https://en.wikipedia.org/wiki/Exploratory_data_analysis\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Fligner </td>\n   <td style=\"text-align:left;\"> The Fligner-Killeen test is a non-parametric test for homogeneity of group variances based on ranks. It is useful when the data is non-normal or when there are outliers. (&lt;a href=\"https://real-statistics.com/one-way-analysis-of-variance-anova/homogeneity-variances/fligner-killeen-test/\"&gt;Real Statistics Using Excel&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Homoscedasticity </td>\n   <td style=\"text-align:left;\"> Homoscedasticity is [homogeneity of variances], contrast is [Heteroscedasticity]. Homoscedasticity is an assumption of correlation and linear regression that requires that the variance of y be constant across all the values of x; visually, this assumption would show points along a fit line between x and y being evenly spread on either side of the line for the full range of the relationship. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Intercept </td>\n   <td style=\"text-align:left;\"> The intercept is the value of the dependent variables if all independent variables have the value zero. (&lt;a href=\"https://link.springer.com/referenceworkentry/10.1007/978-94-007-0753-5_1486\"&gt;Intercept, Slope in Regression&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Levene </td>\n   <td style=\"text-align:left;\"> Levene’s test is a statistical test to determine whether observed data meet the homogeneity of variances assumption; Levene’s test is used to test this assumption for t-tests and analysis of variance. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> OLS </td>\n   <td style=\"text-align:left;\"> Ordinary least square regression (OLS) is a method of estimating a linear regression model that finds the regression line by minimizing the squared differences between each data point and the regression line. (Swr; Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> One-sample </td>\n   <td style=\"text-align:left;\"> One-sample t-test, also known as the single-parameter t-test or single-sample t-test, is an inferential statistical test comparing the mean of a numeric variable to a population or hypothesized mean. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p-value </td>\n   <td style=\"text-align:left;\"> The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Pearson </td>\n   <td style=\"text-align:left;\"> Pearson’s r is a statistic that indicates the strength and direction of the relationship between two numeric variables that meet certain assumptions. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Q-Q-Plot </td>\n   <td style=\"text-align:left;\"> A quantile-quantile plot is a visualization of data using probabilities to show how closely a variable follows a normal distribution. (SwR, Glossary) This plot is made up of points below which a certain percentage of the observations fall. On the x-axis are normally distributed values with a mean of 0 and a standard deviation of 1. On the y-axis are the observations from the data. If the data are normally distributed, the values will form a diagonal line through the graph. (SwR, chapter 6) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Residually </td>\n   <td style=\"text-align:left;\"> Residuals are the differences between the observed values and the predicted values. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Shapiro-Wilk </td>\n   <td style=\"text-align:left;\"> The Shapiro-Wilk test is a statistical test to determine or confirm whether a variable has a normal distribution; it is sensitive to small deviations from normality and not useful for sample sizes above 5,000 because it will nearly always find non-normality. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Simple Linear Regression </td>\n   <td style=\"text-align:left;\"> Simple does not mean easy; instead, it is the term used for a statistical model used to predict or explain a continuous outcome by a single predictor. (SwR, Glossary, Chap09) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Slope </td>\n   <td style=\"text-align:left;\"> The slope is the increase in the dependent variable when the independent variable increases with one unit and all other independent variables remain the same. (&lt;a href=\"https://link.springer.com/referenceworkentry/10.1007/978-94-007-0753-5_1486\"&gt;Intercept, Slope in Regression&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Spearman </td>\n   <td style=\"text-align:left;\"> Spearman’s rho a statistical test used to examine the strength, direction, and significance of the relationship between two numeric variables when they do not meet the assumptions for [Pearson]’s r. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> SSP </td>\n   <td style=\"text-align:left;\"> SSP stands for syringe services program (SwR) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> T-Statistic </td>\n   <td style=\"text-align:left;\"> The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (&lt;a href=\"https://www.statisticshowto.com/t-statistic/\"&gt;Statistics How-To&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Wald </td>\n   <td style=\"text-align:left;\"> Wald test is the statistical test for comparing the value of the coefficient in linear or logistic regression to the hypothesized value of zero; the form is similar to a one-sample t-test, although some Wald tests use a t-statistic and others use a z-statistic as the test statistic. (SwR, Glossary) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n## Session Info {.unnumbered}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\nSession Info\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.3.3 (2024-02-29)\n#>  os       macOS Sonoma 14.4.1\n#>  system   x86_64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2024-04-15\n#>  pandoc   3.1.12.3 @ /usr/local/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package      * version    date (UTC) lib source\n#>  abind          1.4-5      2016-07-21 [1] CRAN (R 4.3.0)\n#>  backports      1.4.1      2021-12-13 [1] CRAN (R 4.3.0)\n#>  base64enc      0.1-3      2015-07-28 [1] CRAN (R 4.3.0)\n#>  broom          1.0.5      2023-06-09 [1] CRAN (R 4.3.0)\n#>  car            3.1-2      2023-03-30 [1] CRAN (R 4.3.0)\n#>  carData        3.0-5      2022-01-06 [1] CRAN (R 4.3.0)\n#>  class          7.3-22     2023-05-03 [2] CRAN (R 4.3.3)\n#>  cli            3.6.2      2023-12-11 [1] CRAN (R 4.3.0)\n#>  colorspace     2.1-1      2024-01-03 [1] R-Forge (R 4.3.2)\n#>  commonmark     1.9.1      2024-01-30 [1] CRAN (R 4.3.2)\n#>  curl           5.2.1      2024-03-01 [1] CRAN (R 4.3.2)\n#>  DBI            1.2.2      2024-02-16 [1] CRAN (R 4.3.2)\n#>  digest         0.6.35     2024-03-11 [1] CRAN (R 4.3.2)\n#>  dplyr          1.1.4      2023-11-17 [1] CRAN (R 4.3.0)\n#>  e1071          1.7-14     2023-12-06 [1] CRAN (R 4.3.0)\n#>  evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.0)\n#>  fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.0)\n#>  farver         2.1.1      2022-07-06 [1] CRAN (R 4.3.0)\n#>  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#>  forcats        1.0.0      2023-01-29 [1] CRAN (R 4.3.0)\n#>  generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#>  GGally         2.2.1      2024-02-14 [1] CRAN (R 4.3.2)\n#>  ggplot2        3.5.0      2024-02-23 [1] CRAN (R 4.3.2)\n#>  ggstats        0.5.1      2023-11-21 [1] CRAN (R 4.3.0)\n#>  glossary     * 1.0.0.9000 2023-08-12 [1] Github (debruine/glossary@819e329)\n#>  glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.0)\n#>  gridExtra      2.3        2017-09-09 [1] CRAN (R 4.3.0)\n#>  gtable         0.3.4      2023-08-21 [1] CRAN (R 4.3.0)\n#>  haven          2.5.4      2023-11-30 [1] CRAN (R 4.3.2)\n#>  highr          0.10       2022-12-22 [1] CRAN (R 4.3.0)\n#>  hms            1.1.3      2023-03-21 [1] CRAN (R 4.3.0)\n#>  htmltools      0.5.8      2024-03-25 [1] CRAN (R 4.3.2)\n#>  htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.3.0)\n#>  jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.0)\n#>  kableExtra     1.4.0      2024-01-24 [1] CRAN (R 4.3.2)\n#>  knitr          1.45       2023-10-30 [1] CRAN (R 4.3.0)\n#>  labeling       0.4.3      2023-08-29 [1] CRAN (R 4.3.0)\n#>  labelled       2.12.0     2023-06-21 [1] CRAN (R 4.3.0)\n#>  lattice        0.22-6     2024-03-20 [2] CRAN (R 4.3.2)\n#>  lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.3.0)\n#>  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#>  markdown       1.12       2023-12-06 [1] CRAN (R 4.3.0)\n#>  Matrix         1.6-5      2024-01-11 [1] CRAN (R 4.3.0)\n#>  mgcv           1.9-1      2023-12-21 [1] CRAN (R 4.3.0)\n#>  mitools        2.4        2019-04-26 [1] CRAN (R 4.3.0)\n#>  munsell        0.5.0      2018-06-12 [1] CRAN (R 4.3.0)\n#>  nlme           3.1-164    2023-11-27 [1] CRAN (R 4.3.2)\n#>  nortest        1.0-4      2015-07-30 [1] CRAN (R 4.3.0)\n#>  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#>  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#>  plyr           1.8.9      2023-10-02 [1] CRAN (R 4.3.0)\n#>  proxy          0.4-27     2022-06-09 [1] CRAN (R 4.3.0)\n#>  purrr          1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n#>  R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#>  RColorBrewer   1.1-3      2022-04-03 [1] CRAN (R 4.3.0)\n#>  Rcpp           1.0.12     2024-01-09 [1] CRAN (R 4.3.0)\n#>  repr           1.1.7      2024-03-22 [1] CRAN (R 4.3.3)\n#>  rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.0)\n#>  rmarkdown      2.26       2024-03-05 [1] CRAN (R 4.3.2)\n#>  rstudioapi     0.16.0     2024-03-24 [1] CRAN (R 4.3.2)\n#>  rversions      2.1.2      2022-08-31 [1] CRAN (R 4.3.0)\n#>  scales         1.3.0      2023-11-28 [1] CRAN (R 4.3.2)\n#>  sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n#>  skimr          2.1.5      2022-12-23 [1] CRAN (R 4.3.0)\n#>  stringi        1.8.3      2023-12-11 [1] CRAN (R 4.3.0)\n#>  stringr        1.5.1      2023-11-14 [1] CRAN (R 4.3.0)\n#>  survey         4.4-2      2024-03-20 [1] CRAN (R 4.3.2)\n#>  survival       3.5-8      2024-02-14 [2] CRAN (R 4.3.3)\n#>  svglite        2.1.3      2023-12-08 [1] CRAN (R 4.3.0)\n#>  systemfonts    1.0.6      2024-03-07 [1] CRAN (R 4.3.2)\n#>  tableone       0.13.2     2022-04-15 [1] CRAN (R 4.3.0)\n#>  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#>  tidyr          1.3.1      2024-01-24 [1] CRAN (R 4.3.2)\n#>  tidyselect     1.2.1      2024-03-11 [1] CRAN (R 4.3.2)\n#>  utf8           1.2.4      2023-10-22 [1] CRAN (R 4.3.0)\n#>  vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n#>  viridisLite    0.4.2      2023-05-02 [1] CRAN (R 4.3.0)\n#>  withr          3.0.0      2024-01-16 [1] CRAN (R 4.3.0)\n#>  xfun           0.43       2024-03-25 [1] CRAN (R 4.3.2)\n#>  xml2           1.3.6      2023-12-04 [1] CRAN (R 4.3.0)\n#>  yaml           2.3.8      2023-12-11 [1] CRAN (R 4.3.0)\n#>  zoo            1.8-12     2023-04-13 [1] CRAN (R 4.3.0)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.3-x86_64/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n\n\n::::\n:::::\n",
    "supporting": [
      "09-linear-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}