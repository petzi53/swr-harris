{
  "hash": "48520cec3d229b84c9fe1c7ebba29c71",
  "result": {
    "engine": "knitr",
    "markdown": "# Binary Logistic regression {#sec-chap10}\n\n\n\n\n\n## Achievements to unlock\n\n::: {#obj-chap10}\n::: my-objectives\n::: my-objectives-header\nObjectives for chapter 10\n:::\n\n::: my-objectives-container\n**SwR Achievements**\n\n-   **Achievement 1**: Using exploratory data analysis before developing\n    a logistic regression models (@sec-chap10-achievement1)\n-   **Achievement 2**: Understanding the binary logistic regression\n    statistical model (@sec-chap10-achievement2)\n-   **Achievement 3**: Estimating a simple logistic regression model and\n    interpreting predictor significance and interpretation (@sec-chap10-achievement3)\n-   **Achievement 4**: Computing and interpreting two measures of model\n    fit (@sec-chap10-achievement4)\n-   **Achievement 5**: Estimating a larger logistic regression model\n    with categorical and continuous predictors (@sec-chap10-achievement5)\n-   **Achievement 6**: Interpreting the results of a larger logistic\n    regression model (@sec-chap10-achievement6)\n-   **Achievement 7**: Checking logistic regression assumptions and\n    using diagnostics to identify outliers and influential values (@sec-chap10-achievement7)\n-   **Achievement 8**: Using the model to predict probabilities for\n    observations that are outside the data set (@sec-chap10-achievement8)\n-   **Achievement 9**: Adding and interpreting interaction terms in\n    logistic regression (@sec-chap10-achievement9)\n-   **Achievement 10**: Using the likelihood ratio test to compare two\n    nested logistic regression models (@sec-chap10-achievement10)\n:::\n:::\n\nAchievements for chapter 10\n:::\n\n## The perplexing libraries problem\n\nHarris defines <a class='glossary' title='The term “digital divide” refers to the gap between individuals, households, businesses and geographic areas at different socio-economic levels with regard to their opportunities to access information and communication technologies (ICTs). (OECD Library)'>digital divide</a> broader and includes both limited access\nto information and communication technologies (ICT) and a deficit in the\nability to use information gained through access to\nICTs[^10-logistic-regression-1]. \n\n[^10-logistic-regression-1]: My current research indicates that mostly\n    only the first problem (limited access) falls under the definition\n    ([@wikipedia2024; @oecd2001].\n\nPeople were more likely to fall into this digital divide if they were poor, a racial minority, had limited education, had a disability, or lived in an area with low population density. The digital divide often exacerbated other problems like finding an employment either by not searching relevant offers using the internet or not getting the job because of missing ICT skills.\n\nThe question this chapter tries to answer: \"Which characteristics are associated with library use?\"\n\n\n\n\n## Resources & Chapter Outline\n\n### Data, codebook, and R packages {#sec-chap10-data-codebook-packages}\n\n::: my-resource\n::: my-resource-header\nData, codebook, and R packages for learning about descriptive statistics\n:::\n\n::: my-resource-container\n**Data**\n\nTwo options for accessing the data:\n\n1. Download the cleaned data set `pew_libraries_2016_cleaned_ch10.csv` from <https://edge.sagepub.com/harris1e>. \n2. Follow the instructions in Box 10.1 to import and clean `pew_libraries_2016_ch10.csv` from <https://edge.sagepub.com/harris1e> or download from the original Internet data source and clean.\n\nI am using the first option because there is nothing new for me to import and clean data files.\n\n**Codebook**\n\nTwo options:\n\n1. Download the `pew_libraries_2016_codebook_ch10.docx` codebook file from <https://edge.sagepub.com/harris1e>.\n2. Use the version that comes with the raw data file from Pew Research Center (https://www.pewinternet.org/dataset/march2016-libraries/)\n\n**Packages**\n\n1.  Packages used with the book (sorted alphabetically)\n\n-   {**car**}: @pak-car (John Fox)\n-   {**lmtest**}: @pak-lmtest (Achim Zeileis) \n-   {**odds.n.ends**}: @pak-odds.n.ends (Jenine Harris) \n-   {**tableone**}: @pak-tableone (Kazuki Yoshida) \n-   {**tidyverse**}: @pak-tidyverse (Hadley Wickham)\n\n2.  My additional packages (sorted alphabetically)\n\n-   {**skimr**}: @pak-skimr (Elin Waring)\n\n:::\n:::\n\n## Achievement 1: EDA {#sec-chap10-achievement1}\n\n### Get, show, and recode data\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap09-eda}\n: EDA: Get, show and recode data\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Get data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-get-data}\n: Import data from .csv file\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-get-data}\n\n::: {.cell}\n\n```{.r .cell-code}\n## run only once (manually)\ntbl10 <- readr::read_csv(\n    \"data/chap10/pew_libraries_2016_cleaned_ch10.csv\",\n    col_types = \"nffffffff\"\n)\n\nsave_data_file(\"chap10\", tbl10, \"tbl10.rds\")\n```\n:::\n\nGet data for chapter 10\n:::\n\n(*For this R code chunk is no output available*)\n\n***\n\nIn my first import trial it turned out that all the factor variables are imported as character variables. So I had to add the columns specifications `col_types = \"nffffffff\"`.\n\n::::\n:::::\n\n\n###### Show data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-show-data}\n: Show raw data for chapter 10\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-show-data}    \n\n::: {.cell}\n\n```{.r .cell-code}\ntbl10 <- base::readRDS(\"data/chap10/tbl10.rds\")\n\nskimr::skim(tbl10)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |      |\n|:------------------------|:-----|\n|Name                     |tbl10 |\n|Number of rows           |1601  |\n|Number of columns        |9     |\n|_______________________  |      |\n|Column type frequency:   |      |\n|factor                   |8     |\n|numeric                  |1     |\n|________________________ |      |\n|Group variables          |None  |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                    |\n|:-------------|---------:|-------------:|:-------|--------:|:-----------------------------|\n|sex           |         0|          1.00|FALSE   |        2|mal: 833, fem: 768            |\n|parent        |         5|          1.00|FALSE   |        2|not: 1205, par: 391           |\n|disabled      |         8|          1.00|FALSE   |        2|no: 1340, yes: 253            |\n|uses.lib      |         0|          1.00|FALSE   |        2|no: 809, yes: 792             |\n|ses           |         0|          1.00|FALSE   |        3|med: 1197, low: 246, hig: 158 |\n|raceth        |       140|          0.91|FALSE   |        3|Non: 1097, His: 194, Non: 170 |\n|educ          |         0|          1.00|FALSE   |        3|HS : 772, Fou: 658, < H: 171  |\n|rurality      |        14|          0.99|FALSE   |        3|rur: 879, sub: 355, urb: 353  |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|  mean|    sd| p0| p25| p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|-----:|-----:|--:|---:|---:|---:|----:|:-----|\n|age           |        30|          0.98| 49.31| 18.85| 16|  33|  51|  64|   95|▆▅▇▆▁ |\n\n\n:::\n:::\n\n\nSkim raw data for chapter 10\n:::\n\n***\n\n\n\nI have used the {**skimr**} package instead of {**tableone**}. It wouldn't be necessary to plot a histogram for `age` to decide if the mean or median has to be used. The tiny histogram at the right side of the `age` line already shows that age is not normally distributed. But for the sake of practice I will create the histogram in the next tab.\n\n::::\n:::::\n\n###### age\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-age-hist}\n: Show age distribution\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-age-dist}\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_hist_dnorm(tbl10, tbl10$age)\n```\n\n::: {.cell-output-display}\n![](10-logistic-regression_files/figure-html/age-dist-1.png){width=672}\n:::\n:::\n\n\nThe distribution of age in the 2016 Pew Research Center library use data set\n:::\n\n::::\n:::::\n\n###### {tableone}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-desc-stats}\n: Table of descriptive statistics\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-desc-stats}\n\n::: {.cell}\n\n```{.r .cell-code}\ntab_desc <- tableone::CreateTableOne(\n    data = tbl10,\n    strata = 'uses.lib',\n    vars = c(\"age\", \"sex\", \"parent\", \"disabled\",\n             \"ses\", \"raceth\", \"educ\", \"rurality\"))\n\nprint(tab_desc,\n      nonnormal = 'age',\n      showAllLevels = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                     Stratified by uses.lib\n#>                      level                    yes                 \n#>   n                                             792               \n#>   age (median [IQR])                          49.00 [31.00, 62.00]\n#>   sex (%)            female                     438 (55.3)        \n#>                      male                       354 (44.7)        \n#>   parent (%)         parent                     222 (28.2)        \n#>                      not parent                 566 (71.8)        \n#>   disabled (%)       no                         679 (86.3)        \n#>                      yes                        108 (13.7)        \n#>   ses (%)            medium                     585 (73.9)        \n#>                      high                        91 (11.5)        \n#>                      low                        116 (14.6)        \n#>   raceth (%)         Non-Hispanic White         540 (75.6)        \n#>                      Hispanic                    83 (11.6)        \n#>                      Non-Hispanic Black          91 (12.7)        \n#>   educ (%)           HS to 2-year degree        341 (43.1)        \n#>                      Four-year degree or more   382 (48.2)        \n#>                      < HS                        69 ( 8.7)        \n#>   rurality (%)       suburban                   196 (24.9)        \n#>                      rural                      401 (51.0)        \n#>                      urban                      189 (24.0)        \n#>                     Stratified by uses.lib\n#>                      no                   p      test   \n#>   n                    809                              \n#>   age (median [IQR]) 53.00 [35.00, 65.00]  0.001 nonnorm\n#>   sex (%)              330 (40.8)         <0.001        \n#>                        479 (59.2)                       \n#>   parent (%)           169 (20.9)          0.001        \n#>                        639 (79.1)                       \n#>   disabled (%)         661 (82.0)          0.024        \n#>                        145 (18.0)                       \n#>   ses (%)              612 (75.6)          0.088        \n#>                         67 ( 8.3)                       \n#>                        130 (16.1)                       \n#>   raceth (%)           557 (74.6)          0.110        \n#>                        111 (14.9)                       \n#>                         79 (10.6)                       \n#>   educ (%)             431 (53.3)         <0.001        \n#>                        276 (34.1)                       \n#>                        102 (12.6)                       \n#>   rurality (%)         159 (19.9)          0.002        \n#>                        478 (59.7)                       \n#>                        164 (20.5)\n```\n\n\n:::\n:::\n\n\nDescriptive statistics with bivariate tests using {**tableone**}\n:::\n***\n\n:::::{.my-remark}\n:::{.my-remark-header}\n:::::: {#rem-chap10-test-all-variables-together}\n: Printing bivariate tests for all variables --- a <a class='glossary' title='Questionable Research Practice (QRP) is a research practice that introduces bias, usually in pursuit of statistical significance; an example of such practices might be dropping or recoding values or variables solely to improve a model fit statistic. (SwR, GLossary)'>QRP</a>\n::::::\n:::\n::::{.my-remark-container}\nI am not feeling comfortable to use {**tableone**} to print descriptive statistics with bivariate test for all variables. Besides the mentioned danger of a <a class='glossary' title='Questionable Research Practice (QRP) is a research practice that introduces bias, usually in pursuit of statistical significance; an example of such practices might be dropping or recoding values or variables solely to improve a model fit statistic. (SwR, GLossary)'>questionable research practice</a> (QRP) in looking for statistically significance I would like to inspect the relationships more slowly and to see more details. I think at a minimum one should examine plots of the bivariate correlations. \n\nI have the same skepticism about the advice to \"examine the frequencies and percentages in the table to identify some possible categories that may be driving the significant results for the bivariate tests\". I think one should be guided to inspect more in detail in the first instance by theoretical assumptions, an approach that is fairly well demonstrated with Bayesian model design in \"Statistical Rethinking\" [@mcelreath2020]. \n\nTo facilitate <a class='glossary' title='Explorative Data Analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. (Wikipedia)'>exploratory data analysis</a> one could use packages that combine the analysis of different variables in one go, as my experiments with `GGally::ggpairs()` in @lst-chap09-plot-ggpairs or with `ggfortify::autoplot()` in @lst-chap09-test-ggfortify have shown. But this approach gets too overwhelming when there are more than 5-6 variables as I will demonstrate in @lst-chap10-bivariate-data with {**GGally**).\n\n\n::::\n:::::\n\n\n\n\n\n::::\n:::::\n\n###### {GGally}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-bivariate-eda}\n: Bivariate exploratory data analysis for variables of chapter 10\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-bivariate-data}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl10 |> \n    GGally::ggpairs()\n```\n\n::: {.cell-output-display}\n![](10-logistic-regression_files/figure-html/plot-ggpairs-1.png){width=960}\n:::\n:::\n\n\nBivariate exploratory data analysis for variables of chapter 10\n:::\n\n*** \n\nThere are too many plots (variables) in this example. I could divide easily the amount of variables into different patches as demonstrated in [Columns and Mapping](https://ggobi.github.io/ggally/articles/ggpairs.html#columns-and-mapping) and inspect these results in more details. But in order to get all variations I have to plan the approach systematically which destroys the advantage of automatic plotting.\n\n::::\n:::::\n\n\n\n:::\n\n::::\n:::::\n\n\n## Achievement 2: Understanding binary logistic regression {#sec-chap10-achievement2}\n\nBinary logistic regression follows a similar format and process as linear regression (@sec-chap09), but the outcome or dependent variable is binary. Because the outcome is binary, or categorical consisting of two categories, the model predicts the probability that a person is in one of the two categories. In this chapter we want to predict the library use `uses.lib`.\n\nBecause of the binary outcome the linear regression model would not work since it requires a continuous outcome. However, the <a class='glossary' title='Linear regression is used to predict the value of an outcome variable Y based on one or more input predictor variables X. The aim is to establish a linear relationship (a mathematical formula) between the predictor variable(s) and the response variable, so that, we can use this formula to estimate the value of the response Y, when only the predictors (Xs) values are known. (r-statistics.co) (Chap.4)'>linear regression</a> statistical model can be transformed using <a class='glossary' title='Logit transformations are transformations that takes the log value of p/(1-p); this transformation is often used to normalize percentage data and is used in the logistic model to transform the outcome. (SwR, Glossary)'>logit transformations</a> in order to be useful for modeling binary outcomes.\n\n### Formula of the logistic model\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap10-logistic-model}\n: Formula for the statistical form of the logistic model\n::::::\n:::\n::::{.my-theorem-container}\n$$\n\\begin{align*}\np(y) = \\frac{1}{1+e^{-(b_{0}+b_{1}x_{1}+b_{2}x_{2})}}\n\\end{align*}\n$$ {#eq-chap10-logistic-model}\n\n***\n\n- $y$: binary outcome variable (e.g., library use) \n- $p(y)$: probability of the outcome (e.g., probability of library use) \n- $b_{0}$: y-intercept \n- $x_{1}$ and $x_{2}$: predictors of the outcome (e.g., age, rurality) \n- $b_{1}$ and $b_{2}$: coefficients for $x_{1}$ and $x_{2}$\n\n::::\n:::::\n\n### Logistic function\n\nThe logistic function has a <a class='glossary' title='A sigmoid function is any mathematical function whose graph has a characteristic S-shaped curve or sigmoid curve. (Wikipedia)'>sigmoid</a> shape that stretches from $–∞$ to $∞$ on the x-axis and from $0$ to $1$ on the y-axis. The function can take any value along the x-axis and give the corresponding value between $0$ and $1$ on the y-axis.\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap10-logistic-function}\n: Formula of the logistic function\n::::::\n:::\n::::{.my-theorem-container}\n$$\n\\begin{align*}\n\\sigma(t) &= \\frac{e^t}{1+e^t} =\\\\\n&= \\frac{1}{1 + e^{-t}}\n\\end{align*}\n$$ {#eq-chap10-logistic-function}\n\n***\n\n$t$: value along the $x$-axis of the function\n$\\sigma$: value of $y$ for a specific value of $t$, or the probability of $y$ given $t$.\n\nIn the case of logistic regression, the value of $t$ will be the right-hand side of the regression model, which looks something like $β_{0} + β_{1}x$, where $x$ is an independent variable, $β_{1}$ is the coefficient (rather than slope) for that variable, and $β_{0}$ is the constant (rather than $y$-intercept).\n\nSubstituting this regression model for $t$ in the logistic function:\n\n$$\n\\begin{align*}\np(y) = \\frac{1}{1 + e^{-(β_{0} + β_{1}x)}}\n\\end{align*}\n$$ {#eq-chap10-logistic-function}\n\n::::\n:::::\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap10-logistic-function}\n: Drawing shape of logistic function empty and with example data points\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Shape\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-logistic-function-shape}\n: Shape of the logistic function\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-logistic-function-shape}\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot(data = data.frame(x = c(-5, 5)), \n         ggplot2::aes(x)) +\n    ggplot2::stat_function(fun = \n           function(x) base::exp(x)/(1 + base::exp(x)), n = 100,\n           linewidth = 1,\n           ggplot2::aes(color = \"Logistic function\")\n           ) +\n    ggplot2::scale_x_continuous(\n        labels = seq.int(10, 35, length.out = 5)\n        ) +\n    ggplot2::scale_color_manual(\n        name = \"\",\n        values = \"hotpink2\"\n    ) +\n    ggplot2::labs(\n        x = \"Values of input\",\n        y = \"Value of outcome\"\n    )\n```\n\n::: {.cell-output-display}\n![](10-logistic-regression_files/figure-html/logistic-function-shape-1.png){width=672}\n:::\n:::\n\nShape of the logistic function\n:::\n\n::::\n:::::\n\n\n\n###### Example with data points\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-logistic-function-data}\n: Example of logistic function with data points\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-logistic-function-data}    \n\n::: {.cell}\n\n```{.r .cell-code}\n## generate fake data frame\nx1 = c(10.2, 13.0, 14.1, 14.3, 14.5, 14.7, 15.0, 15.5, \n      16.1, 17.3, 19.0, 19.2, 19.8, 21.0, 26.5)\ny1 = rep(0, 15)\nx2 = c(17.8, 18.2, 19.0, 21.4, 21.5, 22.7, 24.0, 27.2, 31.0, 32.4, 33.8)\ny2 = rep(1, 11)\ntbl <-  tibble::tibble(x = c(x1, x2),\n                     y = c(y1, y2)) |> \n    dplyr::arrange(x)\n\n## draw logistic function with faked data points\nggplot2::ggplot( \n        data = tbl,\n        ggplot2::aes(x = x, y = y,\n                 color = \"Logistic function\")\n    ) +\n    ggplot2::stat_smooth(\n        data = tbl,\n        formula = y ~ x,\n        method = \"glm\",\n        se = FALSE,\n        method.args = list(family = binomial)\n    ) +\n    ggplot2::geom_point(\n        ggplot2::aes(alpha = \"Observation\"),\n        color = \"grey41\"\n    ) +\n    ggplot2::labs(\n        x = \"Values of input\",\n        y = \"Value of outcome\"\n    ) +\n    ggplot2::scale_color_manual(\n        name = \"\",\n        values = \"hotpink2\"\n    ) +\n    ggplot2::scale_alpha_manual(\n        name = \"\",\n        values = 0.5\n    )\n```\n\n::: {.cell-output-display}\n![](10-logistic-regression_files/figure-html/logistic-function-data-1.png){width=672}\n:::\n:::\n\n\nExample of logistic function with data points\n:::\n\n***\n\n::::\n:::::\n\n###### Probability of outcome\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-logistic-function-data-annotated}\n: Example of logistic function showing probability of outcome for x = 20\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-logistic-function-data-annotated}    \n\n::: {.cell}\n\n```{.r .cell-code}\n## generate fake data frame\nx1 = c(10.2, 13.0, 14.1, 14.3, 14.5, 14.7, 15.0, 15.5, \n      16.1, 17.3, 19.0, 19.2, 19.8, 21.0, 26.5)\ny1 = rep(0, 15)\nx2 = c(17.8, 18.2, 19.0, 21.4, 21.5, 22.7, 24.0, 27.2, 31.0, 32.4, 33.8)\ny2 = rep(1, 11)\ntbl <-  tibble::tibble(x = c(x1, x2),\n                     y = c(y1, y2)) |> \n    dplyr::arrange(x)\n\n## draw logistic function with faked data points\nggplot2::ggplot( \n    data = tbl,\n    ggplot2::aes(x = x, y = y,\n                 color = \"Logistic function\")\n    ) +\n    ggplot2::stat_smooth(\n        data = tbl,\n        formula = y ~ x,\n        method = \"glm\",\n        se = FALSE,\n        method.args = list(family = binomial),\n        ggplot2::aes(linetype = \"predictor = 20 and\\noutcome = .44 example\")\n    ) +\n    ggplot2::geom_point(\n        ggplot2::aes(alpha = \"Observation\"),\n        color = \"grey41\"\n        ) +\n    ggplot2::geom_segment(\n        x = 20, xend = 20,\n        y = 0, yend = .44,\n        color = \"#1f6fca\",\n        linetype = \"dashed\"\n    ) +\n    ggplot2::geom_segment(\n        x = 10, xend = 20,\n        y = .44, yend = .44,\n        color = \"#1f6fca\",\n        linetype = \"dashed\"\n    ) +\n    ggplot2::labs(\n        x = \"Values of input\",\n        y = \"Value of outcome\"\n    ) +\n    ggplot2::scale_color_manual(\n        name = \"\",\n        values = \"hotpink2\"\n    ) +\n    ggplot2::scale_alpha_manual(\n        name = \"\",\n        values = 0.5\n    ) +\n    ggplot2::scale_linetype_manual(\n        name = \"\",\n        values = c(\"dashed\", \"dashed\")\n    ) +\n    ggplot2::annotate(\"text\", x = 10, y = .48, label = \"0.44\", color = \"#1f6fca\" ) +\n    ggplot2::annotate(\"text\", x = 20.5, y = -.02, label = \"20\", color = \"#1f6fca\" )\n```\n\n::: {.cell-output-display}\n![](10-logistic-regression_files/figure-html/logistic-function-data-annotated-1.png){width=672}\n:::\n:::\n\n\nExample of logistic function showing a probability of outcome for $x = 20$\n:::\n\n***\n\nLet's assume that the above graphic is a model for predicting library use from age. Then we can interpret it as a 44% probability of library use for a 20-year-old. Since 44% is lower than a 50% probability of the value of $y$, the model is predicting that the 20-year-old does not have the outcome. So, if the outcome is library use, the logistic model would predict this 20-year-old was not a library user. \n\n\n::::\n:::::\n\n:::\n\n\n\n\n\n::::\n:::::\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap10-odds}\n: Formula of odds related to probability\n::::::\n:::\n::::{.my-theorem-container}\n$$\n\\begin{align*}\nodds = \\frac{probability}{1-probability}\n\\end{align*}\n$$ {#eq-chap10-odds}\n\nSubstituting the logistic model from @eq-chap10-logistic-model:\n\n$$\n\\begin{align*}\nodds &= \\frac{\\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1}x)}}}{1- \\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1}x)}}} = \ne^{\\beta_{0} + \\beta_{1}x}\n\\end{align*}\n$$ {#eq-chap10-odds-logistic-model}\n\nTo be equivalent to the interpretation of the coefficients in linear regression, however, there is one more step. That is, what is the increase or decrease in the odds of the outcome with a one-unit increase in $x$?\n\n\n$$\n\\begin{align*}\nOR = \\frac{e^{b_0+b_{1}(x+1)}}{e^{b_0+b_{1}x}}\n\\end{align*}\n$$ {#eq-chap10-odds-ratio}\n\n::::\n:::::\n\n@eq-chap10-odds-ratio shows that for every one-unit increase in the independent variable $x$, the odds of the outcome increase or decrease by $e^{b_1}$. Taking $e$ to the power of $b_1$ is referred to as exponentiating $b_1.$ After a model is estimated, the analyst will usually exponentiate the b value(s) in order to report odds ratios describing the relationships between each predictor and the outcome.\n\n## Achievement 3: Interpreting a simple logistic regression {#sec-chap10-achievement3}\n\n### NHST\n\n#### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: {.callout-note}\n- **H0**: The model is no better than the baseline at predicting library use.\n- **HA**: The model is better than the baseline at predicting library use.\n:::\n\n#### NHST Step 2\n\nCompute the test statistic. \n\nThe generalized linear model (<a class='glossary' title='A generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. (Wikipedia). the binary logistic regression model is one example of a generalized linear model. (SwR, Glossary)'>GLM</a>) or `stats::glm()` function can be used to estimate a binary logistic regression model. The model is generalized because it starts with the basic linear model and generalizes to other situations.\n\nThe `stats::glm()` function will treat the first category as the reference group (the group *without* the outcome) and the second category as the group *with* the outcome. To see the order of `uses.lib`, use `base::levels()` to show the levels in order and use `stats:relevel()` to re-order levels.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-ID-text}\n: Numbered Example Title\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Reorder `uses.lib`\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-uses-lib-order}\n: Reverse the order of `uses.lib`\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-uses-lib-order}\n\n::: {.cell}\n\n```{.r .cell-code}\nglue::glue(\"Order of uses.lib levels originally\")\nbase::levels(tbl10$uses.lib)\n\ntbl10.1 <- tbl10 |> \n    dplyr::mutate(uses.lib =\n        forcats::fct_rev(uses.lib)\n    )\n\nsave_data_file(\"chap10\", tbl10.1, \"tbl10.1.rds\")\n\n\nglue::glue(\"\")\nglue::glue(\"---------------------------------------\")\nglue::glue(\"Order of uses.lib levels reversed\")\nbase::levels(tbl10.1$uses.lib)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Order of uses.lib levels originally\n#> [1] \"yes\" \"no\" \n#> \n#> ---------------------------------------\n#> Order of uses.lib levels reversed\n#> [1] \"no\"  \"yes\"\n```\n\n\n:::\n:::\n\n\nReversing order of `uses.lib`\n:::\n\n::::\n:::::\n\nIn contrast to the book the order of my levels for the `uses.lib` data is reversed. It makes more sense to ask about if one uses the library, e.g., \"yes\" has to be the second category. To get the same order as in the book, I had to reverse the order. Instead of using `stats:relevel()` I reversed the order with `forcats::fct_rev()`.\n\n\n\n\n###### lm10.1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-lm10.1}\n: Estimate library use model and print results\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-lm10.1}    \n\n::: {.cell}\n\n```{.r .cell-code}\nlm10.1 <- \n    stats::glm(\n        formula = uses.lib ~ age, \n        data = tbl10.1, \n        family = binomial(link = \"logit\")) \n\nsave_data_file(\"chap10\", lm10.1, \"lm10.1.rds\")\n\nbase::summary(lm10.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> stats::glm(formula = uses.lib ~ age, family = binomial(link = \"logit\"), \n#>     data = tbl10.1)\n#> \n#> Coefficients:\n#>              Estimate Std. Error z value Pr(>|z|)   \n#> (Intercept)  0.403785   0.142194   2.840  0.00452 **\n#> age         -0.008838   0.002697  -3.277  0.00105 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 2177.5  on 1570  degrees of freedom\n#> Residual deviance: 2166.7  on 1569  degrees of freedom\n#>   (30 observations deleted due to missingness)\n#> AIC: 2170.7\n#> \n#> Number of Fisher Scoring iterations: 3\n```\n\n\n:::\n:::\n\n\nEstimate the simple library use model and print (= summmarize) the results\n:::\n\n***\n\n@lst-chap10-lm10.1 uses `stats::family = binomial(link = \"logit\")`to specify the outcome variable. The \"family\" argument provides a description of the error distribution and link function to be used in the model. This is a convenient way to specify the details of the models and to distinguish between different types of generalized linear models that are appropriate for different kinds of outcome variables. In addition to the `glm()` help page you get additional information if you type [?family](https://rdrr.io/r/stats/family.html) into the console.\n\n::::\n:::::\n\n###### Odds Ratio\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-compute-odds-ratio}\n: Get model fit, model significance, and odds ratios\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-compute-odds-ratio}\n\n::: {.cell}\n\n```{.r .cell-code}\nodds.n.ends::odds.n.ends(lm10.1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Waiting for profiling to be done...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $`Logistic regression model significance`\n#> Chi-squared        d.f.           p \n#>      10.815       1.000       0.001 \n#> \n#> $`Contingency tables (model fit): frequency predicted`\n#>                 Number observed\n#> Number predicted    1    0  Sum\n#>              1    338  298  636\n#>              0    435  500  935\n#>              Sum  773  798 1571\n#> \n#> $`Count R-squared (model fit): percent correctly predicted`\n#> [1] 53.34182\n#> \n#> $`Model sensitivity`\n#> [1] 0.4372574\n#> \n#> $`Model specificity`\n#> [1] 0.6265664\n#> \n#> $`Predictor odds ratios and 95% CI`\n#>                   OR     2.5 %    97.5 %\n#> (Intercept) 1.497482 1.1339164 1.9804811\n#> age         0.991201 0.9859589 0.9964415\n```\n\n\n:::\n:::\n\n\nModel fit, model significance, and odds ratios\n:::\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n***\n\n#### NHST Step 3\n\nReview and interpret the test statistics: \nCalculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap10-chi-squared-dist}\n: Chi-squared distribution with df = 1\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Theory: df = 1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-chi-squared-graph}\n: Theoretical chi-squared distribution with df = 1 \n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-chi-squared-graph}\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot() +\n    ggplot2::xlim(0, 12) +\n    ggplot2::stat_function(\n        fun = dchisq,\n        args = list(df = 1),\n        linewidth = 1,\n        color = \"hotpink2\"\n    ) +\n    ggplot2::labs(\n        x = \"Chi-squared statistic\",\n        y = \"Probability density\"\n    )\n```\n\n::: {.cell-output-display}\n![](10-logistic-regression_files/figure-html/chi-squared-graph-theory-1.png){width=672}\n:::\n:::\n\n\nTheoretical chi-squared distribution with df = 1 \n:::\n\nGraphing the probability of the chi-squared distribution with 1 degree of freedom to show that the probability that the chi-squared would be 10.815 or higher --- if the null hypothesis were true --- is the very small area under the curve from 10.815 to the right.\n\nTo get a better look at the position $x = 10.815$ let's zoom into the critical region. See @lst-chap10-chi-squared-graph-zoomed.\n\n::::\n:::::\n\n\n###### Theory: df = 1, zoomed\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-chi-squared-graph-zoomed}\n: Theoretical chi-squared distribution with df = 1, zoomed into the critical region of  p = 10.815\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-chi-squared-graph-zoomed}\n\n::: {.cell}\n\n```{.r .cell-code}\n## Define start of shade\nx_shade = 10.815 \ndf = 1\ny_shade = stats::dchisq(x_shade, df)\n\n\n\n## Define sequence of x-values\ntib <- tibble::tibble(x = seq(9, 20, length.out = 300)) |> \n    # Compute density values\n    dplyr::mutate(\n        y = stats::dchisq(x, df)\n    )\n\n## Subset data for shaded area\nshaded_area <- tib |> \n    dplyr::filter(x >= x_shade) |> \n    ## Necessary as starting point for y = 0!\n    tibble::add_row(x = x_shade, y = 0, .before = 1)\n\n\ntib |> \n    ## Plot the Chi-square distribution: df = 1\n    ggplot2::ggplot(ggplot2::aes(x = x, y = y)) +\n    ggplot2::geom_line(\n        linewidth = 1,\n        color = \"hotpink2\"\n    ) +\n    \n    ## Draw segment \n    ggplot2::geom_segment(\n        x = x_shade,\n        y = 0,\n        xend = x_shade,\n        yend = y_shade\n    ) +\n    \n    ## Shade curve\n    ggplot2::geom_polygon(\n        data = shaded_area, \n        fill = \"lightblue\",\n        ggplot2::aes(x = x, y = y)\n        ) +\n    ggplot2::labs(\n        x = \"Chi-squared statistic\",\n        y = \"Probability density\" \n    ) + \n    ggplot2::annotate(\n        geom = \"text\",\n        x = 11.5,\n        y = .00058,\n        label = base::round(stats::dchisq(x_shade, df), 6)\n    )\n```\n\n::: {.cell-output-display}\n![](10-logistic-regression_files/figure-html/chi-squared-graph-zoomed-1.png){width=672}\n:::\n:::\n\n\nTheoretical chi-squared distribution with df = 1, zoomed into the critical region \n:::\n\nHere we see that at the chi-square statistic of 10.815 the p-value is with .0005 very tiny and about 100 times much smaller as the level of statistical significance of .05. \n\n::::\n:::::\n\nWhen I developed above plots I wondered why in the book the graph is a rough line and the sample size (without NA's) of $n = 1501$ was mentioned. I learned that my curve was the theoretical curve whereas I need the randomized values to get a more rough simulation that is nearer the factual data.\n\n\n###### Randomized\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chi-squared-graph-randomized}\n: Randomized chi-squared distribution with df = 1, n = 1571\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chi-squared-graph-randomized}    \n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nn = 1571\ndf = 1\n\nchisq_dist <- tibble::tibble(x = stats::rchisq(n, df))\n\nchisq_dist |> \n    ggplot2::ggplot(\n        ggplot2::aes(x)\n    ) +\n    ggplot2::geom_density(\n        linewidth = 1,\n        color = \"hotpink2\"\n    ) +\n    ggplot2::labs(\n        x = \"Chi-squared statistic\",\n        y = \"Probability density\"\n    )\n```\n\n::: {.cell-output-display}\n![](10-logistic-regression_files/figure-html/chi-squared-graph-randomized-1.png){width=672}\n:::\n:::\n\n\nRandomized chi-squared distribution with df = 1, n = 1571\n:::\n\n\n\nThis is a randomized chi-squared distribution. I could not shade values $p > 10.815$ because there were only three rows with values greater than that. \n\n\n\n::::\n:::::\n\n:::::: {#tdo-chap10-empiricial-chisq-dist}\n:::::{.my-checklist}\n:::{.my-checklist-header}\nTODO: I wonder if I could provide an empirical chi-squared distribution with the actual data.\n:::\n\n:::::\nHow to plot an empirical chi-squared distribution with real data?\n:::\n\n:::\n\n\n::::\n:::::\n\n:::::{.my-resource}\n:::{.my-resource-header}\n:::::: {#lem-chap10-chisq-video}\n: Video about the chi-squared distribution\n::::::\n:::\n::::{.my-resource-container}\nI found a lengthy video tutorial (19:31) using RStudio featuring chi-squared distribution. You will see, <a class='glossary' title='A probability densitiy function (PDF) describes a probability distribution for a random, continuous variable. Use a probability density function to find the chances that the value of a random variable will occur within a range of values that you specify. More specifically, a PDF is a function where its integral for an interval provides the probability of a value occurring in that interval. (Statistics By Jim)'>pdf</a>, <a class='glossary' title='A cumulative distribution function (CDF) tells us the probability that a random variable takes on a value less than or equal to x. (Statology) It sums all parts of the distribution, replacing a lot of calculus work. The CDF takes in a value and returns the probability of getting that value or lower. (BF, Chap.13) A CDF is a hypothetical model of a distribution, the ECDF models empirical (i.e. observed) data. (Statistics How To)'>cdf</a>, <a class='glossary' title='Quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities (Wikipedia)'>quantile</a>, plotting, examples, independence, <a class='glossary' title='The chi-squared goodness-of-fit test is used for comparing the values of a single categorical variable to values from a hypothesized or population variable. The goodness-of-fit test is often used when trying to determine if a sample is a good representation of the population. (SwR, Chap 5)'>goodness of fit</a> and explanations. There is also a accompanying web page with text and the code.\n::::\n:::::\n\n\n\n#### NHST Step 4\n\nConclude and write report.\n\n::: {.callout #rep-chap10-chisq-lm10.1}\nThe chi-squared test statistic for a logistic regression model with age predicting library use had a p-value of .001. This p-value indicates there is a .1% chance of a chi-squared statistic this large or larger if the null hypothesis were true. The null hypothesis is therefore rejected in favor of the alternate hypothesis that the model is better than the baseline at predicting library use. A logistic regression model including age was statistically significantly better than a null model at predicting library use [$χ^2(1) = 10.82; p = .001$].\n:::\n\n### Interpreting odds ratio significance\n\nBoth, `base::summary()` and `odds.n.ends::odds.n.ends()` included values and significance statistics for the age predictor. The `odds.n.ends()` output included <a class='glossary' title='Odds is usually defined in statistics as the probability an event will occur divided by the probability that it will not occur. An odds ratio (OR) is a measure of association between a certain property A and a second property B in a population. Specifically, it tells you how the presence or absence of property A has an effect on the presence or absence of property B. (Statistics How To). An odds ratio is a ratio of two ratios. They quantify the strength of the relationship between two conditions. They indicate how likely an outcome is to occur in one context relative to another. (Statistics by Jim)'>odds ratios</a>, which are easier to interpret. The outcome is transformed by the logistic function, therefore the coefficients from `summary()` are not easy to interpret directly.\n\nLooking at @lst-chap10-compute-odds-ratio we see that `age`has a odds ratio of .99. This could be interpreted as, “The odds of library use decrease by 1% for every 1-year increase in age.” The 1% comes from subtracting the odds ratio of .99 from 1 and multiplying by 100 to convert it to a percent. This is a stategy to make interpreting odds below 1 easier. Otherwise we would have to say: “The odds of library use are .99 times as high with every 1-year increase in age.”\n\n**When the confidence interval includes 1, the odds of the outcome are not statistically significantly different with a change in the independent variable.**\n\nThe odds ratio for the age variable is less than 1, and the confidence interval does not include 1. Thus, the interpretation of this odds ratio would be as follows: \n\n::: {.callout #rep-chap10-odds-ratio}\n##### Interpretation of age odds ratio\n\nThe odds of library use are 1% lower for every 1-year increase in age ($OR = .99; 95% CI: .986–.996$).\n:::\n\n\n#### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: {.callout-note}\n- **H0**: Library use is not associated with age.\n- **HA**: Library use is associated with age.\n:::\n\n#### NHST Step 2\n\nCompute the test statistic. \n\nThe `base::summary()` function following a `stats::glm()` function includes a <a class='glossary' title='A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (StatisticsHowTo)'>z-statistic</a> comparing the coefficient estimate to zero. The z-score is a measure how many standard deviations away a value is from a mean. But for logistic regression, the coefficient divided by the <a class='glossary' title='The standard error (SE) of a statistic is the standard deviation of its [sampling distribution]. If the statistic is the sample mean, it is called the standard error of the mean (SEM). (Wikipedia) The standard error is a measure of variability that estimates how much variability there is in a population based on the variability in the sample and the size of the sample. (SwR, Glossary)'>standard error</a> --- not by the <a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a> --- follows a z-distribution. This z-statistic is the test statistic for the <a class='glossary' title='Wald test is the statistical test for comparing the value of the coefficient in linear or logistic regression to the hypothesized value of zero; the form is similar to a one-sample t-test, although some Wald tests use a t-statistic and others use a z-statistic as the test statistic. (SwR, Glossary)'>Wald test</a>, which has the same purpose (but follows a different distribution) as the Wald test from @sec-chap09-achievement4.\n\nIn the case of the `age` variable, dividing the estimate of $-0.008838$ by its standard error of $0.002697$ gives a z-statistic of $-3.276974$, which is well beyond the boundary of $–1.96$ for statistical significance when $\\alpha$ is set at $.05$.\n\nThe z-distribution is a normal distribution with a mean of 0 and a standard deviation of 1.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap10-z-dist-odds-ratio}\n: z-distribution for sample size of 1571\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap10-z-dist-odds-ratio}\n\n::: {.cell}\n\n```{.r .cell-code}\nnhstplot::plotztest(\n    z = -3.28,\n    tails = \"one\",\n    xmax = 5\n    )\n```\n\n::: {.cell-output-display}\n![](10-logistic-regression_files/figure-html/z-dist-odds-ratio-1.png){width=672}\n:::\n:::\n\n\nz-distribution for sample size of 1571\n:::\n\n::::\n:::::\n\n\n#### NHST Step 3\n\nReview and interpret the test statistics: \nCalculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\nThe area under the curve to the left of the test statistic in @lst-chap10-z-dist-odds-ratio is very small. Given this area is very small, the p-value of .00105 in the output @lst-chap10-lm10.1 from `base::summary(lm10.1)` makes sense. There is a .105% probability that this sample came from a population where there was no relationship between age and library use. Thus, there is a statistically significant relationship between age and library use (z = –3.28; p = .001).\n\n\n#### NHST Step 4\n\nConclude and write report.\n\nThe odds ratio for age is .99 with a 95% CI of .986–.996. The confidence interval shows the range where the odds ratio likely is in the population. Because the confidence interval does not include 1, this indicates that the odds ratio is statistically significantly different from 1. The interpretation would be as follows:\n\n::: {.callout #rep-odds-ratio-significance}\n##### Interpreting odds ratio significance\n\n The null hypothesis of no relationship between library use and age is rejected. The odds of library use are 1% lower for every 1-year increase in age in the sample (OR = .99; 95% CI: .986–.996). The 95% confidence interval indicates that the odds of library use are .4%–1.4% lower with each 1-year increase in age in the population that the sample came from.\n:::\n\n## Achievement 4: Interpreting two measures of model fit {#sec-chap10-achievement4}\n\nFor linear regression, the $R^2$ statistic measured how well the model fit the observed data by measuring how much of the variability in the outcome was explained by the model. The concept of variance is appropriate for continuous but not categorical variables. There are several measures for categorical model fit, including the percent correctly predicted, sometimes called <a class='glossary' title='Count R-squared, or percent correctly predicted, is a measure of model fit for a logistic regression model that is the proportion of the outcome values that were correctly predicted out of all observations modeled. (SwR, Glossary)'>count $R^2$</a> and the <a class='glossary' title='Adjusted count R-squared is a measure of model fit for binary logistic regression that adjusts the percent correctly predicted (or count R-squared) by the model for the number of people in the largest outcome category. (SwR, Glossary)'>adjusted count $R^2$</a>, which adjusts the count $R^2$ for the number of people in the largest of the two categories of the outcome.\n\n### Percent correctly predicted\n\nThe percent correctly predicted by the model is computed using the predicted probabilities, or fitted values, for each of the observations and comparing these probabilities to the true value of the outcome.\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap10-percent-correctly-predicted}\n: Percent correctly predicted or count $R^2$\n::::::\n:::\n::::{.my-theorem-container}\n$$\n\\begin{align*}\nR^2_{count} = \\frac{n_{correct}}{n_{total}}\n\\end{align*}\n$$ {#eq-chap10-percent-correctly-predicted}\n::::\n:::::\n\nFor example, if a person in the data set were predicted to have a chance of more than 50% of library use, this would be transformed into a “yes” or “1” value of the outcome and then compared to the person’s actual library use. If the predicted value and the true value matched, this would be considered a correct prediction. The same vice-versa for predicted values less than 50%. The total number of people the model gets correct out of the total number of people in the data analyzed is the <a class='glossary' title='Count R-squared, or percent correctly predicted, is a measure of model fit for a logistic regression model that is the proportion of the outcome values that were correctly predicted out of all observations modeled. (SwR, Glossary)'>percent correctly predicted</a> or count $R^2$.\n\nThe @lst-chap10-compute-odds-ratio includes a table showing how many observations were correctly predicted in each category of the outcome.\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap10-percent-correctly-predicted-example}\n: Calculate the percent correctly predicted with the `lm10.1` data\n::::::\n:::\n::::{.my-theorem-container}\n$$\n\\begin{align*}\nR^2_{count} = \\frac{338 + 500}{1571} = \\frac{838}{1571} = 0.5334\n\\end{align*}\n$$ {#eq-chapXY-formula}\n::::\n:::::\n\n### Adjusted count $R^2$\n\nAn alternative measure of percent correctly predicted is the <a class='glossary' title='Adjusted count R-squared is a measure of model fit for binary logistic regression that adjusts the percent correctly predicted (or count R-squared) by the model for the number of people in the largest outcome category. (SwR, Glossary)'>adjusted count R^2</a> measure. It adjusts the count $R^2$ for the number of people in the largest of the two categories of the outcome.\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap10-adjusted-count-r-squared}\n: Adjusted count $R^2$\n::::::\n:::\n::::{.my-theorem-container}\n$$\n\\begin{align*}\nR^2_{count.adj} = \\frac{n_{correct}- n_{most.common.outcome}}{n_{total}- n_{most.common.outcome}}\n\\end{align*}\n$$ {#eq-chap10-adjusted-count-r-squared}\n::::\n:::::\n\nThe argument behind this adjustment is that a null model, or a model with no predictors, could get a good percent correctly predicted just by predicting everyone was in the outcome category that had the bigger percentage of people in it.\n\nFor the library use data, the most common category is library nonuse (or 0), with 798 of the 1571 participants with complete data for the model. Without knowing anything about library use, you could predict everyone in the data set was a nonuser and be right $\\frac{788}{1571}$ or 50.8% of the time. Using the age predictor, the model is right $\\frac{338+500}{1571}$ or 53% of the time. While this is not a huge increase, it did classify 40 additional people correctly compared to using the percentages in the outcome categories with no other information.\n\nThe model using age to predict library use was correct 53% of the time (Count $R^2$ = .53). The adjusted count $R^2$ would be $\\frac{338+500-798}{1571-798}$ or .05.\n\n::: {.callout #rep-chap10-adjusted-r-squared}\n##### Interpretation of adjusted $R^2$ in `lm10.1`\n\nThere were 5% more correct predictions by the age model than by the baseline (Adjusted Count $R^2$ = .05).\n:::\n\n### Pseudo-$R^2$\n\nPseudo-$R^2$ is another measure that are reported relatively frequently. This measure generally tries to quantify the reduction in lack of fit between a null and full model. You can compute Pseudo-$R^2$  \n\n:::{.my-bulletbox}\n:::: {.my-bulletbox-header}\n::::: {.my-bulletbox-icon}\n:::::\n:::::: {#bul-chap10-pseudo-r2-functions}\n::::::\n: Packages with function for computing pseudo-$R^2$\n::::\n:::: {.my-bulletbox-body}\n- with r2_mcfadden() from the {**performance**} package (see: @pak-performance)\n- with `PseudoR2()` from the {**DescTools**} package (see: @pak-DescTools),\n- with `lrm()`(instead of the `stats::glm()`!) from the {**rms**} package (see: @pak-rms)\n- with `pR2()`from the {**pscl**} package (see: @pak-pscl), and\n- with `pseudo_r2() from the {**pubh**} package (see: @pak-pubh).\n\nMany of these packages have also function for percent correctly predicted and the adjusted count $R^2$ measure.\n::::\n:::\n\n\n::: {.callout-caution #cau-chap10-pseudo-r2}\n##### Interpreting pseudo-$R^2$\n\nI haven't tried out yet the functions of @bul-chap10-pseudo-r2-functions. Besides I am not certain how to calculate and interpret the pseudo-$R^2$ measure. I have seen that there is diversity in the approaches and I need a better understanding of the theoretical background.\n:::\n\n\n### Sensitivity and specificity\n\nSometimes it is useful to know whether the model is better at predicting people with the outcome or people without the outcome. The measures used for these two concepts are sensitivity and specificity.\n\n<a class='glossary' title='The true positive rate (one minus the false negative rate), is referred to as sensitivity, recall, or probability of detection. (Bayesian Thinking, Chap.1). But also the percentage of “yes” values or 1s a logistic regression model got right. (SwR, Glossary)'>Sensitivity</a> determines the percentage of the 1s or “yes” values the model got correct, while <a class='glossary' title=''>specificity</a> computes the percentage of 0s or “no” values the model got correct.\n\nIn @lst-chap10-compute-odds-ratio, the sensitivity is 43.7% while the specificity is 62.7%. The model was better at predicting the \"no\" values than the \"yes\" values. These percentages could also be computed from the frequency table in the output: The model predicted 500 of the 798 people in the 0 category correctly (62.7%) and 338 of the 773 in the 1 category correctly (43.7%).\n\n## Achievement 5: Estimating a larger logistic regression model {#sec-chap10-achievement5}\n\n## Exercises (empty)\n\n## Glossary\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Adjusted-count-R2 </td>\n   <td style=\"text-align:left;\"> Adjusted count R-squared is a measure of model fit for binary logistic regression that adjusts the percent correctly predicted (or count R-squared) by the model for the number of people in the largest outcome category. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Count-R-squared </td>\n   <td style=\"text-align:left;\"> Count R-squared, or percent correctly predicted, is a measure of model fit for a logistic regression model that is the proportion of the outcome values that were correctly predicted out of all observations modeled. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Cumulative Distribution Function </td>\n   <td style=\"text-align:left;\"> A cumulative distribution function (CDF) tells us the probability that a random variable takes on a value less than or equal to x. (&lt;a href=\"https://www.statology.org/cdf-vs-pdf/\"&gt;Statology&lt;/a&gt;) It sums all parts of the distribution, replacing a lot of calculus work. The CDF takes in a value and returns the probability of getting that value or lower. (BF, Chap.13) A CDF is a hypothetical model of a distribution, the ECDF models empirical (i.e. observed) data. (&lt;a href=\"https://www.statisticshowto.com/empirical-distribution-function/\"&gt;Statistics How To&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Digital Divide </td>\n   <td style=\"text-align:left;\"> The term \"digital divide\" refers to the gap between individuals, households, businesses and geographic areas at different socio-economic levels with regard to their opportunities to access information and communication technologies (ICTs). (&lt;a href= \"https://www.oecd-ilibrary.org/science-and-technology/understanding-the-digital-divide_236405667766\"&gt;OECD Library&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ExDA </td>\n   <td style=\"text-align:left;\"> Explorative Data Analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. (&lt;a href=\"https://en.wikipedia.org/wiki/Exploratory_data_analysis\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> GLM </td>\n   <td style=\"text-align:left;\"> A generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. (&lt;a href=\"https://en.wikipedia.org/w/index.php?title=Generalized_linear_model&amp;oldid=1175448680\"&gt;Wikipedia&lt;/a&gt;). the binary logistic regression model is one example of a generalized linear model. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Goodness-of-fit </td>\n   <td style=\"text-align:left;\"> The chi-squared goodness-of-fit test is used for comparing the values of a single categorical variable to values from a hypothesized or population variable. The goodness-of-fit test is often used when trying to determine if a sample is a good representation of the population. (SwR, Chap 5) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Linear Regression </td>\n   <td style=\"text-align:left;\"> Linear regression is used to predict the value of an outcome variable Y based on one or more input predictor variables X. The aim is to establish a linear relationship (a mathematical formula) between the predictor variable(s) and the response variable, so that, we can use this formula to estimate the value of the response Y, when only the predictors (Xs) values are known. ([r-statistics.co](https://r-statistics.co/Linear-Regression.html)) (Chap.4) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Logit Transformations </td>\n   <td style=\"text-align:left;\"> Logit transformations are transformations that takes the log value of p/(1-p); this transformation is often used to normalize percentage data and is used in the logistic model to transform the outcome. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Odds Ratio </td>\n   <td style=\"text-align:left;\"> Odds is usually defined in statistics as the probability an event will occur divided by the probability that it will not occur. An odds ratio (OR) is a measure of association between a certain property A and a second property B in a population. Specifically, it tells you how the presence or absence of property A has an effect on the presence or absence of property B. (&lt;a href=\"https://www.statisticshowto.com/probability-and-statistics/probability-main-index/odds-ratio/\"&gt;Statistics How To&lt;/a&gt;). An odds ratio is a ratio of two ratios. They quantify the strength of the relationship between two conditions. They indicate how likely an outcome is to occur in one context relative to another. (&lt;a href=\"https://statisticsbyjim.com/probability/odds-ratio/\"&gt;Statistics by Jim&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PDF </td>\n   <td style=\"text-align:left;\"> A probability densitiy function (PDF) describes a probability distribution for a random, continuous variable. Use a probability density function to find the chances that the value of a random variable will occur within a range of values that you specify. More specifically, a PDF is a function where its integral for an interval provides the probability of a value occurring in that interval. (&lt;a href=\"https://statisticsbyjim.com/probability/probability-density-function/\"&gt;Statistics By Jim&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> QRP </td>\n   <td style=\"text-align:left;\"> Questionable Research Practice (QRP) is a research practice that introduces bias, usually in pursuit of statistical significance; an example of such practices might be dropping or recoding values or variables solely to improve a model fit statistic. (SwR, GLossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Quantile </td>\n   <td style=\"text-align:left;\"> Quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities (&lt;a href=\"https://en.wikipedia.org/wiki/Quantile\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Sensitivity </td>\n   <td style=\"text-align:left;\"> The true positive rate (one minus the false negative rate), is referred to as sensitivity, recall, or probability of detection. (Bayesian Thinking, Chap.1). But also the percentage of “yes” values or 1s a logistic regression model got right. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Sigmoid </td>\n   <td style=\"text-align:left;\"> A sigmoid function is any mathematical function whose graph has a characteristic S-shaped curve or sigmoid curve. (&lt;a href=\"https://en.wikipedia.org/wiki/Sigmoid_function\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> specificity </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standard Deviation </td>\n   <td style=\"text-align:left;\"> The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter $\\sigma$ (sigma), for the population standard deviation, or the Latin letter $s$ for the sample standard deviation. ([Wikipedia](https://en.wiki </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standard Error </td>\n   <td style=\"text-align:left;\"> The standard error (SE) of a statistic is the standard deviation of its [sampling distribution]. If the statistic is the sample mean, it is called the standard error of the mean (SEM). (&lt;a href=\"https://en.wikipedia.org/wiki/Standard_error\"&gt;Wikipedia&lt;/a&gt;) The standard error is a measure of variability that estimates how much variability there is in a population based on the variability in the sample and the size of the sample. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Wald </td>\n   <td style=\"text-align:left;\"> Wald test is the statistical test for comparing the value of the coefficient in linear or logistic regression to the hypothesized value of zero; the form is similar to a one-sample t-test, although some Wald tests use a t-statistic and others use a z-statistic as the test statistic. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Z-score </td>\n   <td style=\"text-align:left;\"> A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (&lt;a href=\"https://www.statisticshowto.com/probability-and-statistics/z-score/#Whatisazscore\"&gt;StatisticsHowTo&lt;/a&gt;) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Session Info {.unnumbered}\n\n::: my-r-code\n::: my-r-code-header\nSession Info\n:::\n\n::: my-r-code-container\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.4.0 (2024-04-24)\n#>  os       macOS Sonoma 14.4.1\n#>  system   x86_64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2024-04-28\n#>  pandoc   3.1.13 @ /usr/local/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package     * version    date (UTC) lib source\n#>  base64enc     0.1-3      2015-07-28 [1] CRAN (R 4.4.0)\n#>  cli           3.6.2      2023-12-11 [1] CRAN (R 4.4.0)\n#>  colorspace    2.1-0      2023-01-23 [1] CRAN (R 4.4.0)\n#>  commonmark    1.9.1      2024-01-30 [1] CRAN (R 4.4.0)\n#>  crayon        1.5.2      2022-09-29 [1] CRAN (R 4.4.0)\n#>  curl          5.2.1      2024-03-01 [1] CRAN (R 4.4.0)\n#>  digest        0.6.35     2024-03-11 [1] CRAN (R 4.4.0)\n#>  dplyr         1.1.4      2023-11-17 [1] CRAN (R 4.4.0)\n#>  evaluate      0.23       2023-11-01 [1] CRAN (R 4.4.0)\n#>  fansi         1.0.6      2023-12-08 [1] CRAN (R 4.4.0)\n#>  farver        2.1.1      2022-07-06 [1] CRAN (R 4.4.0)\n#>  fastmap       1.1.1      2023-02-24 [1] CRAN (R 4.4.0)\n#>  forcats       1.0.0      2023-01-29 [1] CRAN (R 4.4.0)\n#>  generics      0.1.3      2022-07-05 [1] CRAN (R 4.4.0)\n#>  ggplot2       3.5.1      2024-04-23 [1] CRAN (R 4.4.0)\n#>  glossary    * 1.0.0.9003 2024-04-25 [1] Github (debruine/glossary@05e4a61)\n#>  glue          1.7.0      2024-01-09 [1] CRAN (R 4.4.0)\n#>  gtable        0.3.5      2024-04-22 [1] CRAN (R 4.4.0)\n#>  here          1.0.1      2020-12-13 [1] CRAN (R 4.4.0)\n#>  highr         0.10       2022-12-22 [1] CRAN (R 4.4.0)\n#>  htmltools     0.5.8.1    2024-04-04 [1] CRAN (R 4.4.0)\n#>  htmlwidgets   1.6.4      2023-12-06 [1] CRAN (R 4.4.0)\n#>  jsonlite      1.8.8      2023-12-04 [1] CRAN (R 4.4.0)\n#>  kableExtra    1.4.0      2024-01-24 [1] CRAN (R 4.4.0)\n#>  knitr         1.46       2024-04-06 [1] CRAN (R 4.4.0)\n#>  labeling      0.4.3      2023-08-29 [1] CRAN (R 4.4.0)\n#>  lattice       0.22-6     2024-03-20 [1] CRAN (R 4.4.0)\n#>  lifecycle     1.0.4      2023-11-07 [1] CRAN (R 4.4.0)\n#>  magrittr      2.0.3      2022-03-30 [1] CRAN (R 4.4.0)\n#>  markdown      1.12       2023-12-06 [1] CRAN (R 4.4.0)\n#>  Matrix        1.7-0      2024-03-22 [1] CRAN (R 4.4.0)\n#>  mgcv          1.9-1      2023-12-21 [1] CRAN (R 4.4.0)\n#>  munsell       0.5.1      2024-04-01 [1] CRAN (R 4.4.0)\n#>  nhstplot      1.3.0      2024-03-01 [1] CRAN (R 4.4.0)\n#>  nlme          3.1-164    2023-11-27 [1] CRAN (R 4.4.0)\n#>  odds.n.ends   0.1.4      2021-09-17 [1] CRAN (R 4.4.0)\n#>  pillar        1.9.0      2023-03-22 [1] CRAN (R 4.4.0)\n#>  pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 4.4.0)\n#>  purrr         1.0.2      2023-08-10 [1] CRAN (R 4.4.0)\n#>  R6            2.5.1      2021-08-19 [1] CRAN (R 4.4.0)\n#>  repr          1.1.7      2024-03-22 [1] CRAN (R 4.4.0)\n#>  rlang         1.1.3      2024-01-10 [1] CRAN (R 4.4.0)\n#>  rmarkdown     2.26       2024-03-05 [1] CRAN (R 4.4.0)\n#>  rprojroot     2.0.4      2023-11-05 [1] CRAN (R 4.4.0)\n#>  rstudioapi    0.16.0     2024-03-24 [1] CRAN (R 4.4.0)\n#>  rversions     2.1.2      2022-08-31 [1] CRAN (R 4.4.0)\n#>  scales        1.3.0      2023-11-28 [1] CRAN (R 4.4.0)\n#>  sessioninfo   1.2.2      2021-12-06 [1] CRAN (R 4.4.0)\n#>  skimr         2.1.5      2022-12-23 [1] CRAN (R 4.4.0)\n#>  stringi       1.8.3      2023-12-11 [1] CRAN (R 4.4.0)\n#>  stringr       1.5.1      2023-11-14 [1] CRAN (R 4.4.0)\n#>  svglite       2.1.3      2023-12-08 [1] CRAN (R 4.4.0)\n#>  systemfonts   1.0.6      2024-03-07 [1] CRAN (R 4.4.0)\n#>  tibble        3.2.1      2023-03-20 [1] CRAN (R 4.4.0)\n#>  tidyr         1.3.1      2024-01-24 [1] CRAN (R 4.4.0)\n#>  tidyselect    1.2.1      2024-03-11 [1] CRAN (R 4.4.0)\n#>  utf8          1.2.4      2023-10-22 [1] CRAN (R 4.4.0)\n#>  vctrs         0.6.5      2023-12-01 [1] CRAN (R 4.4.0)\n#>  viridisLite   0.4.2      2023-05-02 [1] CRAN (R 4.4.0)\n#>  withr         3.0.0      2024-01-16 [1] CRAN (R 4.4.0)\n#>  xfun          0.43       2024-03-25 [1] CRAN (R 4.4.0)\n#>  xml2          1.3.6      2023-12-04 [1] CRAN (R 4.4.0)\n#>  yaml          2.3.8      2023-12-11 [1] CRAN (R 4.4.0)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}