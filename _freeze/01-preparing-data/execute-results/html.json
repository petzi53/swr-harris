{
  "hash": "e80a0450e5814d9c98f46e2b9bc7b2b7",
  "result": {
    "engine": "knitr",
    "markdown": "# Preparing Data\n\n::: my-objectives\n::: my-objectives-header\nLearning Objectives:\n:::\n\n::: my-objectives-container\n**SWR Objectives**\n\n1.  (~~Observations and variables~~)\n2.  Using reproducible research practices\n3.  (~~Understanding and changing data types~~)\n4.  (~~Entering or loading data into R~~)\n5.  Identifying and treating missing values\n6.  (~~Building a basic bar chart~~)\n\nI will skip the crossed out achievements in parenthesis as I know\nalready these procedures. But I will add some other personal objectives\nfor this chapter.\n\n**My Additional Objectives**\n\n1.  Registering [GSS Data Explorer](https://gssdataexplorer.norc.org/)\n2.  Find the appropriate variable names in the [GSS code\n    book](https://gssdataexplorer.norc.org/gssReports/2)\n3.  Extracting appropriate data\n4.  Preparing dataset to simulate SWR data\n:::\n:::\n\n## Using reproducible research practices\n\n### Script files\n\nSWR explains writing script files, but I am using\n<a class='glossary' title='Literate programming is a methodology that combines a programming language with a documentation language, thereby making programs more robust, more portable, more easily maintained, and arguably more fun to write than programs that are written only in a high-level language. The main idea is to treat a program as a piece of literature, addressed to human beings rather than to a computer.(Donald Knuth)'>Literate Programming</a> with Quarto. This has the\nconsequence that in addition to short comments inside code cells I have\nthe possibility to write extensively in the other parts of the file\nabout approach, code, results etc.\n\nA practical advice for scripts is to include a <a class='glossary' title='A set of comments at the top of a code file that provides information about what is in the file (Harries, SWR)'>prolog</a>.\nPossible prolog sections:\n\n-   Project name\n-   Project purpose\n-   Name(s) of data set(s) used in the project\n-   Location(s) of data set(s) used in the project\n-   Code author name (you!)\n-   Date code created\n-   Date last time code was edited\n\nMost of these information are naturally occurring in the writing process\nof Quarto books.\n\n::: my-resource\n::: my-resource-header\nLiterate Statistical Programming\n:::\n\n::: my-resource-container\n-   Literate Programming:\n    ([Wikipedia](https://en.wikipedia.org/wiki/Literate_programming))\n-   Introduction to Literate Programming with Quarto ([Online\n    Slides](https://gesiscss.github.io/quarto-workshop/material/slides/01_introduction.html#/title-slide))\n-   Reproducibility and literate programming in R ([bookdown\n    course](https://exeter-data-analytics.github.io/LitProg/index.html))\n-   Introduction to Data Science in R for Biologists (Module on\n    [Literate Statistical Programming and\n    Quarto](https://mbutler808.github.io/rclass/posts/2023-01-26-intro-quarto/index.html))\n-   Letâ€™s build a blog with Quarto [Literate programming in\n    Quarto](https://ivelasq.quarto.pub/building-a-blog-with-quarto/workflow/write-docs/))\n    by Isabella VelÃ¡squez. The site has other material (for Quarto\n    blogs) as well: [Migrate from R\n    Markdown](https://ivelasq.quarto.pub/building-a-blog-with-quarto/learn-more/migrate-blog/),\n    [Additional\n    resources](https://ivelasq.quarto.pub/building-a-blog-with-quarto/learn-more/resources/)\n-   Introduction to literate programming with Quarto and Markdown by\n    Gesis\n    ([Slides](https://gesiscss.github.io/quarto-workshop/material/slides/01_introduction.html#/title-slide))\n:::\n:::\n\n### Naming objects\n\nI am used to apply the [tidyverse style\nguide](https://style.tidyverse.org/). It requires to use underlines\n(\"snake_code\") as separators in object names. (In contrast to\n\"camelCase\" code style). But reading the book I thought it might be a\ngood idea to use special additional styles for certain specific objects.\n\n-   **Naming constants**: Prefix name of constants with `k_`.\n-   **Naming variables**: Standard snake code.\n-   **Naming functions**: Prefix name of private functions with a dot\n    `.`. I had already experienced that didn't know from which package a\n    function was. Only to learn after looking around for minutes that it\n    was a function I wrote myself!\n-   **Naming data frames**: Prefix name with `df_` for data.frame and\n    `dt_` for tibble. I might also use a suffix to refer to the status\n    e.g., `_raw` (raw data), `_clean` (cleaned data), `_v2` (version\n    number).\n-   **Naming files**: It could be helpful to add at the start the\n    chapter number e.g. `chap02_`. And maybe also --- as in naming data\n    frames --- the status as suffix.\n\n## Import data frames from outside resources\n\nR has many possibilities to import data from other statistical packages.\nI will not go into the import details here.\n\n### Some common file extensions\n\n-   **.csv**: comma separated values\n-   **.txt**: text file\n-   **.xls or .xlsx**: Excel file\n-   **.sav**: SPSS file\n-   **.sasb7dat**: SAS file\n-   **.xpt**: SAS transfer file\n-   **.dta**: Stata file\n\n### Some packages for import data sources\n\n-   {**readr**}: Read Rectangular Text Data, part of {**tidyverse**}\n-   {**vroom**}: Read and Write Rectangular Text Data Quickly\n-   {**haven**}: Import and Export 'SPSS', 'Stata' and 'SAS' Files\n-   {**foreign**}: Read Data Stored by 'Minitab', 'S', 'SAS', 'SPSS',\n    'Stata', 'Systat', 'Weka', 'dBase', ...\n-   {**readxl**}: Read Excel Files\n-   {**openxslx**}: Read, Write and Edit xslx Files\n-   {**readODS**}: Read and Write ODS Files (e.g. LibreOffice)\n-   {**clipr**}: Read and Write from the System Clipboard\n\n### Importing data from General Social Survey (GSS)\n\n> â€œWhile the GSS data can be read into R directly from the GSS website,\n> Kiara had experienced this and knew that it could be frustrating.â€\n> ([Harris, 2020](zotero://select/groups/5254842/items/9N29QMJB))\n> ([pdf](zotero://open-pdf/groups/5254842/items/3NDRGBBW?page=107&annotation=SFD9FHQD))\n\nI have found several resources helping to work with the\n<a class='glossary' title='A large survey of a sample of people in the United States conducted regularly since 1972; the General Social Survey is abbreviated GSS and is conducted by the National Opinion Research Center at the University of Chicago. (Harris, Glossary)'>GSS</a>.\n\n::: my-resource\n::: my-resource-header\nWorking with the GSS\n:::\n\n::: my-resource-container\n## gssr {.unnumbered}\n\n[GSSR Package](https://kjhealy.github.io/gssr/): The General Social\nSurvey Cumulative Data (1972-2022) and Panel Data files packaged for\neasy use in R. {**gssr**} is a data package, developed and maintained by\n[Kieran Healy](https://kieranhealy.org/), the author of [Data\nVisualization](https://kieranhealy.org/publications/dataviz/). The\npackage bundles several datasets into a convenient format. Because of\nits large size {**gssr**} is not hosted on CRAN but as a [GitHub\nrepository](https://github.com/kjhealy/gssr/).\n\nInstead of browsing and examining the complex dataset with the [GSS Data\nExplorer](https://gssdataexplorer.norc.org/) or [download datasets\ndirectly](https://gss.norc.org/Get-The-Data) from the The National\nOpinion Research Center ([NORC](http://norc.org/)) you can now just work\ninside R. The current package 0.4 (see: [gssr\nUpdate](https://kieranhealy.org/blog/archives/2023/12/02/gssr-update/))\nprovides the GSS Cumulative Data File (1972-2022), three GSS Three Wave\nPanel Data Files (for panels beginning in 2006, 2008, and 2010,\nrespectively), and the 2020 panel file.\n\nVersion 0.40 also integrates survey code book information about\nvariables directly into Râ€™s help system, allowing them to be accessed\nvia the help browser or from the console with ?, as if they were\nfunctions or other documented objects.\n\n## asdfree {.unnumbered}\n\n[Analyze Survey Data for Free](http://asdfree.com/) is a bookdown\nwebsite by [Anthony\nDamico](https://www.youtube.com/@anthonyjosephdamico/playlists) with\ncurrently 64 locations to grab free survey data. As expected it features\nalso a [description of the\nGSS](http://asdfree.com/general-social-survey-gss.html) including\nanalysis examples with the {**survey**} package and {**lodown**}, a\n[package on GitHub]() to facilitate data imports. (See the section\n[Prerequisites](http://asdfree.com/prerequisites.html))\n:::\n:::\n\n::: my-example\n::: my-example-header\n::: {#exm-chap01-get-gss-data}\n: Get the GSS data\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Register\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-chap01-gss-account}\n: Register for the GSS Data Explorer\n:::\n:::\n\nTo use all the facilities of the GSS Data Explorer (tagging, tabulating,\ndata extracting) you need to register for a free account. The good thing\nis: This is a onetime procedure.\n\n::: my-procedure-container\n1.  Create a free account for the [GSS Data\n    Explorer](https://gssdataexplorer.norc.org/), a tool that allows to\n    browse the data that have been collected in the surveys.\n    -   Fill out the form\n    -   Wait for an email with the verification code\n    -   Confirm the registration with the verification code\n2.  Go for the tour to learn the interface (Link \"Tour Guide\")\n:::\n:::\n\n###### Explore\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-chap01-explore}\n: Exploring the <a class='glossary' title='A large survey of a sample of people in the United States conducted regularly since 1972; the General Social Survey is abbreviated GSS and is conducted by the National Opinion Research Center at the University of Chicago. (Harris, Glossary)'>General Social Survey</a> with the GSS Data\nExplorer\n:::\n:::\n\n::: my-procedure-container\n1.  **Click Link**: [Search GSS\n    Variables](https://gssdataexplorer.norc.org/variables/)\n2.  **Specify Period**: \"Select specific years\" did not work for me. I\n    always got the full dataset of the selected variable. I had to write\n    \"2016\" in both year fields to filter the search successfully.\n3.  **Type keyword**: in the filter field. For instance \"marijuana\",\n    \"grass\" or \"age\" and choose from the appearing drop down list or\n    enter return and inspect the resulting page. You will get several\n    variables with your search name in it. Read the short description on\n    the right side from the green variable name or open \"Associated\n    Questions\" to inspect the survey questions\n4.  **Add to myGSS**: Click the line of the appropriate variable into \"+\n    Add to myGSS\".\n5.  If you have added all wanted variable to \"myGSS\" open it with a\n    click at the top menu \"myGSS\" and\n6.  **Create extract**: If you have stored all required variables click\n    at the right green button \"+ Extracts\" and when the menu opens click\n    at \"Create Extract\". This opens now a form \"Create Extract\".\n7.  **Name the extract**: Describe it if necessary and confirm with\n    \"Next\".\n8.  **Build the extract** Add your desired variables, in our case \"age\"\n    and \"grass\". The ID number of the respondent is added automatically.\n9.  **Choose output option**: Besides that you have here again the\n    possibility to choose the years you can select the output format.\n10. **Click \"Create Extract\"**: This opens a new page where you can see\n    the queue of all your extracts that GSS is going to process. After\n    some seconds click \"Refresh\" to see if the extract is finished. You\n    extract should now be listed under \"Completed\".\n11. **Download extract**: Select your new extract and choose \"Download\"\n    from the \"Actions\" menu. (Choose \"Export\" if you want to transfer\n    the extract into [Google Drive](https://drive.google.com/drive/home)\n    or [Dropbox](https://www.dropbox.com/home).)\n:::\n:::\n\nAs one can see this is a somewhat cumbersome procedure to download the\ndesired data. It seems to me much easier to export the complete data, to\nload the gigantic file into R. After this is onetime process and then\nthe complete dataset is always available.\n\nBut using the GSS Data Explorer is very helpful to *explore* the\ndataset. Apply the first three steps of the above list to find the\ncorrect variable names, to read the exact wording of the question asked\nand to inspect the different codes that are used for the variable.\nOtherwise you have to skim the more than 700 pages of the GSS codebook.\nðŸ¤¬\n\n###### Download\n\nAnother approach is to download the complete dataset (or all variables\nof those years you are interested in) and --- after importing the full\ndataset into R --- manage the data in such a way that it can be easily\nused for your research question. This action is often called \"data\nwrangling\" and all to frequent not taught in statistics. It is one of\nthe major virtues of \"Statistics with R\" the building up of these\nimportant skills are addressed.\n\n\n\n::: {.cell}\n\n```{#lst-chap01-load-full-gss .r .cell-code  lst-cap=\"Load the full GSS Cumulative Data Set (cross-sectional samples 1972-2022)\"}\ngss_1972_2022_full <- haven::read_dta(\"data-raw/gss7222_r2.dta\")\nsaveRDS(gss_1972_2022_full, file = \"data-raw/gss_1972_2022_full.rds\" )\n```\n:::\n\n\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-chap01-load-full-gss}\n: Import the Cumulative Data Set (cross-sectional samples from all years) into R\n::::::\n:::\n::::{.my-procedure-container}\n1.  I visited <https://gss.norc.org/Get-The-Data> and chose under the\n    section \"Download the Data\" the \"STATA\" format. I read elsewhere\n    that this is the preferred format to convert the data into R with\n    the {**haven**} package.\n2.  From the [STATA-page](https://gss.norc.org/get-the-data/stata) I\n    chose the link under \"Cumulative Data Set (cross-sectional samples\n    from all years)\". It was Release 2 from November 2023 and was\n    downloaded in about 5-10 seconds.\n3.  After I unzipped `GSS_stata.zip` (42.3MB) I received a folder\n    `GSS_stata` (557.9MB) with four files:\n    -   `GSS 2022 Codebook.pdf` with 759 page(!).\n    -   `gss722_r2.dta` (554.9MB). After loading into R I discovered\n        that it contains 72,390 rows with 6693 columns.\n    -   `ReadMe.txt` with the short message to read the documentation\n        carefully because of methodological changes in this round.\n    -   `Release Notes 7222.pdf` with some notes (5 pages) about issues\n        in the previous release (r1) and fixes in the current release\n        (r2).\n4.  It took my about 2-3 minutes to load the gigantic dataset into\n    `gss_1972_2022_full`. It used 3.6 GB of my computer memory.\n5.  I chose the `base::saveRDS()` option (and not `base::save()`)\n    because when later reading into R again with `base::readRDS()` it\n    does not overwrite a variable with the same name respectively I can\n    assign the file to another variable name. It took me about 10-15\n    seconds to save a compressed version of the data file (38.7MB) and\n    about 5 second to load it via `base::readRDS()`.\n::::\n:::::\n\n\nAfter I have saved the data to do some data wrangling. To get the data structure for the book Figure 1.2 I need to:\n- reduce the dataset to the year 2016\n- select only the variables `age` and `grass`\n- drop all NAâ€™s\n- convert `grass` into factor\n- recode `grass` from \"1! and \"2\" to \"Yes\" and \"No\"\n- convert `age` from double to numeric\n- divide `age` into appropriate age intervals and label them accordingly\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-age-grass-2016-v1}\n: Get the data structure for the book Figure 1.2\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_age_grass_2016 <- base::readRDS(\"data-raw/gss_1972_2022_full.rds\")\n\ngss_age_grass_2016_v1  <- gss_age_grass_2016 |> \n    haven::zap_labels() |> \n    dplyr::filter(year == 2016) |> \n    dplyr::select(grass, age) |>\n    tidyr::drop_na() |>\n    dplyr::mutate(grass = forcats::as_factor(grass)) |> \n    dplyr::mutate(grass = \n          forcats::fct_recode(grass, Yes = \"1\", No = \"2\")) |> \n    dplyr::mutate(age = base::as.numeric(age)) |> \n    dplyr::mutate(age_cut = cut(age,\n                  breaks = c(-Inf, 29, 59, 74, Inf),\n                  labels = c(\"< 30\", \"30 - 59\", \"60 - 74\", \"75+\")))\n\nbase::summary(gss_age_grass_2016_v1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  grass           age           age_cut   \n#>  Yes:1123   Min.   :18.00   < 30   :332  \n#>  No : 713   1st Qu.:33.00   30 - 59:989  \n#>             Median :48.00   60 - 74:364  \n#>             Mean   :48.26   75+    :151  \n#>             3rd Qu.:61.00                \n#>             Max.   :89.00\n```\n\n\n:::\n:::\n\n\n\n::::\n:::::\n\n###### gssr\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-age-grass-2016-v1}\n: Get the data structure for the book Figure 1.2\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_age_grass_2016_v2 <- gssr::gss_get_yr(year = 2016) |> \n    haven::zap_labels() |> \n    dplyr::select(grass, age) |>\n    tidyr::drop_na() |>\n    dplyr::mutate(grass = forcats::as_factor(grass)) |> \n    dplyr::mutate(grass = \n          forcats::fct_recode(grass, Yes = \"1\", No = \"2\")) |> \n    dplyr::mutate(age = base::as.numeric(age)) |> \n    dplyr::mutate(age_cut = cut(age,\n                  breaks = c(-Inf, 29, 59, 74, Inf),\n                  labels = c(\"< 30\", \"30 - 59\", \"60 - 74\", \"75+\")))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Fetching: https://gss.norc.org/documents/stata/2016_stata.zip\n```\n\n\n:::\n\n```{.r .cell-code}\nbase::summary(gss_age_grass_2016_v2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  grass           age           age_cut   \n#>  Yes:1123   Min.   :18.00   < 30   :332  \n#>  No : 713   1st Qu.:33.00   30 - 59:989  \n#>             Median :48.00   60 - 74:364  \n#>             Mean   :48.26   75+    :151  \n#>             3rd Qu.:61.00                \n#>             Max.   :89.00\n```\n\n\n:::\n:::\n\n::::\n:::::\n\n\n:::\n\n\n:::\n:::\n\n# I STOPPED HERE\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_grass_age_2016 <- readxl::read_xlsx(\n    path = \"data-raw/gss-grass-age-2016.xlsx\"\n    )\n```\n:::\n\n\n::: {#lst-chap01-load-my-data}\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_marijuana_age <- readxl::read_xlsx(\n    path = \"data-raw/gss-marijuana-age.xlsx\"\n    )\n\nchap01_gss_marijuana_age_2016 <- \n    gss_marijuana_age |> \n    dplyr::filter(year == 2016) |> \n    dplyr::select(grass, age)\n\nset.seed(2016)\nbayr::as_tbl_obs(chap01_gss_marijuana_age_2016)\n```\n\n::: {.cell-output-display}\n\n\nTable: Data set with 3 variables, showing 8 of 2867 observations.\n\n|  Obs|grass               |age |\n|----:|:-------------------|:---|\n|  172|Should be legal     |27  |\n|  562|Should not be legal |70  |\n|  898|Should not be legal |60  |\n| 1019|Should be legal     |30  |\n| 1176|Should not be legal |80  |\n| 1505|.i:  Inapplicable   |53  |\n| 1911|Should not be legal |54  |\n| 2622|.i:  Inapplicable   |62  |\n\n\n\n:::\n:::\n\n\nLoad my version of marijuana data\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}