{
  "hash": "0128177a2e8dbbd1d429512cfb52e7dd",
  "result": {
    "engine": "knitr",
    "markdown": "# Analysis of variance {#sec-chap07}\n\n\n\n\n\n\n\n\n\n\n\n## Achievements to unlock\n\n::: my-objectives\n::: my-objectives-header\nObjectives\n:::\n\n::: my-objectives-container\n**SwR Achievements**\n\n- **Achievement 1**: Exploring the data using graphics and descriptive statistics {@sec-chap07-achievement1}\n- **Achievement 2**: Understanding and conducting one-way ANOVA {@sec-chap07-achievement2}\n- **Achievement 3**: Choosing and using post hoc tests and contrasts {@sec-chap07-achievement3}\n- **Achievement 4**: Computing and interpreting effect sizes for ANOVA {@sec-chap07-achievement4}\n- **Achievement 5**: Testing ANOVA assumptions {@sec-chap07-achievement5}\n- **Achievement 6**: Choosing and using alternative tests when ANOVA assumptions are not met {@sec-chap07-achievement6}\n- **Achievement 7**: Understanding and conducting two-way ANOVA {@sec-chap07-achievement7}\n\n:::\n:::\n\n<a class='glossary' title='Analysis of variance is a statistical method used to compare means across groups to determine whether there is a statistically significant difference among the means; typically used when there are three or more means to compare. (SwR, Glossary)'>ANOVA</a> is the statistical method used for comparing means across three or more groups. \n\n- Like the <a class='glossary' title='Student t-test is a statistical test used to test whether the difference between the response of two groups is statistically significant or not. (Wikipedia)'>t-test</a>, ANOVA has underlying assumptions.\n- Similar to <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a>, ANOVA is an <a class='glossary' title='An omnibus is a statistical test that identifies that there is some relationship going on between variables, but not what that relationship is. (SwR, Glossary)'>omnibus</a> test.\n- Instead of using <a class='glossary' title='Standardized residuals are the standardized differences between observed and expected values in a chi-squared analysis; a large standardized residual indicates that the observed and expected values were very different. (SwR, Glossary)'>standardized residuals</a>, ANOVA uses planned contrasts and post hoc tests.\n- Instead of <a class='glossary' title='Cramér’s V is an effect size to determine the strength of the relationship between two categorical variables; often reported with the results of a chi-squared. (SwR, Glossary)'>Cramér’s V</a> or <a class='glossary' title='Odds is usually defined in statistics as the probability an event will occur divided by the probability that it will not occur. An odds ratio (OR) is a measure of association between a certain property A and a second property B in a population. Specifically, it tells you how the presence or absence of property A has an effect on the presence or absence of property B. (Statistics How To). An odds ratio is a ratio of two ratios. They quantify the strength of the relationship between two conditions. They indicate how likely an outcome is to occur in one context relative to another. (Statistics by Jim)'>odds ratios</a> for chi-squared and <a class='glossary' title='Cohen’s d is a standardized effect size for measuring the difference between two group means. It is frequently used to compare a treatment to a control group. It can be a suitable effect size to include with t-test and ANOVA results. (Statistics by Jim)'>Cohen’s d</a> for t-tests, $η^2$ and $ω^2$ are often reported as <a class='glossary' title='Effect size is a measure of the strength of a relationship; effect sizes are important in inferential statistics in order to determine and communicate whether a statistically significant result has practical importance. (SwR, Glossary)'>effect sizes</a> for ANOVA.\n\n\n## The technical difficulties problem (empty)\n\n## Resources & Chapter Outline\n\n### Data, codebook, and R packages {#sec-chap04-data-codebook-packages}\n\n::: my-resource\n::: my-resource-header\nData, codebook, and R packages for learning about descriptive statistics\n:::\n\n::: my-resource-container\n\n**Data**\n\n\nTwo options:\n\n1. Download the `gss2018.rda` data set from <https://edge.sagepub.com/harris1e>.\n2. Use {**gssr**} to download the year 2018.\n\n(As a direct download with the {**gssr**} package results in labelled data with different column names and the necessary transformation will not gain any additional knowledge for me, I will take the `gss2018.rda` data set from the book.)\n\n**Codebook**\n\nTwo options:\n\n1. Access variable documentation (not a full codebook) on the GSS Data Explorer website at <https://gssdataexplorer.norc.org/> \n2. Use the help pages from {**gssr**} package.\n\n\n**Packages**\n\n1. Packages used with this chapter (sorted alphabetically)\n\n-   {**tidyverse**}: @pak-tidyverse (Hadley Wickham)\n-   {**car**): @pak-car (John Fox)\n-   {**dunn.test**} @pak-dunn.test (Alexis Dinno)\n\n    \n2. My additional packages (sorted alphabetically)\n\n\n\n:::\n:::\n\n\n\n### Get data {#sec-chap07-get-data}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-get-gss2018-book}\n: Get book GSS data set `gss2018.rda` and save it as `gss_2018.rds`\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## run only once (manually)\n## load \"GSS\" data.frame into memory\nbase::load(\"data/chap07/gss2018.rda\")\n\ngss_2018 <- GSS\nsave_data_file(\"chap07\", gss_2018, \"gss_2018.rds\")\n```\n:::\n\n\n(*For this R code chunk is no output available*)\n::::\n:::::\n\n\n### Show raw data {#sec-chap07-show-data}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-show-gss-2018-data}\n: Show summary for some `gss_2018` data\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018 <- base::readRDS(\"data/chap07/gss_2018.rds\")\ngss_2018 |> \n    dplyr::select(c(\"USETECH\", \"HAPPY\", \"SEX\", \"AGE\", \"DEGREE\")) |> \n    base::summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>     USETECH           HAPPY            SEX             AGE       \n#>  Min.   : -1.00   Min.   :1.000   Min.   :1.000   Min.   :18.00  \n#>  1st Qu.: -1.00   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:34.00  \n#>  Median : 10.00   Median :2.000   Median :2.000   Median :48.00  \n#>  Mean   : 48.09   Mean   :1.855   Mean   :1.552   Mean   :49.13  \n#>  3rd Qu.: 80.00   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:63.00  \n#>  Max.   :999.00   Max.   :8.000   Max.   :2.000   Max.   :99.00  \n#>      DEGREE     \n#>  Min.   :0.000  \n#>  1st Qu.:1.000  \n#>  Median :1.000  \n#>  Mean   :1.684  \n#>  3rd Qu.:3.000  \n#>  Max.   :4.000\n```\n\n\n:::\n:::\n\n***\n\n- **USETECH**: During a typical week, about what percentage of your total time at work would you normally spend using different types of electronic technologies (such as computers, tablets, smart phones, cash registers, scanners, GPS devices, robotic devices, and so on)?\n- **HAPPY**: Taken all together, how would you say things are these days -- would you say that you are very happy, pretty happy, or not too happy?\n- **SEX**: Respondent’s sex\n- **AGE**: Respondent’s age\n- **DEGREE**: Respondent’s highest degree\n- \n::::\n:::::\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-chap07-gss-procedure}\n: To get the full information for a variable in GSS\n::::::\n:::\n::::{.my-procedure-container}\n1. Go to <https://gssdataexplorer.norc.org/>\n2. In the box \"Access and Analyze GSS Data\" click on the \"SEARCH VARIABLES\" button.\n3. Click at \"Select specific years\", choose \"2018\" and confirm by pressing the \"Apply\"-button.\n4. Input the name of the variable \"USETECH\" into the field and confirm with <enter>.\n5. Open \"Associated questions\" by clicking the `>` symbol or by pressing the \"Show Expanded View\"-button.\n6. Click on the green variable name in the result list to get more detailed information about the variable.\n7. In contrast to the result in the book we get a slightly different coding scheme: We got four (not three) values outside the logical range of 0 to 100: -97, -98, -99, -100.\n\n![Screenshot of GSS Data Explorer 2018 USETECH variable values outside logical range](img/chap07/gss-usetech-codes-min.png){#fig-gss-usetech-codebook\nfig-alt=\"Table of the first 10 lines of GSS Data Explorer 2018 USETECH variable values\"\nfig-align=\"center\"}\n\n::::\n:::::\n\n:::::{.my-important}\n:::{.my-important-header}\nRecoding data exactly as in the book\n:::\n::::{.my-important-container}\nAs we are going to use the data set from the book and not the current data set as it is today (2024-03-25) saved at the GSS website, we will for instance USETECH recode -1, 998 and 999 as missing data in our data frame (and not the current values).  \n\nGenerally: There is nothing new for me in recoding the data. So I will apply all the necessary recoding in the next subsection in only one R code chunk.\n::::\n:::::\n\n\n\n### Recode data {#sec-chap07-recode-data}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-recode-gss2018}\n: Clean `gss_2018` data\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## run only once (manually) ############\ngss_2018 <- base::readRDS(\"data/chap07/gss_2018.rds\")\n\ngss_2018_clean <- gss_2018 |> \n    dplyr::select(c(\"USETECH\", \"HAPPY\", \"SEX\", \"AGE\", \"DEGREE\")) |> \n    dplyr::mutate(USETECH = dplyr::na_if(x = USETECH, y = -1)) |> \n    dplyr::mutate(USETECH = dplyr::na_if(x = USETECH, y = 998)) |>\n    dplyr::mutate(USETECH = dplyr::na_if(x = USETECH, y = 999)) |>\n    dplyr::mutate(AGE = dplyr::na_if(x = AGE, y = 98)) |>\n    dplyr::mutate(AGE = dplyr::na_if(x = AGE, y = 99)) |>\n    dplyr::mutate(DEGREE = dplyr::na_if(x = DEGREE, y = 8)) |>\n    dplyr::mutate(DEGREE = dplyr::na_if(x = DEGREE, y = 9)) |>\n    dplyr::mutate(HAPPY = dplyr::na_if(x = HAPPY, y = 8)) |>\n    dplyr::mutate(HAPPY = dplyr::na_if(x = HAPPY, y = 9)) |>\n    dplyr::mutate(HAPPY = dplyr::na_if(x = HAPPY, y = 0)) |> \n    \n    dplyr::mutate(SEX = forcats::as_factor(SEX)) |> \n    dplyr::mutate(DEGREE = forcats::as_factor(DEGREE)) |> \n    dplyr::mutate(HAPPY = forcats::as_factor(HAPPY)) |> \n    \n    dplyr::mutate(SEX = forcats::fct_recode(SEX, \n                                            male = \"1\", \n                                            female = \"2\")) |> \n    dplyr::mutate(DEGREE = forcats::fct_recode(DEGREE, \n                                            \"< high school\" = \"0\", \n                                            \"high school\" = \"1\",\n                                            \"junior college\" = \"2\",\n                                            \"bachelor\" = \"3\",\n                                            \"graduate\" = \"4\")) |> \n    dplyr::mutate(HAPPY = forcats::fct_recode(HAPPY, \n                                        \"very happy\" = \"1\",\n                                        \"pretty happy\" = \"2\",\n                                        \"not too happy\" = \"3\"))\n\nsave_data_file(\"chap07\", gss_2018_clean, \"gss_2018_clean.rds\")\n\nbase::summary(gss_2018_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>     USETECH                 HAPPY          SEX            AGE       \n#>  Min.   :  0.00   very happy   : 701   male  :1051   Min.   :18.00  \n#>  1st Qu.: 15.00   pretty happy :1304   female:1294   1st Qu.:34.00  \n#>  Median : 60.00   not too happy: 336                 Median :48.00  \n#>  Mean   : 55.15   NA's         :   4                 Mean   :48.98  \n#>  3rd Qu.: 90.00                                      3rd Qu.:63.00  \n#>  Max.   :100.00                                      Max.   :89.00  \n#>  NA's   :936                                         NA's   :7      \n#>             DEGREE    \n#>  < high school : 262  \n#>  high school   :1175  \n#>  junior college: 196  \n#>  bachelor      : 465  \n#>  graduate      : 247  \n#>                       \n#> \n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n\n## Achievement 1: Descriptive statistics {#sec-chap07-achievement1}\n\nThe work in this section is done in @sec-chap07-get-data, @sec-chap07-show-data and @sec-chap07-recode-data.\n\n### Explorative Data Analysis (EDA)\n\n**Question to explore**: Do people with higher educational degrees use technology at work more than people with lower degree?\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap07-eda}\n: Explorative Data Analysis (EDA)\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### mean / sd\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-usetech-mean-sd}\n: Mean and standard deviation of technology use by respondent’s highest degree\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean <- base::readRDS(\"data/chap07/gss_2018_clean.rds\")\n\nusetech_degree <- gss_2018_clean |> \n    tidyr::drop_na(USETECH, DEGREE) |>\n    dplyr::group_by(DEGREE) |> \n    dplyr::summarize(mean_usetech = mean(USETECH),\n                     sd_usetech = sd(USETECH))\nusetech_degree\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 5 × 3\n#>   DEGREE         mean_usetech sd_usetech\n#>   <fct>                 <dbl>      <dbl>\n#> 1 < high school          24.8       36.2\n#> 2 high school            49.6       38.6\n#> 3 junior college         62.4       35.2\n#> 4 bachelor               67.9       32.1\n#> 5 graduate               68.7       30.2\n```\n\n\n:::\n:::\n\n***\nIt seems that we could affirm our question. With higher degree the value of the mean (representing the percentage of technology usage) is rising. But we have a big standard deviation, especially in the lowest degree group ($sd \\approx 1.5 mean$). This could indicate that we have not a normal distribution because of high <a class='glossary' title='Kurtosis is a measure of how many observations are in the tails of a distribution; distributions that look bell-shaped, but have a lot of observations in the tails (platykurtic) or very few observations in the tails (leptokurtic) (SwR, Glossary)'>kurtosis</a>, e.g. we could have more observations in the tails than a normal distribution would have (<a class='glossary' title='Platykurtic is a distribution of a numeric variable that has more observations in the tails than a normal distribution would have; platykurtic distributions often look flatter than a normal distribution. (SwR, Glossary)'>platykurtic</a>).\n\n::::\n:::::\n\n\n###### replicate Figure 7.4\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-replicate-7-4}\n: Distribution of work time spent using technology by educational attainment\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngg_gss_2018 <- gss_2018_clean |> \n  tidyr::drop_na(USETECH) |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          x = DEGREE,\n          y = USETECH\n          )\n      ) +\n  ggplot2::geom_jitter(\n      ggplot2::aes(color = DEGREE), alpha = .6\n      ) +\n  ggplot2::geom_boxplot(\n      ggplot2::aes(fill = DEGREE), alpha = .4\n      ) +\n  ggplot2::scale_fill_brewer(\n      palette = \"Spectral\", \n      guide = \"none\"\n      ) +\n  ggplot2::scale_color_brewer(\n      palette = \"Spectral\", \n      guide = \"none\") +\n  ggplot2::theme_bw() +\n  ggplot2::labs(\n      x = \"Highest educational attainment\", \n      y = \"Percent of time spent using technology\"\n      )\n\ngg_gss_2018\n```\n\n::: {.cell-output-display}\n![Distribution of work time spent using technology by educational attainment, using palette 'Spectral' of brewer scales](07-analysis-of-variance_files/figure-html/fig-replicate-7-4-1.png){#fig-replicate-7-4 width=672}\n:::\n:::\n\n***\n\nHarris uses with this graph (Figure 7.4 in her book) the color schemes from [ColorBrewer](https://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3). See for more details the [color palettes of RColorBrewer](https://renenyffenegger.ch/notes/development/languages/R/packages/RColorBrewer/index) and the [screenshot](https://renenyffenegger.ch/notes/development/languages/R/packages/tmaptools/index#r-tmaptools-palette_explorer) of the `tmaptools::palette_explorer()` function. \n\nBut the chosen color palette is not appropriate for people with color vision deficiency (<a class='glossary' title='Color vision deficiency (CVD) or color blindness (also spelled colour blindness) includes a wide range of causes and conditions and is actually quite complex. It’s a condition characterized by an inability or difficulty in perceiving and differentiating certain colors due to abnormalities in the three color-sensing pigments of the cones in the retina. (EnChroma)'>CVD</a>). Mainly the yellow color is problematic as can be demonstrated with the following plot:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolorblindr::cvd_grid(gg_gss_2018)\n```\n\n::: {.cell-output-display}\n![](07-analysis-of-variance_files/figure-html/fig-check-colorblind-save-gss_2018-plot-1.png){#fig-check-colorblind-save-gss_2018-plot width=672}\n:::\n:::\n\n\n\n::::\n:::::\n\n###### better colors\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-techuse-dist}\n: Distribution of work time spent using technology by educational attainment (colorblind save)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ngg2_gss_2018 <- gss_2018_clean |> \n  tidyr::drop_na(USETECH) |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          x = DEGREE,\n          y = USETECH\n          )\n      ) +\n  ggplot2::geom_jitter(\n      ggplot2::aes(color = DEGREE), alpha = .6\n      ) +\n  ggplot2::geom_boxplot(\n      ggplot2::aes(fill = DEGREE), alpha = .4\n      ) + \n  ggokabeito::scale_color_okabe_ito(guide = \"none\") +\n  ggokabeito::scale_fill_okabe_ito(guide = \"none\") +\n  ggplot2::theme_bw() +\n  ggplot2::labs(\n      x = \"Highest educational attainment\", \n      y = \"Percent of time spent using technology\"\n  )\n\ngg2_gss_2018\n```\n\n::: {.cell-output-display}\n![Distribution of work time spent using technology by educational attainment, using the color save palette 'Okabe-Ito'](07-analysis-of-variance_files/figure-html/techuse-dist-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncolorblindr::cvd_grid(gg2_gss_2018)\n```\n\n::: {.cell-output-display}\n![Distribution of work time spent using technology by educational attainment, using the color save palette 'Okabe-Ito'](07-analysis-of-variance_files/figure-html/techuse-dist-2.png){width=672}\n:::\n:::\n\n\n***\n\nAlthough the color palette \"Okabe-Ito\" is also using a kind of yellow the result is much better in all CVD variants. Compare it with @fig-check-colorblind-save-gss_2018-plot.\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\nWhat we can see with the graph is that there are many observation at the bottom and at the top of the range of the variable `USETECH`.  \n\n- Many people in the first two categories had selected 0% of their time at work is spent using technology (<a class='glossary' title='A floor effect happens when a variable has many observations that take the lowest value of the variable, which can indicate that the range of values was insufficient to capture the true variability of the data. (SwR, Glossary)'>Floor effect</a>).\n- For all but the first category, there were a lot of people who selected 100% of their time at work is spent using technology (<a class='glossary' title='A ceiling effect happens when many observations are at the highest possible value for a variable. (SwR, Glossary)'>Ceiling effect</a>).\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! ANOVA with floor and ceiling effect\n:::\n::::{.my-watch-out-container}\n\nWhen there are floor or ceiling effects, this often means that the variation in a measure is limited by its range. Since ANOVA is an analysis of variance, which examines central tendency and variation together, the limitations of floor and ceiling effects can result in not finding differences when there are differences.\n\nSometimes floor or ceiling effects are hints that the range of the variable is not chosen correctly. But this does not apply in our case, because the range of using technology from 0 to 100% is as wide as it can be. Besides I believe these extreme values do not relate to the true value of technology use in work. I think that today there is almost no work without some sort of technology support. On the other hand it is no very likely that 100% (every second of work) of technology use is realistic.\n::::\n:::::\n\n## Achievement 2: Conducting one-way ANOVA {#sec-chap07-achievement2}\n\n### Introduction\n\nYou can't apply the <a class='glossary' title='A t-test is a type of statistical analysis used to compare the averages of two groups and determine whether the differences between them are more likely to arise from random chance. (Wikipedia)'>t-test</a> as a number of different pairwise tests to compare categorical variables that have several levels (groups). The problem is that the <a class='glossary' title='Rejecting the null hypothesis when it should be retained is called Type I error or alpha and used as the threshold to determine statistical significance. (SwR, Glossary)'>Type I error</a> piles ab with several tests. For example: With five groups in the `DEGREE` variable, pairwise comparisons with a t-test (i.e., conducting <a class='glossary' title='Pairwise comparisons are comparisons between every pair of groups to identify which are statistically significantly different from one another following a statistically significant result in an analysis of variance (ANOVA) or other multigroup analysis. (SwR, Glossary)'>pairwise comparisons</a>) would result in 10 t-tests. If each t-test had a p-value threshold of .05 for statistical significance, the probability of at least one Type I error is fairly high.\n\n***\n\n::: {#exp-chap07-type-1-2-errors}\n\n- **Type I error**: A glossary(\"Type-1\", \"Type I error\")`, also called $\\alpha$, is when there is no relationship but the study detects one. The $\\alpha$ for Type I error is also the threshold set for statistical significance. The threshold for statistical significance is the amount of uncertainty tolerated in concluding whether or not a result is statistically significant.\n- **Type II error**: A <a class='glossary' title='Retaining the null hypothesis when it should be rejected is calles Type II error or beta. (SwR, Glossary)'>Type II error</a>, also called $\\beta$, occurs when there is a relationship but the study did not detect it.\n- **Statistical power**: The power of a statistical test is the probability that the results of the test are not a Type II error.\n\nDefinition of Type I and Type II errors\n:::\n***\n\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap07-familywise-type-1}\n: Probability of a Type I error when there are multiple comparisons\n::::::\n:::\n::::{.my-theorem-container}\n$$\n\\begin{align*}\n\\alpha_{f} = 1 - (1 - \\alpha_{i})^c \\\\\nc = \\frac{k(k-1)}{2}\n\\end{align*}\n$$ {#eq-chap07-familywise-type-1}\n\n***\n\n- $\\alpha_{f}$: <a class='glossary' title='Familywise error is the alpha or Type I error rate when conducting multiple statistical tests. A large familywise alpha is one of the reasons that analysis of variance is preferable to conducting multiple t-tests when comparing means across more than two groups. (SwR, Glossary)'>familywise</a> Type I error rate\n- $\\alpha_{i}$: the individual alpha set as the statistical significance threshold\n- $c$: number of comparisons\n- $k$: total number of groups\n\n***\n\nFor a five-group `DEGREE` variable with $\\alpha = .05$ for each pairwise comparison the familywise $\\alpha_{f}$ would be:\n\n$$\n\\begin{align*}\n\\alpha_{f} = 1-(1-0.05_{i})^\\frac{5(5-1)}{2} = \\\\\n\\alpha_{f} = 1-(1-0.05_{i})^{10} = 0.4012631\n\\end{align*}\n$$\n\n::::\n:::::\n\n> With 10 pairwise comparisons, the familywise $\\alpha_{f}$ indicated there would be a 40% probability of making a Type I error. To control this error rate, and for efficiency, use a single ANOVA test instead of 10 t-tests. ANOVA is useful for testing whether three or more means are equal. It can be used with two means, but the t-test is preferable because it is more straightforward.\n\n:::::{.my-remark}\n:::{.my-remark-header}\nRunning many tests\n:::\n::::{.my-remark-container}\nI believe that the problem of a rising Type I error with a growing number of pairwise comparisons is also valid for other type of tests. Each test has the .05 threshold and a collection of many different tests with the same data rises the probability of making a Type I error. Using this strategy consciously is one form of <a class='glossary' title='P-hacking is a set of statistical decisions and methodology choices during research that artificially produces statistically significant results. These decisions increase the probability of false positives—where the study indicates an effect exists when it actually does not. P-hacking is also known as data dredging, data fishing, and data snooping. (Statistics by Jim)'>p-hacking</a>.\n::::\n:::::\n\n\n### F-Test statistic for ANOVA\n\nThe F-statistic is a ratio where the variation between the groups is compared to the variation within the groups. The between-group variation is in the numerator to calculate F, while the within-group variation is in the denominator.\n\n> Subtracting the grand mean from the group mean results in the difference between the group and the overall sample. The difference between the grand mean and the group mean can be positive or negative and so is squared for the sum to more accurately represent the total of the differences. This squared value is then multiplied by nj or the number of people in the group and divided by k – 1, where k is the number of groups. This results in a numerator that quantifies the difference between the group means and grand mean for all the participants in the sample.\n\n> The denominator sums the squared difference between each individual observation yij and the mean of the individual’s group, quantifying how far the individuals in the group are from the mean of the group. This is divided by the number of individuals in the whole sample minus the number of groups.\n\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap07-f-statistic}\n: F-Statistic\n::::::\n:::\n::::{.my-theorem-container}\n\n$$\n\\begin{align*}\nF &= \\frac{\\text{between-group variability}}{\\text{within-group variability}} \\\\\n&= \\frac{\\text{explained variance}}{\\text{unexplained variance}} \\\\\n&= \\frac{s_{between}^2}{s_{within}^2}\n\\end{align*}\n$$ {#eq-chap07-f-statistic}\n\n***\n\n> The F-statistic, then, could be referred to as a ratio of explained to unexplained variance. That is, how much of the variability in the outcome does the model explain compared to how much it leaves unexplained? The larger the F-statistic, the more the model has explained compared to what it has left unexplained.\n\n> The F-statistic can also be represented as the ratio of the variance between the groups to the variance within the groups.\n\n::::\n:::::\n\n### Computing F-test for using technology at the work place\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap07-f-test}\n: F-test for using technology at the work place\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### F-test\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-f-test-usetech-degree}\n: Applying F-test `stats::oneway.test()`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {#tbl-f-test-usetech-degree .cell tbl-cap='Applying F-test statistic (ANOVA)'}\n\n```{.r .cell-code}\ngss_2018_clean <- base::readRDS(\"data/chap07/gss_2018_clean.rds\")\n\nstats::oneway.test(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean,\n    var.equal = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tOne-way analysis of means\n#> \n#> data:  USETECH and DEGREE\n#> F = 43.304, num df = 4, denom df = 1404, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n###### grand & group means\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-replicate-figure7-5}\n: Distribution of work time spent using technology by educational attainment, with grand mean and group means (Replication of book Figure 7.5)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrand_mean <- \n    base::mean(gss_2018_clean$USETECH, na.rm = TRUE)\n\ngroup_means <- gss_2018_clean |> \n    tidyr::drop_na(USETECH, DEGREE) |> \n    dplyr::group_by(DEGREE) |> \n    dplyr::summarize(mean = mean(USETECH))\n\ngg3_gss_2018 <- gss_2018_clean |> \n  tidyr::drop_na(USETECH) |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          x = DEGREE,\n          y = USETECH\n          )\n      ) +\n  ggplot2::geom_jitter(\n      ggplot2::aes(\n          alpha = .6\n      ),\n      color = \"darkgrey\"\n  ) +\n  ggplot2::geom_hline(\n      ggplot2::aes(\n          yintercept = grand_mean,\n          linetype = \"solid\"\n      ),\n      color = \"steelblue\",\n      linewidth = 1\n  ) +\n  ggplot2::geom_point(\n      data = group_means,\n      ggplot2::aes(\n          x = DEGREE,\n          y = mean,\n          size = 3\n      ),\n      color = \"purple4\",\n      inherit.aes = FALSE\n  ) +\n  ggplot2::theme_bw() +\n  ggplot2::labs(\n      x = \"Highest educational attainment\", \n      y = \"Percent of time spent using technology\"\n  ) +\n  ggplot2::scale_size_continuous(\n      name = \"\",\n      labels = \"Group mean\"\n  ) +\n  ggplot2::scale_linetype_discrete(\n      name = \"\",\n      labels = \"Grand mean\"\n  ) +\n  ggplot2::scale_alpha_continuous(\n      name = \"\",\n      labels = \"Observation\"\n  )\n\ngg3_gss_2018\n```\n\n::: {.cell-output-display}\n![Distribution of work time spent using technology by educational attainment, with grand mean and group means](07-analysis-of-variance_files/figure-html/techuse-dist-grand-mean-1.png){width=672}\n:::\n:::\n\n\n***\nThis R-Code junk replicates book’s Figure 7.5, which has no accompanying R code.\n\n> For each group, the group mean does a better job than the <a class='glossary' title='Grand mean is the overall mean of a continuous variable that is used to determine distances from the mean for individuals and groups in ANOVA. (SwR, Glossary)'>overall mean</a> of explaining tech use *for that group*. The difference between the group mean and the overall mean is *how much better* the group mean is at representing the data in the group. This difference is used to compute the numerator of the <a class='glossary' title='F-statistic is a test statistic comparing explained and unexplained variance in [ANOVA] and linear regression. The F-statistic is a ratio where the variation between the groups is compared to the variation within the groups. (SwR, Glossary)'>F-statistic</a>.\n::::\n:::::\n\n###### F-distributions\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-f-dist-examples}\n: F-distribution examples\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot(\n    tibble::tibble(x = c(0, 5)), \n    ggplot2::aes(x = x,\n                 color = \"text\")\n    ) +\n    ggplot2::stat_function(fun = df, \n                           args = list(df1 = 4, df2 = 2000), \n                           ggplot2::aes(color = \"4, 2000\"),\n                           linewidth = .7) +\n    ggplot2::stat_function(fun = df, \n                           args = list(df1 = 4, df2 = 25), \n                           ggplot2::aes(color = \"4, 25\"),\n                           linewidth = .7) +\n    ggplot2::stat_function(fun = df, \n                           args = list(df1 = 2, df2 = 2000), \n                           ggplot2::aes(color = \"2, 2000\"),\n                           linewidth = .7) +\n    ggplot2::stat_function(fun = df, \n                           args = list(df1 = 2, df2 = 25), \n                           ggplot2::aes(color = \"2, 25\"),\n                           linewidth = .7) +\n    ggplot2::scale_color_manual(\n        name = \"Degress of freedom\\n(num, denom)\",\n        values = c(\"purple4\", \"purple\", \"green\", \"seagreen\"),\n        breaks = c(\"4, 2000\",\"4, 25\",\"2, 2000\", \"2, 25\")\n    )\n```\n\n::: {.cell-output-display}\n![F-distribution examples](07-analysis-of-variance_files/figure-html/fig-f-dist-examples-1.png){#fig-f-dist-examples width=672}\n:::\n:::\n\n\n::::\n:::::\n\n###### Technology use (ANOVA)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-f-statistic-tech-use}\n: F-distribution for the technology use by degree ANOVA (df = 4 and 1404)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot(\n    tibble::tibble(x = c(0, 5)), \n    ggplot2::aes(x = x)\n    ) +\n    ggplot2::stat_function(fun = df, \n                           args = list(df1 = 4, df2 = 1404), \n                           linewidth = .7)\n```\n\n::: {.cell-output-display}\n![F-distribution for the technology use by degree ANOVA (df = 4 and 1404)](07-analysis-of-variance_files/figure-html/fig-f-statistic-tech-use-1.png){#fig-f-statistic-tech-use width=672}\n:::\n:::\n\n***\n\n> The F-distribution in @fig-f-statistic-tech-use suggested the F-statistic of 43.30 was far to the right in the tail of the distribution. The probability of an F-statistic this large or larger if the null were true was reported in the output as < 2.2e-16, which is < .001. With a p-value this tiny, the F-statistic would be considered statistically significant.\n\n\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n::: {.callout-tip}\n> The mean time spent on technology use was significantly different across degree groups [F(4, 1404) = 43.3; p < .05], indicating that these groups likely came from a population with different mean time spent on technology use by educational attainment. The highest mean was the percent of time used for technology by those with graduate degrees. The lowest mean was the percent of time used for technology by those with less than a high school diploma.\n:::\n\n\n## Achievement 3: Post hoc tests & contrasts {#sec-chap07-achievement3}\n\n<a class='glossary' title='Analysis of variance is a statistical method used to compare means across groups to determine whether there is a statistically significant difference among the means; typically used when there are three or more means to compare. (SwR, Glossary)'>ANOVA</a> is --- similar as the <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared test</a> --- an <a class='glossary' title='An omnibus is a statistical test that identifies that there is some relationship going on between variables, but not what that relationship is. (SwR, Glossary)'>omnibus</a> test: iIt identifies whether there are any differences, but doesn’t give any information about what is driving the significant results.\n\nThere are two main ways to determine where significant differences among groups are following a significant omnibus test:\n\n- **Post hoc tests**: Examining each pair of means to determine which means are the most different from each other.\n- **Planned contrasts**: Comparing specified subsets of means or groups of means.\n\n### Post hoc tests\n\n#### Bonferroni\n\nThere are several different types of post hoc tests, and one of the more commonly used is the <a class='glossary' title='Bonferroni post hoc test is a pairwise test used after a statistically significant ANOVA that conducts a t-test for each pair of means but adjusts the threshold for statistical significance to ensure that there is a small enough risk of Type I error; it is generally considered a very conservative post hoc test that only identifies the largest differences between means as statistically significant. (SwR, Glossary)'>Bonferroni post hoc test</a>.\n\nThe Bonferroni adjustment multiplies each <a class='glossary' title='The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary)'>p-value</a> from each <a class='glossary' title='A t-test is a type of statistical analysis used to compare the averages of two groups and determine whether the differences between them are more likely to arise from random chance. (Wikipedia)'>t-test</a> by the overall number of t-tests conducted. There were 10 pairwise comparisons (5 groups each pairwise = 5 * 2), so these p-values have been multiplied by 10. Higher p-values will not reach the threshold for statistical significance as often. Sometimes there are resulting p-values of 1.0000. As the p-value cannot be over 1, so for p-values that are over 1 when adjusted by the multiplication, they are rounded to exactly 1.0000.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-bonferroni-test}\n: Bonferroni post hoc test\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::pairwise.t.test(\n    x = gss_2018_clean$USETECH,\n    g = gss_2018_clean$DEGREE,\n    p.adjust.method = \"bonferroni\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tPairwise comparisons using t tests with pooled SD \n#> \n#> data:  gss_2018_clean$USETECH and gss_2018_clean$DEGREE \n#> \n#>                < high school high school junior college bachelor\n#> high school    3.8e-11       -           -              -       \n#> junior college 2.8e-15       0.0022      -              -       \n#> bachelor       < 2e-16       8.0e-13     1.0000         -       \n#> graduate       < 2e-16       7.3e-09     1.0000         1.0000  \n#> \n#> P value adjustment method: bonferroni\n```\n\n\n:::\n:::\n\n***\n\nThe output is different from previous statistical testing. Instead of a test statistic like t or F, the output is a matrix of p-values.\n\nThe adjusted p-values for seven of the t-tests fall below .05 and so indicate that the difference in mean time using technology between two groups is statistically significant.\n\n- There are significant differences in mean time between less than high school and all of the other groups (p < .05); \n- Likewise, there are significant differences in mean time using technology between high school and all other groups. \n- There are no significant differences among the means of the three college groups (junior college, bachelor, graduate).\n\n\n::::\n:::::\n\nFor the report it would be more informative if one could add the group means for the interpretation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean |> \n    tidyr::drop_na(USETECH, DEGREE) |> \n    dplyr::group_by(DEGREE) |> \n    dplyr::summarize(\n        group_means = base::round(base::mean(USETECH), 1)\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 5 × 2\n#>   DEGREE         group_means\n#>   <fct>                <dbl>\n#> 1 < high school         24.8\n#> 2 high school           49.6\n#> 3 junior college        62.4\n#> 4 bachelor              67.9\n#> 5 graduate              68.7\n```\n\n\n:::\n:::\n\n\n::: {.callout-tip}\n> Mean percentage of time using technology at work was statistically significantly (p < .05) lower for people with less educational attainment than a high school diploma (m = 24.8) compared to each of the other groups, where the mean percentage of time using technology ranged from 49.6 to 68.7.\n:::\n\n#### Tukey’s Honestly Significance Difference (HSD)\n\nTukey’s HSD post hoc test is a modified <a class='glossary' title='A t-test is a type of statistical analysis used to compare the averages of two groups and determine whether the differences between them are more likely to arise from random chance. (Wikipedia)'>t-test</a> with the test statistic, `q`. The `q` test statistic formula is the same as some versions of `t`, but the q-distribution is different from the t-distribution, raising the critical value necessary to reach statistical significance. Even with the same test statistic, it is more difficult to reach statistical significance with a Tukey’s <a class='glossary' title='Tukey’s Honestly Significant Difference (HSD) is a post hoc test to determine which means are statistically significantly different from each other following a significant ANOVA result; Tukey’s HSD compares each pair of means and so is considered a pairwise test, but it is less conservative than the Bonferroni post hoc test. (SwR, Glossary)'>HSD</a> q-statistic compared to a t-test.\n\n$$\nq = \\frac{m_{1} - m_{2}}{se}\n$$ {#eq-chap07-hsd}\n\nThe `stats::TukeyHSD()` function does not work well with the `stats::oneway.test()` output from earlier, so the entire ANOVA model has to be re-estimated. The `stats:aov()` function works and takes similar arguments to the `stats::oneway.test()` function, so nesting the `stats::aov()` inside the `stats::TukeyHSD()` is one way to go.\n\n:::::{.my-important}\n:::{.my-important-header}\nDifference between test and fitting a model\n:::\n::::{.my-important-container}\nAfter my first reading it wasn’t quite clear for me what the difference is between `stats::oneway.test()` and `stats::aov()`. \n\nLater --- when I worked on the two-way ANOVA in @sec-chap07-achievement7 --- I learned that the first approach is a test whereas the second functions fits a model by a call to `lm()`. To get the full data for testing if the data are statistically significant one has to wrap `stats::aov()` into to the `base::summary()` function. \n\n***\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean <- base::readRDS(\"data/chap07/gss_2018_clean.rds\")\n\nglue::glue(\"##################  oneway.test()    ####################\")\nstats::oneway.test(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean,\n    var.equal = TRUE\n)\n\nglue::glue(\" \")\nglue::glue(\"######################  aov()    ##########################\")\nglue::glue(\" \")\nstats::aov(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean\n)\n\nglue::glue(\" \")\nglue::glue(\"##################  summary(aov())    ####################\")\nglue::glue(\" \")\nbase::summary(\n    stats::aov(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean\n    )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ##################  oneway.test()    ####################\n#> \n#> \tOne-way analysis of means\n#> \n#> data:  USETECH and DEGREE\n#> F = 43.304, num df = 4, denom df = 1404, p-value < 2.2e-16\n#> \n#>  \n#> ######################  aov()    ##########################\n#>  \n#> Call:\n#>    stats::aov(formula = USETECH ~ DEGREE, data = gss_2018_clean)\n#> \n#> Terms:\n#>                    DEGREE Residuals\n#> Sum of Squares   221300.6 1793757.2\n#> Deg. of Freedom         4      1404\n#> \n#> Residual standard error: 35.7436\n#> Estimated effects may be unbalanced\n#> 936 observations deleted due to missingness\n#>  \n#> ##################  summary(aov())    ####################\n#>  \n#>               Df  Sum Sq Mean Sq F value Pr(>F)    \n#> DEGREE         4  221301   55325    43.3 <2e-16 ***\n#> Residuals   1404 1793757    1278                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> 936 observations deleted due to missingness\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-hsd}\n: Compute Tukey’s Honestly Significance Difference (HSD)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::TukeyHSD(\n    stats::aov(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean\n    )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   Tukey multiple comparisons of means\n#>     95% family-wise confidence level\n#> \n#> Fit: stats::aov(formula = USETECH ~ DEGREE, data = gss_2018_clean)\n#> \n#> $DEGREE\n#>                                    diff       lwr      upr     p adj\n#> high school-< high school    24.8247754 15.145211 34.50434 0.0000000\n#> junior college-< high school 37.6070313 25.201887 50.01218 0.0000000\n#> bachelor-< high school       43.0859568 32.653180 53.51873 0.0000000\n#> graduate-< high school       43.9107249 32.256416 55.56503 0.0000000\n#> junior college-high school   12.7822558  3.362603 22.20191 0.0020352\n#> bachelor-high school         18.2611813 11.651711 24.87065 0.0000000\n#> graduate-high school         19.0859494 10.679691 27.49221 0.0000000\n#> bachelor-junior college       5.4789255 -4.713166 15.67102 0.5833665\n#> graduate-junior college       6.3036936 -5.135659 17.74305 0.5592907\n#> graduate-bachelor             0.8247681 -8.438819 10.08835 0.9992282\n```\n\n\n:::\n:::\n\n\nThe number of significant results of the 10 test are the same, although the Bonferroni test is more conservative. For example the p-value of the pairwise test between junior college and bachelor are 0.58 in HSD but 1.0 in Bonferroni.\n\n::::\n:::::\n\n::: {.callout-tip}\nThe mean time spent on technology use was significantly different across education groups [F(4, 1404) = 43.3; p < .05], indicating that these groups likely came from a population with different mean time spent on technology use depending on educational attainment. The highest mean was 68.7% of time used for technology for those with graduate degrees. The lowest mean was 24.8% of the time for those with less than a high school diploma. Mean percentage of time using technology was statistically significantly (p < .05) *lower* for people with less than a high school diploma (m = 24.8) compared to each of the other groups where the mean percentage of time using technology ranged from 49.6 to 68.7.\n:::\n\n\n### Planned comparisons\n\n#### Introduction\n\n<a class='glossary' title='Planned comparisons is a statistical strategy for comparing different groups, often used after a statistically significant analysis of variance to test hypotheses about which group means are statistically significantly different from one another. (SwR, Glossary)'>Planned comparisons</a> are computed by developing <a class='glossary' title='Contrasts are sets of numbers used in planned contrasts to specify which means or groups of means to compare to each other, usually to identify statistically significant differences among means after a statistically significant analysis of variance. (SwR, Glossary)'>contrasts</a> that specify which means to compare to which other means.\n\n::: {#bul-planned-contrasts}\n:::::{.my-bullet-list}\n:::{.my-bullet-list-header}\nBullet List\n:::\n::::{.my-bullet-list-container}\n\n\n- The order of the factor variable is the exact order that should be used in the contrast.\n- A contrast is a group of numbers used to group categories. \n- The categories grouped together should all be represented by the same number in the contrast. \n- The numbers in the contrast should all add to zero. \n- Any category not included in the contrast should be represented by a zero.\n\n::::\n:::::\nRules for planned contrasts \n:::\n\n***\n\nFor example, to compare all the college groups to the high school group, the contrast would omit the less than high school group and compare the mean for everyone in the high school group to the mean of the combined three college groups: junior college, bachelor, and graduate.\n\n- 0 (< high school: do not include) \n- 3 (high school) \n- –1 (junior college) \n- –1 (bachelor) \n- –1 (graduate)\n\nThe three categories represented by –1 will be grouped together because they are all represented by the same number.\n\n#### Compute planned contrasts\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap07-planned-constrasts}\n: Planned contrasts of using technology by degree\n::::::\n:::\n::::{.my-example-container}\n\n\n::: {.panel-tabset}\n\n###### high school & colleges\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-contrast-high-school-colleges}\n: Planned contrasts of using technology for the college groups to high school group\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## 1. put the contrast into a vector\ncontrast1 <- c(0, 3, -1, -1, -1)\n\n## 2. link the contrast to the categorical variable using contrasts()\nstats::contrasts(x = gss_2018_clean$DEGREE) <- contrast1\n\n## 2a. view the structure of the DEGREE variable with contrast\nglue::glue(\"************** View the structure of DEGREE variable *****************\")\nutils::str(object = gss_2018_clean$DEGREE)\n\n## 3. re-run the model using aov()\nusetech_degree_aov <- stats::aov(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean\n)\n\n## 4. apply the contrasts to the aov object\nglue::glue(\" \")\nglue::glue(\"******************* Summarize the aov object ****************************\")\nstats::summary.aov(\n    object = usetech_degree_aov,\n    split = list(DEGREE = \n            list(\"high school vs. all college\" = 1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ************** View the structure of DEGREE variable *****************\n#>  Factor w/ 5 levels \"< high school\",..: 3 2 4 4 5 4 2 2 1 2 ...\n#>  - attr(*, \"contrasts\")= num [1:5, 1:4] 0 3 -1 -1 -1 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:5] \"< high school\" \"high school\" \"junior college\" \"bachelor\" ...\n#>   .. ..$ : NULL\n#>  \n#> ******************* Summarize the aov object ****************************\n#>                                         Df  Sum Sq Mean Sq F value   Pr(>F)    \n#> DEGREE                                   4  221301   55325   43.30  < 2e-16 ***\n#>   DEGREE: high school vs. all college    1   64411   64411   50.41 1.97e-12 ***\n#> Residuals                             1404 1793757    1278                     \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> 936 observations deleted due to missingness\n```\n\n\n:::\n:::\n\n\n***\n\nThe output showed that mean technology use for those who finished high school was statistically significantly different from mean technology use for the three college groups combined [F(1, 1404) = 50.41; p < .001].\n\nTo understand more of what was happening we need to lookat the means being compared with this contrast.\n\n::::\n:::::\n\n\n###### recode high school & colleges\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-contrast-no-high-school-colleges}\n: Using technology for the recoded planned contrast between college groups to high school group\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean |> \n    dplyr::mutate(DEGREE =\n          forcats::fct_collapse(DEGREE,\n                `all college` = c(\n                    \"junior college\",\n                    \"bachelor\",\n                    \"graduate\"\n                    ) \n                )\n          ) |> \n    dplyr::group_by(DEGREE) |> \n    dplyr::summarize(mean_usetech = mean(USETECH, na.rm = TRUE),\n                     sd_usetech = sd(USETECH, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 3 × 3\n#>   DEGREE        mean_usetech sd_usetech\n#>   <fct>                <dbl>      <dbl>\n#> 1 < high school         24.8       36.2\n#> 2 high school           49.6       38.6\n#> 3 all college           67.0       32.3\n```\n\n\n:::\n:::\n\n***\n\nI have used here the first time the `forcats::fct_collaps()` function. This is more understandable as the option in the book, where (a) all factors are recoded and the thre highest factors with the same name \"all college\".\n\nThe difference between the mean technology use time for high school (m = 49.61) compared to all college groups combined (m = 66.97) is pretty large.\n\n::::\n:::::\n\n###### Plot hs & colleges\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-graph-contrast-no-high-school-colleges}\n: Plot of using technology for the recoded planned contrast between college groups to high school group\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean |> \n    dplyr::mutate(DEGREE =\n          forcats::fct_collapse(DEGREE,\n                `all college` = c(\n                    \"junior college\",\n                    \"bachelor\",\n                    \"graduate\"\n                    ) \n                )\n          ) |> \n    dplyr::filter(DEGREE == \"high school\" | DEGREE == \"all college\") |> \n    \n    ggplot2::ggplot(\n        ggplot2::aes(\n            y = USETECH, \n            x = DEGREE, \n            fill = DEGREE, \n            color = DEGREE\n        )\n    ) +\n    ggplot2::geom_boxplot(\n        alpha = .4,\n        na.rm = TRUE\n        ) + \n    ggplot2::geom_jitter(\n        alpha = .6,\n        na.rm = TRUE\n        ) + \n    ggplot2::scale_fill_manual(\n        values = c(\"gray70\", \"#7463AC\"), \n        guide = \"none\"\n        ) + \n    ggplot2::scale_color_manual(\n        values = c(\"gray70\", \"#7463AC\"), \n        guide = \"none\"\n        ) + \n    ggplot2::labs(\n        x = \"Educational attainment\", \n        y = \"Percent of time spent using technology\"\n        )\n```\n\n::: {.cell-output-display}\n![Contrast between time using technology for the high school group to the three college groups (junior college, beachelor, graduate)](07-analysis-of-variance_files/figure-html/fig-graph-contrast-no-high-school-colleges-1.png){#fig-graph-contrast-no-high-school-colleges width=672}\n:::\n:::\n\n***\nIt is clear that the means of these two groups are different. The same probably would also be true for the less than high school group with the three college groups.\n::::\n:::::\n\n###### < hs to colleges\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-contrast-less-hs-colleges}\n: Planned contrasts of time using technology on the job between college groups to less than high school group\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## 1. less than high school v. all college contrast\ncontrast2 <- base::c(3, 0, -1, -1, -1)\n\n## 2. bind the two contrasts together (matrix required!)\nmy_contrasts <- \n    as.matrix(dplyr::bind_cols(\n        contrast1 = contrast1, \n        contrast2 = contrast2))\n\n## 3. connect the tibble with factor variable\nstats::contrasts(gss_2018_clean$DEGREE) <-  my_contrasts\n\n## 4. compute ANOVA with planned contrasts\nstats::summary.aov(\n    object = usetech_degree_aov,\n    split = list(DEGREE = \n            list(\"high school vs. all college\" = 1,\n                 \"< high school vs. all college\" = 2)\n            )\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                                           Df  Sum Sq Mean Sq F value   Pr(>F)\n#> DEGREE                                     4  221301   55325   43.30  < 2e-16\n#>   DEGREE: high school vs. all college      1   64411   64411   50.41 1.97e-12\n#>   DEGREE: < high school vs. all college    1   20188   20188   15.80 7.39e-05\n#> Residuals                               1404 1793757    1278                 \n#>                                            \n#> DEGREE                                  ***\n#>   DEGREE: high school vs. all college   ***\n#>   DEGREE: < high school vs. all college ***\n#> Residuals                                  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> 936 observations deleted due to missingness\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n###### plot < hs & colleges\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-graph-less-than-hs-all-colleges}\n: Plot of using technology for the recoded planned contrast between college groups to less than high school group\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean |> \n    dplyr::mutate(DEGREE =\n          forcats::fct_collapse(DEGREE,\n                `all college` = c(\n                    \"junior college\",\n                    \"bachelor\",\n                    \"graduate\"\n                    ) \n                )\n          ) |> \n    \n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = DEGREE, \n            y = USETECH, \n            fill = DEGREE, \n            color = DEGREE\n        )\n    ) +\n    ggplot2::geom_boxplot(\n        alpha = .4,\n        na.rm = TRUE\n        ) + \n    ggplot2::geom_jitter(\n        alpha = .6,\n        na.rm = TRUE\n        ) + \n    ggplot2::scale_fill_manual(\n        values = c(\"gray70\", \"#7463AC\", \"dodgerblue\"), \n        guide = \"none\"\n        ) + \n    ggplot2::scale_color_manual(\n        values = c(\"gray70\", \"#7463AC\", \"dodgerblue\"), \n        guide = \"none\"\n        ) + \n    ggplot2::labs(\n        x = \"Educational attainment\", \n        y = \"Percent of time spent using technology\"\n        )\n```\n\n::: {.cell-output-display}\n![Distribution of tech use at work by degree for contrast comparing all college groups combined to each of the other groups](07-analysis-of-variance_files/figure-html/fig-graph-less-than-hs-all-colleges-1.png){#fig-graph-less-than-hs-all-colleges width=672}\n:::\n:::\n\n\n::::\n:::::\n\n###### 4 contrasts\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-4-contrasts}\n: Four planned comparisons\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## contrasts for ANOVA of tech time by degree \nc1 <- c(2, -1, -1, 0, 0) \nc2 <- c(0, 3, -1, -1, -1) \nc3 <- c(0, 0, 2, -1, -1) \nc4 <- c(0, 0, 0, -1, 1) \n\n## bind the contrasts into a matrix \nconts <- cbind(c1, c2, c3, c4) \nconts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>      c1 c2 c3 c4\n#> [1,]  2  0  0  0\n#> [2,] -1  3  0  0\n#> [3,] -1 -1  2  0\n#> [4,]  0 -1 -1 -1\n#> [5,]  0 -1 -1  1\n```\n\n\n:::\n:::\n\n\n***\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-chap07-check-contrasts}\n: Check the values of the contrasts\n::::::\n:::\n::::{.my-procedure-container}\n1. Add up each contrast to make sure it adds to zero. \n2. Multiply each value in each contrast with the corresponding values in the other contrasts and add up the products; this should also add to zero.\n::::\n:::::\n\n**ad 1**: The vectors are now columns. An example of a check is to sum the column c1: $2 + (-1) + (-1) + 0 + 0 = 0$. Columns c2, c3 and c4 also result to $0$. So the first check conditions was passed.\n\n**ad 2**: Now we have to multiply the values row-wise: $2 \\times 0 \\times 0 \\times 0 = 0$ It is easy to check: Whenever there is $0$ in one of the columns then the result is also $0$. Adding all these products together results in $0 + 0 + 0 + 0 + 0 = 0$, so the second requirement is also met.\n\n::::\n:::::\n\n###### `aov()` 4 contrasts\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-aov-4-contrasts}\n: Conncet the matrix with all levels of the factor variable\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## connect the matrix with the factor variable \nstats::contrasts(x = gss_2018_clean$DEGREE) <- conts \n\n## estimate the ANOVA with 4 independent contrasts \nusetech_degree_4_contrasts <- \n    summary.aov(object = usetech_degree_aov, \n        split = list(DEGREE = \n             list(\"< high school vs. high school & jr college\" = 1, \n                  \"high school vs. all college\" = 2, \n                  \"jr college vs. bach or grad degree\" = 3, \n                  \"bachelor’s vs. graduate degree\" = 4)\n             )\n        ) \nusetech_degree_4_contrasts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                                                        Df  Sum Sq Mean Sq\n#> DEGREE                                                  4  221301   55325\n#>   DEGREE: < high school vs. high school & jr college    1   64411   64411\n#>   DEGREE: high school vs. all college                   1   20188   20188\n#>   DEGREE: jr college vs. bach or grad degree            1   63902   63902\n#>   DEGREE: bachelor’s vs. graduate degree                1   72800   72800\n#> Residuals                                            1404 1793757    1278\n#>                                                      F value   Pr(>F)    \n#> DEGREE                                                 43.30  < 2e-16 ***\n#>   DEGREE: < high school vs. high school & jr college   50.41 1.97e-12 ***\n#>   DEGREE: high school vs. all college                  15.80 7.39e-05 ***\n#>   DEGREE: jr college vs. bach or grad degree           50.02 2.40e-12 ***\n#>   DEGREE: bachelor’s vs. graduate degree               56.98 7.88e-14 ***\n#> Residuals                                                                \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> 936 observations deleted due to missingness\n```\n\n\n:::\n:::\n\n\n\n::::\n:::::\n\n###### Adjust p-values\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-adjust-p-values}\n: Adjust p-values for multiple comparisons\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nadj_p_values <- stats::p.adjust(\n    p = usetech_degree_4_contrasts[[1]][[\"Pr(>F)\"]], \n    method = \"bonferroni\"\n    )\nadj_p_values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                                            \n#>                               1.329994e-33 \n#> < high school vs. high school & jr college \n#>                               9.850341e-12 \n#>                high school vs. all college \n#>                               3.696043e-04 \n#>         jr college vs. bach or grad degree \n#>                               1.198280e-11 \n#>             bachelor’s vs. graduate degree \n#>                               3.939123e-13 \n#>                                            \n#>                                         NA\n```\n\n\n:::\n:::\n\n***\n\nThe adjusted p-values were still very small, so the conclusions about statistical significance did not change, even when using a conservative adjustment like <a class='glossary' title='Bonferroni post hoc test is a pairwise test used after a statistically significant ANOVA that conducts a t-test for each pair of means but adjusts the threshold for statistical significance to ensure that there is a small enough risk of Type I error; it is generally considered a very conservative post hoc test that only identifies the largest differences between means as statistically significant. (SwR, Glossary)'>Bonferroni</a>.\n\n::::\n:::::\n\n###### Violin plot\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-4-planned-comp-violin-plot}\n: Visualizing the distribution of technology use at work by educational attainment for four planned comparisons\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## < high school / hs & jr college ########\ngg_violin1 <- gss_2018_clean |>\n  dplyr::mutate(DEGREE = \n        dplyr::if_else(DEGREE == \"< high school\", \"< high school\",\n        dplyr::if_else(DEGREE %in% c(\"high school\", \n                                     \"junior college\"),\n                                     \"high school & jr college\",\n                                     NA))) |>\n  dplyr::mutate(DEGREE = \n         base::factor(DEGREE, levels = c(\"< high school\",\n                                         \"high school & jr college\"),\n                                         ordered = T)) |>\n  dplyr::filter(DEGREE == \"< high school\" | \n                DEGREE == \"high school & jr college\") |> \n\n      ggplot2::ggplot(\n        ggplot2::aes(\n            y = USETECH, \n            x = DEGREE, \n            fill = DEGREE, \n            color = DEGREE\n        )\n    ) +\n    ggplot2::geom_violin(\n        alpha = .4,\n        na.rm = TRUE\n        ) + \n    ggplot2::geom_jitter(\n        alpha = .6,\n        na.rm = TRUE\n        ) + \n    ggplot2::scale_fill_manual(\n        values = c(\"gray70\", \"#7463AC\"), \n        guide = \"none\"\n        ) + \n    ggplot2::scale_color_manual(\n        values = c(\"gray70\", \"#7463AC\"), \n        guide = \"none\"\n        ) + \n    ggplot2::labs(\n        x = \"Educational attainment\", \n        y = \"Percent of time spent using technology\"\n        )\n    \n    \n## high school & three college groups##########\ngg_violin2 <- gss_2018_clean |> \n    dplyr::mutate(DEGREE =\n          forcats::fct_collapse(DEGREE,\n                `all college` = c(\n                    \"junior college\",\n                    \"bachelor\",\n                    \"graduate\"\n                    ) \n                )\n          ) |> \n    dplyr::filter(DEGREE == \"high school\" | DEGREE == \"all college\") |>\n    ggplot2::ggplot(\n        ggplot2::aes(\n            y = USETECH, \n            x = DEGREE, \n            fill = DEGREE, \n            color = DEGREE\n        )\n    ) +\n    ggplot2::geom_violin(\n        alpha = .4,\n        na.rm = TRUE\n        ) + \n    ggplot2::geom_jitter(\n        alpha = .6,\n        na.rm = TRUE\n        ) + \n    ggplot2::scale_fill_manual(\n        values = c(\"gray70\", \"#7463AC\"), \n        guide = \"none\"\n        ) + \n    ggplot2::scale_color_manual(\n        values = c(\"gray70\", \"#7463AC\"), \n        guide = \"none\"\n        ) + \n    ggplot2::labs(\n        x = \"Educational attainment\", \n        y = \"Percent of time spent using technology\"\n        )\n\n## jr college & bachelor & graduate ##########\ngg_violin3 <- gss_2018_clean |> \n    dplyr::mutate(DEGREE = \n    dplyr::if_else(DEGREE %in% c(\"< high school\", \"high school\"), NA,\n    dplyr::if_else(DEGREE == \"junior college\", \"jr college\",\n    dplyr::if_else(DEGREE %in% c(\"bachelor\", \"graduate\"),\n                                 \"bachelor or graduate\", NA)))) |>\n    dplyr::mutate(DEGREE = \n      base::factor(DEGREE, \n                   levels = c(\"jr college\", \n                              \"bachelor or graduate\", \n                              ordered = T))) |>\n    dplyr::filter(DEGREE == \"jr college\" | DEGREE == \"bachelor or graduate\") |>\n    \n    ggplot2::ggplot(\n        ggplot2::aes(\n            y = USETECH, \n            x = DEGREE, \n            fill = DEGREE, \n            color = DEGREE\n        )\n    ) +\n    ggplot2::geom_violin(\n        alpha = .4,\n        na.rm = TRUE\n        ) + \n    ggplot2::geom_jitter(\n        alpha = .6,\n        na.rm = TRUE\n        ) + \n    ggplot2::scale_fill_manual(\n        values = c(\"gray70\", \"#7463AC\"), \n        guide = \"none\"\n        ) + \n    ggplot2::scale_color_manual(\n        values = c(\"gray70\", \"#7463AC\"), \n        guide = \"none\"\n        ) + \n    ggplot2::labs(\n        x = \"Educational attainment\", \n        y = \"Percent of time spent using technology\"\n        )\n\n## bachelor & graduate ############\ngg_violin4 <- gss_2018_clean |> \n    dplyr::mutate(DEGREE = \n        dplyr::if_else(DEGREE == \"bachelor\", \"bachelor\",\n        dplyr::if_else(DEGREE == \"graduate\", \"graduate\", NA))) |>\n      dplyr::mutate(DEGREE = \n        base::factor(DEGREE, \n                     levels = c(\"bachelor\", \n                                \"graduate\", \n                                ordered = T))) |>\n    dplyr::filter(DEGREE == \"bachelor\" | DEGREE == \"graduate\") |> \n\n    ggplot2::ggplot(\n        ggplot2::aes(\n            y = USETECH, \n            x = DEGREE, \n            fill = DEGREE, \n            color = DEGREE\n        )\n    ) +\n    ggplot2::geom_violin(\n        alpha = .4,\n        na.rm = TRUE\n        ) + \n    ggplot2::geom_jitter(\n        alpha = .6,\n        na.rm = TRUE\n        ) + \n    ggplot2::scale_fill_manual(\n        values = c(\"gray70\", \"#7463AC\"), \n        guide = \"none\"\n        ) + \n    ggplot2::scale_color_manual(\n        values = c(\"gray70\", \"#7463AC\"), \n        guide = \"none\"\n        ) + \n    ggplot2::labs(\n        x = \"Educational attainment\", \n        y = \"Percent of time spent using technology\"\n        )\n\ngridExtra::grid.arrange(\n    gg_violin1, gg_violin2,\n    gg_violin3, gg_violin4,\n    ncol = 2)\n```\n\n::: {.cell-output-display}\n![Visualizing the distribution of technology use at work by educational attainment for four planned comparisons (violin plot)](07-analysis-of-variance_files/figure-html/fig-4-planned-comp-violin-plot-1.png){#fig-4-planned-comp-violin-plot width=672}\n:::\n:::\n\n\n***\nThis is the replication of book’s Figure 7.10.\n::::\n:::::\n\n###### Contrasts statistic\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-planned-comp-contrasts-statistic}\n: Summary of the constrasts statistic\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## contrast 1 statistics ###########\nglue::glue(\" \")\nglue::glue(\"#################### contrast 1 statistics ###################\")\ngss_2018_clean  |> \n  dplyr::mutate(DEGREE = \n        dplyr::if_else(DEGREE == \"< high school\", \"< high school\",\n        dplyr::if_else(DEGREE %in% c(\"high school\", \n                                     \"junior college\"),\n                                     \"high school & jr college\",\n                                     NA))) |>\n  dplyr::mutate(DEGREE = \n         base::factor(DEGREE, levels = c(\"< high school\",\n                                         \"high school & jr college\"),\n                                         ordered = T)) |>\n  dplyr::group_by(DEGREE) |>\n  dplyr::summarise(\n      mean_usetech = base::mean(x = USETECH, na.rm = T),\n      sd_usetech = stats::sd(x = USETECH, na.rm = T))\n\n\n\n## contrast 2 statistics ##############\nglue::glue(\" \")\nglue::glue(\"#################### contrast 2 statistics ###################\")\ngss_2018_clean |>\n  dplyr::mutate(DEGREE = \n         base::factor(DEGREE, labels = c(NA,\n               \"high school\", \"all college\",\n               \"all college\", \"all college\"),\n               ordered = T)) |>\n  dplyr::group_by(DEGREE) |>\n  dplyr::summarise(\n      mean_usetech = mean(x = USETECH, na.rm = T),\n      sd.usetech = sd(x = USETECH, na.rm = T)) \n\n## contrast 3 statistics ##############\nglue::glue(\" \")\nglue::glue(\"#################### contrast 3 statistics ###################\")\ngss_2018_clean |>\n  dplyr::mutate(DEGREE = \n    dplyr::if_else(DEGREE %in% c(\"< high school\", \"high school\"), NA,\n    dplyr::if_else(DEGREE == \"junior college\", \"jr college\",\n    dplyr::if_else(DEGREE %in% c(\"bachelor\", \"graduate\"),\n                                 \"bach or grad degree\", NA)))) |>\n    dplyr::mutate(DEGREE = \n      base::factor(DEGREE, \n                   levels = c(\"jr college\", \n                              \"bach or grad degree\", \n                              ordered = T))) |>\n  dplyr::group_by(DEGREE) |>\n  dplyr::summarise(\n      mean_usetech = mean(x = USETECH, na.rm = T),\n      sd.usetech = sd(x = USETECH, na.rm = T))\n\n# contrast 4 statistics ####################\nglue::glue(\" \")\nglue::glue(\"#################### contrast 4 statistics ###################\")\ngss_2018_clean |>\n  dplyr::mutate(DEGREE = \n    dplyr::if_else(DEGREE == \"bachelor\", \"bachelor\",\n    dplyr::if_else(DEGREE == \"graduate\", \"graduate\", NA))) |>\n  dplyr::mutate(DEGREE = \n    base::factor(DEGREE, \n                 levels = c(\"bachelor\", \n                            \"graduate\", \n                            ordered = T))) |>\n  dplyr::group_by(DEGREE) |>\n  dplyr::summarise(\n     mean_usetech = mean(x = USETECH, na.rm = T),\n     sd_usetech = sd(x = USETECH, na.rm = T))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  \n#> #################### contrast 1 statistics ###################\n#> # A tibble: 3 × 3\n#>   DEGREE                   mean_usetech sd_usetech\n#>   <ord>                           <dbl>      <dbl>\n#> 1 < high school                    24.8       36.2\n#> 2 high school & jr college         51.7       38.4\n#> 3 <NA>                             68.2       31.4\n#>  \n#> #################### contrast 2 statistics ###################\n#> # A tibble: 3 × 3\n#>   DEGREE      mean_usetech sd.usetech\n#>   <ord>              <dbl>      <dbl>\n#> 1 <NA>                24.8       36.2\n#> 2 high school         49.6       38.6\n#> 3 all college         67.0       32.3\n#>  \n#> #################### contrast 3 statistics ###################\n#> # A tibble: 3 × 3\n#>   DEGREE              mean_usetech sd.usetech\n#>   <fct>                      <dbl>      <dbl>\n#> 1 jr college                  62.4       35.2\n#> 2 bach or grad degree         68.2       31.4\n#> 3 <NA>                        45.8       39.3\n#>  \n#> #################### contrast 4 statistics ###################\n#> # A tibble: 3 × 3\n#>   DEGREE   mean_usetech sd_usetech\n#>   <fct>           <dbl>      <dbl>\n#> 1 bachelor         67.9       32.1\n#> 2 graduate         68.7       30.2\n#> 3 <NA>             48.1       39.1\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n\n\n:::\n\n::::\n:::::\n\n::: {.callout-tip}\nThe mean time spent on technology use at work was significantly different across educational attainment groups [F(4, 1404) = 43.3; p < .05], indicating these groups likely came from populations with different mean time spent on technology use. The highest mean was percent of time used for technology for those with graduate degrees. The lowest mean was percent of time for those with less than a high school diploma. A set of planned comparisons found that the mean time spent using technology was statistically significantly (p < .05) lower for (a) those with < high school education (m = 24.8) compared to those with high school or junior college (m = 51.7), (b) those with a high school education (m = 49.61) compared to those with all college groups combined (m = 67.0), (c) those with a junior college degree (m = 62.4) compared to those with a bachelor’s or graduate degree (m = 68.2), and (d) those with a bachelor’s degree (m = 67.9) compared to those with a graduate degree (m = 68.7). Overall, the patterns show statistically significant increases in time spent using technology at work for those with more education.\n:::\n\nHow many contrasts could be done and how all these statistical comparisons might be inflating the Type I error? --- In addition to each comparison comparing two things and each comparison adding to zero, the planned comparisons as a group should isolate each group (e.g., the high school group) *only one time*. This ensures that the contrasts are independent of each other since the variance for each group is only used by itself in a statistical comparison one time. Because each group is isolated one time, the total maximum number of contrasts allowable is one less than the number of groups.\n\n> When you have hypotheses ahead of time about which groups are different from one another, use <a class='glossary' title='Planned comparisons is a statistical strategy for comparing different groups, often used after a statistically significant analysis of variance to test hypotheses about which group means are statistically significantly different from one another. (SwR, Glossary)'>planned comparisons</a>. When you do not have hypotheses ahead of time about which means are different from each other, use post hoc tests if the ANOVA has a statistically significant <a class='glossary' title='F-statistic is a test statistic comparing explained and unexplained variance in [ANOVA] and linear regression. The F-statistic is a ratio where the variation between the groups is compared to the variation within the groups. (SwR, Glossary)'>F-statistic</a>. Good research practices suggest that having hypotheses ahead of time is a stronger strategy unless the research is truly exploratory.\n\n::: {#bul-characeristic-contrast}\n:::::{.my-bullet-list}\n:::{.my-bullet-list-header}\nBullet List\n:::\n::::{.my-bullet-list-container}\n\n\n- Contrast values add to zero. \n- Each contrast compares two groups. \n- Each category is only isolated one time. \n- The maximum number of contrasts is one less than the number of categories.\n\n::::\n:::::\nCharacteristics of contrasts\n\n:::\n\n***\n\n## Achievement 4: Effect sizes for ANOVA {#sec-chap07-achievement4}\n\nSimilar as the effect size <a class='glossary' title='Cramér’s V is an effect size to determine the strength of the relationship between two categorical variables; often reported with the results of a chi-squared. (SwR, Glossary)'>Cramér’s V</a> for <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a> tests and <a class='glossary' title='Cohen’s d is a standardized effect size for measuring the difference between two group means. It is frequently used to compare a treatment to a control group. It can be a suitable effect size to include with t-test and ANOVA results. (Statistics by Jim)'>Cohen’s d</a> for t-test, there are also effect size indices for ANOVA:\n\n- **<a class='glossary' title='Eta-squared is an [effect size] interpreted as the proportion of variability in the continuous outcome variable that is explained by groups in an analysis of variance; recent research suggests that eta-squared is biased and that [omega-squared] may be a less biased alternative following analysis of variance. (SwR, Glossary)'>eta-squared</a>**: It has a positive bias\n- **<a class='glossary' title='Omega-squared is an effect size for determining the strength of a relationship following an analysis of variance ([ANOVA]) statistical test. In contrast to [eta-squared] it is adjusted to account for the positive bias, and is more stable when assumptions are not completely met. (SwR, Glossary)'>omega-squared</a>**: An unbiased modern alternative\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap07-omega-squared}\n: Formula for omega-squared\n::::::\n:::\n::::{.my-theorem-container}\n$$\n\\omega^2 = \\frac{F - 1}{f + \\frac{n-k+1}{k-1}}\n$$ {#eq-chap07-omega-squared}\n\n- **F**: Statistics from the ANOVA result\n- **n**: Number of observations\n- **k**: Number of groups\n\n::::\n:::::\n\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-omega-squared-manual}\n: Manual computation of effect size omega-squared\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean <- base::readRDS(\"data/chap07/gss_2018_clean.rds\")\n\nusetech_degree_aov <- stats::aov(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean\n)\n\nglue::glue(\"############# ANOVA model USETECH ~ DEGREE ##################\")\n(\n    usetech_degree_aov_summary <- base::summary(usetech_degree_aov)\n)\n\nF_value = usetech_degree_aov_summary[[1]][[\"F value\"]][[1]]\nn = base::nrow(usetech_degree_aov[[\"model\"]])\nk = usetech_degree_aov[[\"rank\"]]\n\n\nglue::glue(\" \")\nglue::glue(\"############# Omega-squared ##################\")\nomega_squared <- (F_value - 1) / (F_value + ((n - k + 1) / (k - 1)))\nomega_squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ############# ANOVA model USETECH ~ DEGREE ##################\n#>               Df  Sum Sq Mean Sq F value Pr(>F)    \n#> DEGREE         4  221301   55325    43.3 <2e-16 ***\n#> Residuals   1404 1793757    1278                   \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> 936 observations deleted due to missingness\n#>  \n#> ############# Omega-squared ##################\n#> [1] 0.1072194\n```\n\n\n:::\n:::\n\n***\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Error in the book’s calculation\n:::\n::::{.my-watch-out-container}\nIn the first term in the book `-1` is missing! The formula is `F-1` that should be therefore `(summ.tech.anova[[1]][1, 4] -1)`. \n\nWith this correction we get the same value as in the manual calculation: 0.1072194.\n::::\n:::::\n\n::::\n:::::\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-chap07-omega-squared}\n: Interpretation of omega-squared effect size\n::::::\n:::\n::::{.my-assessment-container}\n\n- **Small**: $\\omega^2 = .01 \\text{ to } \\omega^2 < .06$\n- **Medium**: $\\omega^2 = .06 \\text{ to } \\omega^2 < .14$\n- **Large**: $\\omega^2 ≥ .14$\n\n***\nThis is quite similar to the eta_squared ($\\eta^2$) assessment as listed in [Computation of Effect Sizes](https://www.psychometrica.de/effect_size.html). \n\n::::\n:::::\n\n::: {.callout-tip}\nThe mean time spent on technology use at work was significantly different across educational attainment groups [F(4, 1404) = 43.3; p < .05], indicating these groups likely came from populations with different mean time spent on technology use. The highest mean was percent of time used for technology for those with graduate degrees. The lowest mean was percent of time for those with less than a high school diploma. A set of planned comparisons found that the mean time spent using technology was statistically significantly (p < .05) lower for (a) those with < high school education (m = 24.8) compared to those with high school or junior college (m = 51.7), (b) those with a high school education (m = 49.61) compared to those with all college groups combined (m = 67.0), (c) those with a junior college degree (m = 62.4) compared to those with a bachelor’s or graduate degree (m = 68.2), and (d) those with a bachelor’s degree (m = 67.9) compared to those with a graduate degree (m = 68.7). Overall, the patterns show statistically significant increases in time spent using technology at work for those with more education. The strength of the relationship between degree and time using technology at work was medium ($ω^2$ = .11).\n:::\n\n## Achievement 5: Testing ANOVA assumptions {#sec-chap07-achievement5}\n\n0. **Continuous variable and independent groups**: This is the prerequisite for ANOVA.\n1. **Normality**: Each sample was drawn from a normally distributed population.\n2. **Equal Variances**: The variances of the populations that the samples come from are equal.\n3. **Independence**: The observations in each group are independent of each other and the observations within groups were obtained by a random sample.\n\n### Testing normality\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap07-testing-normality}\n: Testing normality\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Density plot\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-testing-normality-density-plot}\n: Testing normality with density plots\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean |> \n    tidyr::drop_na(USETECH) |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = USETECH)\n    ) +\n    ggplot2::geom_density(\n        ggplot2::aes(\n            fill = DEGREE\n        )\n    ) +\n    ggplot2::facet_wrap(\n        facets = ggplot2::vars(DEGREE),\n        nrow = 2\n    ) +\n    ggokabeito::scale_fill_okabe_ito(guide = \"none\") +\n    ggplot2::labs(\n        x = \"Percent of time using tech\", \n        y = \"Probability density\")\n```\n\n::: {.cell-output-display}\n![Density Plot: Testing normality of time spent with technology at the job diferentiated by highest educational attainment](07-analysis-of-variance_files/figure-html/fig-testing-normality-density-plot-1.png){#fig-testing-normality-density-plot width=672}\n:::\n:::\n\n***\n\nNone of these graphs looks normally distributed!\n::::\n:::::\n\n###### Q-Q plot small\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-testing-normality-qq-plot1}\n: Testing normality with Q-Q plots\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean  |> \n  tidyr::drop_na(USETECH) |>\n  \n  ggplot2::ggplot(\n      ggplot2::aes(sample = USETECH)\n      ) +\n  ggplot2::geom_abline(\n      ggplot2::aes(\n          intercept = mean(USETECH), \n          slope = sd(USETECH), \n          linetype = \"Normally distributed\"),\n          color = \"gray60\", \n          linewidth = 1\n      ) +\n  ggplot2::stat_qq(\n      ggplot2::aes(color = DEGREE)\n      ) +\n  ggokabeito::scale_color_okabe_ito(guide = \"none\") +\n  ggplot2::scale_linetype_manual(\n      name = \"\",\n      values = 1) +\n  ggplot2::labs(\n      x = \"Theoretical normal distribution\",\n      y = \"Observed values of percent time using tech\"\n      ) +\n  ggplot2::facet_wrap(\n      facets = ggplot2::vars(DEGREE), \n      nrow = 2\n      )\n```\n\n::: {.cell-output-display}\n![Q-Q Plot: Testing normality of time spent with technology at the job diferentiated by highest educational attainment](07-analysis-of-variance_files/figure-html/fig-testing-normality-qq-plot1-1.png){#fig-testing-normality-qq-plot1 width=672}\n:::\n:::\n\n\n***\n\nThe text in the books says that \"none of the groups appeared to be normally distributed based on either type of plot.\" This is ok for me with the density plot. But for me with not so much experience it is difficult to decide with the small Q-Q plots.\n\nThe next tab display the Q-Q plots for each group much bigger.\n\n::::\n:::::\n\n###### Q-Q plot big\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-testing-normality-qq-plot2}\n: Testing normality with Q-Q plots\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean  |> \n  tidyr::drop_na(USETECH) |>\n  \n  ggplot2::ggplot(\n      ggplot2::aes(sample = USETECH)\n      ) +\n  ggplot2::geom_abline(\n      ggplot2::aes(\n          intercept = mean(USETECH), \n          slope = sd(USETECH), \n          linetype = \"Normally distributed\"),\n          color = \"gray60\", \n          linewidth = 1\n      ) +\n  ggplot2::stat_qq(\n      ggplot2::aes(color = DEGREE)\n      ) +\n  ggokabeito::scale_color_okabe_ito(guide = \"none\") +\n  ggplot2::scale_linetype_manual(\n      name = \"\",\n      values = 1) +\n  ggplot2::theme(legend.position = \"top\") +\n  ggplot2::labs(\n      x = \"Theoretical normal distribution\",\n      y = \"Observed values of percent time using tech\"\n      ) +\n  ggplot2::facet_wrap(\n      facets = ggplot2::vars(DEGREE), \n      nrow = 5\n      )\n```\n\n::: {.cell-output-display}\n![Q-Q Plot: Testing normality of time spent with technology at the job diferentiated by highest educational attainment](07-analysis-of-variance_files/figure-html/fig-testing-normality-qq-plot2-1.png){#fig-testing-normality-qq-plot2 width=672}\n:::\n:::\n\n***\n\nNow I can see it very clearly: All groups are not normally distributed. One can also observe that the floor and ceiling values are driving some of the non-normality.\n::::\n:::::\n\n###### Shapiro-Wilk test\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-testing-normality-shapiro-wilk}\n: Numbered R Code Title\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nglue::glue(\"###### Test for the whole numeric USETECH vector #######\")\nstats::shapiro.test(gss_2018_clean$USETECH)\n\nglue::glue(\" \")\nglue::glue(\"###### Test for DEGREE groups of USETECH vector #######\")\ngss_2018_clean |> \n    dplyr::select(USETECH, DEGREE) |> \n    tidyr::drop_na() |> \n    dplyr::group_by(DEGREE) |> \n    dplyr::summarize(\n        shapiro_p_value = stats::shapiro.test(USETECH)$p.value\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ###### Test for the whole numeric USETECH vector #######\n#> \n#> \tShapiro-Wilk normality test\n#> \n#> data:  gss_2018_clean$USETECH\n#> W = 0.8645, p-value < 2.2e-16\n#> \n#>  \n#> ###### Test for DEGREE groups of USETECH vector #######\n#> # A tibble: 5 × 2\n#>   DEGREE         shapiro_p_value\n#>   <fct>                    <dbl>\n#> 1 < high school         1.83e-14\n#> 2 high school           5.99e-24\n#> 3 junior college        2.92e- 9\n#> 4 bachelor              1.22e-16\n#> 5 graduate              4.34e-11\n```\n\n\n:::\n:::\n\n\n***\n\nIn contrast with the systolic blood pressure data we have this time less than 5,000 observations and can therefore apply the <a class='glossary' title='The Shapiro-Wilk test is a statistical test to determine or confirm whether a variable has a normal distribution; it is sensitive to small deviations from normality and not useful for sample sizes above 5,000 because it will nearly always find non-normality. (SwR, Glossary)'>Shapiro-Wilk test</a>  (see @sec-chap06-omnibus-tests).\n\nAll five of the Shapiro-Wilk tests were statistically significant, indicating that the null hypothesis for this test (i.e., the data are normally distributed) has to be rejected for each group.\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n### Testing homogeneity of variances\n\nThe data need to be not only normally distributed, but also spread out equally in each group, e.g. we need equal variances across groups.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-testing-homogeneity-levene-test}\n: Levene’s test: Testing homogeneity of variances\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::leveneTest(\n    y = USETECH ~ DEGREE, \n    data = gss_2018_clean, \n    center = mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Levene's Test for Homogeneity of Variance (center = mean)\n#>         Df F value    Pr(>F)    \n#> group    4  18.312 1.121e-14 ***\n#>       1404                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n***\nThe p-value for the Levene’s test suggests that we need to reject the null hypothesis. The variances of the `USETECH` variable are statistically significantly different across groups (p < .05). The ANOVA fails the assumption of homogeneity of variances.\n::::\n:::::\n\n## Achievement 6: Alternative tests for ANOVA {#sec-chap07-achievement6}\n\n### Homogeneity of variances failed\n\nThere are several tests available when the assumption of homogeneity fails. The book mentions two but there are others as well:\n\n- **Brown-Forsythe F-statistic**: Manually with `stats::oneway.test()` and `onewaytests::bf.test()`\n- **Welch Test**: `stats::oneway.test()` with `var.equal = FALSE`, and `onwaytests::welch.test()`, \n- **Levene's Homogeneity Test**: `car::leveneTest()` with `center = median` (default), `onewaytests::homog.test()` with `method = \"Levene\"`.\n- **Bartlett Homogeneity Test**: `onewaytests::homog.test()` with `method = \"Bartlett\"`\n- **Fligner-Killeen Homogeneity Test**: `onewaytests::homog.test()` with `method = \"Fligner\"`\n\n\n#### Brown-Forsythe F-statistic\n\nThe <a class='glossary' title='Brown-Forsythe is an alternate F-statistic used for analysis of variance when the assumption of homogeneity of variance is not met; the Brown-Forsythe F-statistic is computed after transforming the values of the outcome to represent the distance from the median. (SwR, Glossary)'>Brown-Forsythe F-statistic</a> uses the distance each observation has from the median of the variable. The alternate <a class='glossary' title='F-statistic is a test statistic comparing explained and unexplained variance in [ANOVA] and linear regression. The F-statistic is a ratio where the variation between the groups is compared to the variation within the groups. (SwR, Glossary)'>F-statistic</a> is then computed using the same F formula but with the means computed from the distance to the median instead using the raw values of the continuous variable.\n\n:::::{.my-resource}\n:::{.my-resource-header}\n:::::: {#lem-chap07-brown-forsythe}\nPackages for Brown-Forsythe F-statistic\n::::::\n:::\n::::{.my-resource-container}\nThere are several packages with Brown-Forsythe F-statistic. Most of the explanations and examples in the internet propose {**onewaytests**}. Although this package has a low download figure under 100, I have added it to my package description in @sec-annex-a, because it includes many tests specialized for ANOVA with failed assumptions.\n\nInstead of the Brown-Forsythe F-statistic one could also use <a class='glossary' title='Levene’s test is a statistical test to determine whether observed data meet the homogeneity of variances assumption; Levene’s test is used to test this assumption for t-tests and analysis of variance. (SwR, Glossary)'>Levene’s test</a> from the {**car**} packages with the argument `center = median`.\n\n> Both Levene’s test and Brown-Forsythe test can be used for testing for homogeneity of variances on nonnormal data. But, the Brown-Forsythe test is more robust than Levene’s test when data distributions are skewed or heavy-tailed (Cauchy distribution). Brown-Forsythe test is a modified version of Levene’s test. ([Brown-Forsythe test for equality of variances in R](https://www.reneshbedre.com/blog/brown-forsythe-test-variance.html)) [@bedre2016].\n\nThe {**CGPfunctions**} can’t be downloaded from CRAN beginning with R version 4.0, but you could use the [GitHub version](https://github.com/ibecav/CGPfunctions) after you installed the {**pwr**} package. (See [issue #44](https://github.com/ibecav/CGPfunctions/issues/44)).\n\n\n::: {#tbl-donwload-numbers-brown-forsythe-packages .cell tbl-cap='Download average numbers of packages with Brown-Forsythe tests'}\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 6 × 4\n#>   package      average from       to        \n#>   <chr>          <dbl> <date>     <date>    \n#> 1 car            13044 2024-03-21 2024-03-27\n#> 2 onewaytests       82 2024-03-21 2024-03-27\n#> 3 ALSM              49 2024-03-21 2024-03-27\n#> 4 CGPfunctions      39 2024-03-21 2024-03-27\n#> 5 doex              14 2024-03-21 2024-03-27\n#> 6 homnormal          6 2024-03-21 2024-03-27\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n#### Welch’s F-statistic\n\nWelchs’s F-statistic is an alternate F-statistic used in analysis of variance when the assumption of homogeneity of variance is not met. The calculations for the Welch’s F-statistic use weights to calculate the group means and the grand mean.\n\nInstead of using the complex formula it is better to use R functions.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap07-testing-homogeneity-variances}\n: Testing homogeneity of variances with different tests\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Manual BF\n\n**NHST Step 1**\n\nWrite the null and alternate hypotheses:\n\n::: {.callout-note}\n- **H0**: The mean value of the transformed technology use variable is the same across educational attainment groups.\n- **HA**: The mean value of the transformed technology use variable is not the same across educational attainment groups.\n:::\n\n**NHST Step 2**\n\nCompute the test statistic.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-testing-homogeneity-of-variances-bf-manually}\n: Transform the outcome variable and use the `oneway.test()`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean <- base::readRDS(\"data/chap07/gss_2018_clean.rds\")\n\ngss_2018_clean2 <- gss_2018_clean |> \n    dplyr::group_by(DEGREE) |> \n    dplyr::mutate(\n        usetech_tran = base::abs(USETECH - median(USETECH, na.rm = TRUE))\n        ) |> \n    dplyr::ungroup()\n\nsave_data_file(\"chap07\", gss_2018_clean2, \"gss_2018_clean2.rds\")\n\nbase::summary(gss_2018_clean2$usetech_tran)\n\nbf_test_manual <- stats::oneway.test(\n    formula = usetech_tran ~ DEGREE, \n    data = gss_2018_clean2)\nbf_test_manual\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    0.00   10.00   30.00   30.22   50.00  100.00     936 \n#> \n#> \tOne-way analysis of means (not assuming equal variances)\n#> \n#> data:  usetech_tran and DEGREE\n#> F = 19.747, num df = 4.00, denom df = 364.77, p-value = 9.965e-15\n```\n\n\n:::\n:::\n\n\n\n::::\n:::::\n\n**NHST Step 3**\n\nReview and interpret the test statistics: \nCalculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\nThe p-value in this case is much less than .05. The value of an $F_{BF}$-statistic being this large or larger happens a tiny percentage of the time when the null hypothesis is true.\n\n**NHST Step 4**\n\nConclude and write report.\n\n::: {.callout-tip}\n> The results show a statistically significant difference of the means of the transformed technology use variable by educational attainment group [$F_{BF}(4, 364.77) = 19.747; p < .05$].\n:::\n\n\n###### BF onewaytests\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-testing-homogeneity-of-variances-bf-onewaytests}\n: Testing homogeneity of variances with Brown-Forsythe test from {**onewaytests**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nonewaytests::bf.test(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#>   Brown-Forsythe Test (alpha = 0.05) \n#> ------------------------------------------------------------- \n#>   data : USETECH and DEGREE \n#> \n#>   statistic  : 47.04051 \n#>   num df     : 4 \n#>   denom df   : 769.209 \n#>   p.value    : 2.152414e-35 \n#> \n#>   Result     : Difference is statistically significant. \n#> -------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n###### Welch (book)\n\n\n**NHST Step 1**\n\nWrite the null and alternate hypotheses:\n\n::: {.callout-note}\n- **H0**: Time spent using technology is the same across educational attainment groups.\n- **HA**: Time spent using technology is not the same across educational attainment groups.\n:::\n\n### NHST Step 2\n\nCompute the test statistic. \n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-homogeneity-of-variances-welch-book}\n: Testing homogeneity of variances with Welch’s test (book version)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::oneway.test(\n    formula = USETECH ~ DEGREE, \n    data = gss_2018_clean, \n    var.equal = FALSE\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tOne-way analysis of means (not assuming equal variances)\n#> \n#> data:  USETECH and DEGREE\n#> F = 46.06, num df = 4.00, denom df = 400.31, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n### NHST Step 3\n\nReview and interpret the test statistics: \nCalculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\nThe p-value in this case is < 2.2e-16, which is much less than .05. The value of an $F_{W}$-statistic being this large or larger happens a tiny amount of the time when the null hypothesis is true.\n\n### NHST Step 4\n\nConclude and write report.\n\n::: {.callout-tip}\nThe results show a statistically significant difference in the mean of the USETECH variable by degree group [$F_{W}(4, 400.31) = 46.06; p < .05$].\n:::\n\nIn contrast to the original ANOVA with 1404 <a class='glossary' title='Degree of Freedom (df) is the number of pieces of information that are allowed to vary in computing a statistic before the remaining pieces of information are known; degrees of freedom are often used as parameters for distributions (e.g., chi-squared, F). (SwR, Glossary)'>degrees of freedom</a> in the denominator(see: @tbl-f-test-usetech-degree), we have now with 400.31 fewer degrees of freedom. With fewer degrees of freedom, the <a class='glossary' title='F-statistic is a test statistic comparing explained and unexplained variance in [ANOVA] and linear regression. The F-statistic is a ratio where the variation between the groups is compared to the variation within the groups. (SwR, Glossary)'>F-statistic</a> has to be a larger number to reach statistical significance.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-different-f-distributions}\n: Comparison of different F-distributions with 2000, 25 and 10 degrees of freedom\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot() +\n  ggplot2::xlim(0, 5) +\n  ggplot2::geom_function(fun = df, \n                           args = list(df1 = 4, df2 = 2000), \n                           ggplot2::aes(color = \"4, 2000\"),\n                           linewidth = .7) +\n    ggplot2::geom_function(fun = df, \n                           args = list(df1 = 4, df2 = 25), \n                           ggplot2::aes(color = \"4, 25\"),\n                           linewidth = .7) +\n    ggplot2::geom_function(fun = df, \n                           args = list(df1 = 4, df2 = 10), \n                           ggplot2::aes(color = \"4, 10\"),\n                           linewidth = .7) +\n    ggplot2::geom_function(fun = df, \n                           args = list(df1 = 2, df2 = 2000), \n                           ggplot2::aes(color = \"2, 2000\"),\n                           linewidth = .7) +\n    ggokabeito::scale_color_okabe_ito(\n        name = \"Degress of freedom\\n(num, denominator)\",\n        breaks = c(\"4, 2000\",\"4, 25\",\"4, 10\", \"2, 2000\")\n    )\n```\n\n::: {.cell-output-display}\n![F-distributions with 4 degrees of freedom in the numerator and 10, 25, 2000 in the denominator and one with 2 df and 2000](07-analysis-of-variance_files/figure-html/fig-different-f-distributions-1.png){#fig-different-f-distributions width=672}\n:::\n:::\n\n***\n\nAlthough there isn’t much difference between three of the distributions, the area under the curves is what matters for the <a class='glossary' title='The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary)'>p-value</a> cutoff. When the line is just slightly closer to the x-axis, this changes things quickly for the area under the curve.\n\nThe thresholds for statistical significance (p < .05) for these three lines are \n\n- 2.38 for the 2000 degrees of freedom, \n- 2.76 for the 25 degrees of freedom, and \n- 3.48 for the 10 degrees of freedom. \n\nThe numerator degrees of freedom had a much bigger impact on the significance threshold. For example, an ANOVA with 2 degrees of freedom and the same 2000 degrees of freedom in the denominator would have a threshold for (p < .05) significance of 3.00 instead of 2.38 for the 4 and 2000 threshold. You can see the very different function curve in the graph.\n\n\n::::\n:::::\n\n\n###### Welch onewaytests\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-homogeneity-of-variances-welch-onewaytests}\n: Testing homogeneity of variances with Welch’s test with {**onewaytests**}\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nonewaytests::welch.test(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#>   Welch's Heteroscedastic F Test (alpha = 0.05) \n#> ------------------------------------------------------------- \n#>   data : USETECH and DEGREE \n#> \n#>   statistic  : 46.06012 \n#>   num df     : 4 \n#>   denom df   : 400.3121 \n#>   p.value    : 7.87281e-32 \n#> \n#>   Result     : Difference is statistically significant. \n#> -------------------------------------------------------------\n```\n\n\n:::\n:::\n\n***\n\nWith the exception of a smaller p-value the results are the same as with `stats::oneway.test()`.\n::::\n:::::\n\n\n###### Misc tests with onewaytests\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-testing-homogeneity-of-variances-misc-onewaytests}\n: Testing homogeneity of variances with different tests from the {**onewaytests**} package\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nonewaytests::homog.test(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean,\n    method = \"Levene\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#>   Levene's Homogeneity Test (alpha = 0.05) \n#> ----------------------------------------------- \n#>   data : USETECH and DEGREE \n#> \n#>   statistic  : 18.31161 \n#>   num df     : 4 \n#>   denum df   : 1404 \n#>   p.value    : 1.121265e-14 \n#> \n#>   Result     : Variances are not homogeneous. \n#> -----------------------------------------------\n```\n\n\n:::\n\n```{.r .cell-code}\nonewaytests::homog.test(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean,\n    method = \"Bartlett\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#>   Bartlett's Homogeneity Test (alpha = 0.05) \n#> ----------------------------------------------- \n#>   data : USETECH and DEGREE \n#> \n#>   statistic  : 24.20202 \n#>   parameter  : 4 \n#>   p.value    : 7.27617e-05 \n#> \n#>   Result     : Variances are not homogeneous. \n#> -----------------------------------------------\n```\n\n\n:::\n\n```{.r .cell-code}\nonewaytests::homog.test(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean,\n    method = \"Fligner\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#>   Fligner-Killeen Homogeneity Test (alpha = 0.05) \n#> --------------------------------------------------- \n#>   data : USETECH and DEGREE \n#> \n#>   statistic  : 36.33757 \n#>   parameter  : 4 \n#>   p.value    : 2.465992e-07 \n#> \n#>   Result     : Variances are not homogeneous. \n#> ---------------------------------------------------\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n### Normality failed\n\n#### Kruskal-Wallis test\n\nThe <a class='glossary' title='Kruskal-Wallis test is used to compare ranks across three or more groups when the normal distribution assumption fails for analysis of variance (ANOVA) (SwR, Glossary)'>Kruskal-Wallis test</a> is used to compare three or more groups when the normal distribution assumption fails for ANOVA. Like several of the tests used when the outcome is not normally distributed for a t-test, the Kruskal-Wallis (K-W) test compares ranks among groups. Specifically, the observations are put in order by size, and each is assigned a rank. The mean rank for each group is then computed and used to calculate the K-W test statistic, `H`.\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap07-kruskal-wallis-test}\n: Kruskal-Wallis test\n::::::\n:::\n::::{.my-theorem-container}\n\n$$\nH = \\frac{12}{n(n+1)}\\sum_{j=1}^{k}n_{j}(\\bar{r_{j}}-\\frac{n+1}{2})^2\n$$ {#eq-chap07-kruskal-wallis}\n\n***\n\n- **$n$**: overall sample size\n- **$n_{j}$**: sample size for group $j$\n- **$\\bar{r_{j}}$**: mean rank for group $j$\n\n\n::::\n:::::\n\n\n##### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: {.callout-note}\n- **H0**: The mean rank of technology use is the same across educational attainment groups\n- **HA**: The mean rank of technology use is not the same across educational attainment groups\n:::\n\n##### NHST Step 2\n\nCompute the test statistic. \n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap07-kruskal-wallis-tests}\n: Testing normality with the Kruskal-Wallis test\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### stats\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-kruskal-wallis-stats}\n: Compare using technology by highest educational attainment with `stats::kruskal.test()`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkw_test <- stats::kruskal.test(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean\n)\nkw_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tKruskal-Wallis rank sum test\n#> \n#> data:  USETECH by DEGREE\n#> Kruskal-Wallis chi-squared = 142.21, df = 4, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n***\n\nThe distribution of the H statistic is approximately a <a class='glossary' title='Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary)'>chi-squared</a> distribution, so the R output lists “chi-squared” instead of `H`.\n\n\n::::\n:::::\n\n\n###### onewaytests\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-kruskal-wallist-onewaytests}\n: Compare using technology by highest educational attainment with `onewaytests::kw.test()`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nonewaytests::kw.test(\n    formula = USETECH ~ DEGREE,\n    data = gss_2018_clean\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#>   Kruskal-Wallis Test (alpha = 0.05) \n#> ------------------------------------------------------------- \n#>   data : USETECH and DEGREE \n#> \n#>   statistic  : 142.2141 \n#>   parameter  : 4 \n#>   p.value    : 9.474913e-30 \n#> \n#>   Result     : Difference is statistically significant. \n#> -------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nThe value of the statistic is the same as with `stats::kruskal.test()` but the p-value is lower, e.g. it is more stringent.\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n\n\n\n##### NHST Step 3\n\nReview and interpret the test statistics: \nCalculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\nThe p-value in both tests, as usual, is very tiny. The value of an `H`-statistic being this large or larger happens a tiny percentage of the time when the null hypothesis is true.\n\n##### NHST Step 4\n\nConclude and write report.\n\n::: {.callout-tip}\nThere is a difference in the mean rank for technology use by degree group $[H(4) = 142.21; p < .05]$.\n:::\n\n##### Dunn’s post hoc test for Kruskal-Wallis\n\nLike the ANOVA results, the <a class='glossary' title='Kruskal-Wallis test is used to compare ranks across three or more groups when the normal distribution assumption fails for analysis of variance (ANOVA) (SwR, Glossary)'>Kruskal-Wallis</a> test identifies whether there is a difference somewhere among the means, but it does not identify which groups are different from one another. A post hoc test like <a class='glossary' title='Bonferroni post hoc test is a pairwise test used after a statistically significant ANOVA that conducts a t-test for each pair of means but adjusts the threshold for statistical significance to ensure that there is a small enough risk of Type I error; it is generally considered a very conservative post hoc test that only identifies the largest differences between means as statistically significant. (SwR, Glossary)'>Bonferroni</a> or <a class='glossary' title='Tukey’s Honestly Significant Difference (HSD) is a post hoc test to determine which means are statistically significantly different from each other following a significant ANOVA result; Tukey’s HSD compares each pair of means and so is considered a pairwise test, but it is less conservative than the [Bonferroni] post hoc test. (SwR, Glossary)'>Tukey’s HSD</a> could help. For K-W, the <a class='glossary' title='Dunn’s post hoc test is a pairwise comparisons to determine which groups are statistically significantly different from one another following a significant [Kruskal-Wallis] test. (SwR, Glossary)'>Dunn’s post hoc test of multiple comparisons</a> is useful for identifying which groups are statistically significantly different from which other groups.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap07-dunn-post-hoc-test}\n: Dunn’s post hoc test for Kruskal-Wallis\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### dunn test\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-dunn-test}\n: Dunn’s post hoc test for Kruskal-Wallis\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ndunn.test::dunn.test(\n    x = gss_2018_clean$USETECH,\n    g = gss_2018_clean$DEGREE,\n    method = \"bonferroni\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   Kruskal-Wallis rank sum test\n#> \n#> data: x and group\n#> Kruskal-Wallis chi-squared = 142.2141, df = 4, p-value = 0\n#> \n#> \n#>                            Comparison of x by group                            \n#>                                  (Bonferroni)                                  \n#> Col Mean-|\n#> Row Mean |   < high s   bachelor   graduate   high sch\n#> ---------+--------------------------------------------\n#> bachelor |  -10.43723\n#>          |    0.0000*\n#>          |\n#> graduate |  -9.495755  -0.191842\n#>          |    0.0000*     1.0000\n#>          |\n#> high sch |  -6.834568   6.465520   5.294962\n#>          |    0.0000*    0.0000*    0.0000*\n#>          |\n#> junior c |  -7.755300   1.244464   1.264131  -3.190136\n#>          |    0.0000*     1.0000     1.0000    0.0071*\n#> \n#> alpha = 0.05\n#> Reject Ho if p <= alpha/2\n```\n\n\n:::\n:::\n\n\n***\n\nThe Dunn’s test is a rank-sum test just like the Mann-Whitney U and can be interpreted in the same way. \n\n**No difference in technology use**\n\n- for graduate versus bachelor, \n- junior college versus bachelor, \n- junior college versus graduate.\n\n**Differences in technology use**\n\n- less than high school versus bachelor\n- less than high school versus graduate\n- less than high school versus high school\n- less than high school versus junior college\n- high school versus bachelor\n- high school versus graduate\n- high school versus junior college\n\n::: {.callout-tip}\nThere are differences in technology use between all lower educational attainments (less than high school and high school) versus the higher educational attainments (junior college, bachelor, and graduate). There are also differences between less than high school versus high high school.\n\nThere are no differences between the higher educational attainments junior college, bachelor, and graduate.\n:::\n\n::::\n:::::\n\n\n###### preparing graph\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-preparing-graph-dunn-test}\n: Preparing a graphic for visualizing the differences in the Dunn test\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean3 <- gss_2018_clean2  |> \n    dplyr::mutate(\n        usetech_rank = rank(\n            x = USETECH, \n            na.last = \"keep\"\n            )\n        )\n\nsave_data_file(\"chap07\", gss_2018_clean3, \"gss_2018_clean3.rds\")\n\n# check new variable\nsummary(object = gss_2018_clean3$usetech_rank)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    88.5   357.5   699.5   705.0  1019.0  1272.0     936\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n###### graph\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-graph-dunn-test}\n: Visualizing the differences in the Dunn test\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean3 <-  base::readRDS(\"data/chap07/gss_2018_clean3.rds\")\n\ngss_2018_clean3 |> \n  ggplot2::ggplot(\n      ggplot2::aes(\n          y = usetech_rank, \n          x = DEGREE)\n      ) +\n  ggplot2::geom_jitter(\n      ggplot2::aes(color = DEGREE), \n      alpha = .6,\n      na.rm = TRUE\n      ) +\n  ggplot2::geom_boxplot(\n      ggplot2::aes(fill = DEGREE), \n      alpha = .4,\n      na.rm = TRUE\n      ) +\n  ggokabeito::scale_color_okabe_ito(guide = \"none\") +\n  ggokabeito::scale_fill_okabe_ito(guide = \"none\") +\n  ggplot2::labs(\n      x = \"Educational attainment\", \n      y = \"Ranks of technology use time\"\n      )\n```\n\n::: {.cell-output-display}\n![](07-analysis-of-variance_files/figure-html/graph-dunn-test-1.png){width=672}\n:::\n:::\n\n\n***\n\nThe graph confirms the result: The three college groups were very similar to one another, and there were differences among the other groups.\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n##### Effect size for Kurskal-Wallis test\n\n<a class='glossary' title='Eta-squared is an [effect size] interpreted as the proportion of variability in the continuous outcome variable that is explained by groups in an analysis of variance; recent research suggests that eta-squared is biased and that [omega-squared] may be a less biased alternative following analysis of variance. (SwR, Glossary)'>Eta-squared</a> works for Kruskal-Wallis. (But didn't we say in @sec-chap07-achievement4 that eta-squared has a positive bias and that we therefore should use <a class='glossary' title='Omega-squared is an effect size for determining the strength of a relationship following an analysis of variance ([ANOVA]) statistical test. In contrast to [eta-squared] it is adjusted to account for the positive bias, and is more stable when assumptions are not completely met. (SwR, Glossary)'>omega-squared</a> as outlined in @eq-chap07-omega-squared?) \n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap07-eta-squared}\n: Computing eta-squared effect size\n::::::\n:::\n::::{.my-theorem-container}\n$$\n\\eta_{H}^2 = \\frac{H-k+1}{n-k}\n$$ {#eq-chap07-eta-squared}\n\n***\n\n- **H**: Test statistic\n- **k**: Groups\n- **n**: Number of observations\n\n\n::::\n:::::\n\nBesides a manual calculation I am going also to use {**effectsize**} and {**lsr**}.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap07-eta-squared}\n: Computation of eta-squared\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### manual\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-eta-squared-manual}\n: Computing eta-squared manually\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk = 5 # group size\nn = nrow(tidyr::drop_na(gss_2018_clean, USETECH)) # number of observations\nH = kw_test[[\"statistic\"]][[\"Kruskal-Wallis chi-squared\"]] # statistic\n\n(H - k + 1) / (n - k)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.0984431\n```\n\n\n:::\n:::\n\n***\n\nThe value .098 is smaller than the omega-squared result of .11.\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-chap07-eta-squared}\n: Interpretation of eta-squared effect size\n::::::\n:::\n::::{.my-assessment-container}\n\nThe cutoff values are the same as for <a class='glossary' title='Omega-squared is an effect size for determining the strength of a relationship following an analysis of variance ([ANOVA]) statistical test. In contrast to [eta-squared] it is adjusted to account for the positive bias, and is more stable when assumptions are not completely met. (SwR, Glossary)'>omega-squared</a>:\n\n- **Small**: $\\eta^2 = .01 \\text{ to } \\eta^2 < .06$\n- **Medium**: $\\eta^2 = .06 \\text{ to } \\eta^2 < .14$\n- **Large**: $\\eta^2 ≥ .14$\n::::\n:::::\n\n::: {.callout-tip}\nA Kruskal-Wallis test found a statistically significant difference in technology use time at work across educational attainment groups (H = 142.21; p < .05). Based on a Dunn’s post hoc test, those with less than a high school education had statistically significantly lower mean ranked technology use time than all of the other groups (p < .05), and people with a bachelor’s degree, a graduate degree, or a junior college degree had significantly higher mean ranks than those with a high school diploma. There were no statistically significant differences among the three college groups. There was a medium effect size for the relationship between educational attainment and ranked values of technology use time (η2 = .098).\n:::\n::::\n:::::\n\n\n###### effectsize\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-eta-squared-effectsize}\n: Computing eta-squared with {**effectsize**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\neffectsize::eta_squared(\n    model = usetech_degree_aov\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> For one-way between subjects designs, partial eta squared is equivalent\n#>   to eta squared. Returning eta squared.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # Effect Size for ANOVA\n#> \n#> Parameter | Eta2 |       95% CI\n#> -------------------------------\n#> DEGREE    | 0.11 | [0.08, 1.00]\n#> \n#> - One-sided CIs: upper bound fixed at [1.00].\n```\n\n\n:::\n:::\n\n\n***\nThis is the same result as with omega-squared!\n::::\n:::::\n\n###### lsr\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-eta-squared-lsr}\n: Computing eta-squared with {**effectsize**}\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nlsr::etaSquared(\n    x = usetech_degree_aov\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>           eta.sq eta.sq.part\n#> DEGREE 0.1098235   0.1098235\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n\n\n\n## Achievement 7: Two-way ANOVA {#sec-chap07-achievement7}\n\n### Introduction\n\n- **One-way ANOVA**: A single categorical variable (with 3+ categories) and the means of a continuous variable being compared across the categories.\n- **Two-way ANOVA**: Two categorical variables and the means of a continuous variable being compared across both categories.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap07-usetech-sex-degree}\n: Time spent with technology at job by sex and educational attainment (degree)\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Boxplot: USETECH by SEX\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-boxplot-usetech-sex}\n: Boxplot: Percentage of time using technology at Work by sex \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean <- base::readRDS(\"data/chap07/gss_2018_clean.rds\")\n\ngss_2018_clean |> \n    tidyr::drop_na(USETECH, SEX) |> \n    dplyr::group_by(SEX) |> \n    \n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = SEX,\n            y = USETECH\n        )\n    ) +\n    ggplot2::geom_boxplot(\n        ggplot2::aes(\n            fill = SEX\n        ),\n        alpha = .2\n    ) +\n    ggplot2::geom_jitter(\n        ggplot2::aes(\n            color = SEX\n        ),\n        alpha = 0.6\n    ) +\n    ggokabeito::scale_color_okabe_ito(guide = \"none\") +\n    ggokabeito::scale_fill_okabe_ito(guide = \"none\") +\n    ggplot2::labs(\n        x = \"Sex\",\n        y = \"Percent of work time using technology\"\n    )\n```\n\n::: {.cell-output-display}\n![Percentage of time using technology at Work by sex ](07-analysis-of-variance_files/figure-html/fig-boxplot-usetech-sex-1.png){#fig-boxplot-usetech-sex width=672}\n:::\n:::\n\n***\n\nIt seems that sex has some relationship to time spent on technology use.\n::::\n:::::\n\n\n###### Boxplot: USETECH by SEX & DEGREE\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-boxplot-usetech-sex-degree}\n: Boxplot: Percentage of time using technology at Work by sex & degree\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean |> \n    tidyr::drop_na(USETECH, SEX, DEGREE) |> \n    dplyr::group_by(SEX, DEGREE) |> \n    \n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = DEGREE,\n            y = USETECH\n        )\n    ) +\n    ggplot2::geom_boxplot(\n        ggplot2::aes(\n            fill = SEX\n        ),\n        alpha = .2\n    ) +\n    \n    ggokabeito::scale_color_okabe_ito(guide = \"none\") +\n    ggokabeito::scale_fill_okabe_ito() +\n    ggplot2::labs(\n        x = \"Educational attainment\",\n        y = \"Percent of work time using technology\"\n    )\n```\n\n::: {.cell-output-display}\n![Percentage of time using technology at Work by sex & degree ](07-analysis-of-variance_files/figure-html/fig-boxplot-usetech-sex-degree-1.png){#fig-boxplot-usetech-sex-degree width=672}\n:::\n:::\n\n***\n\nMales have higher use on technology only in the lowest educational achievement group (< high school.). In both highest groups (bachelor and graduate) males and females use technology at the job approximately at the same rate, whereas females with high school or junior college degree spend using technology at the job more time then males.\n::::\n:::::\n\n###### means plot1\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-means-plot1}\n: Means plot of technology use at work by educational attainment and sex\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean |> \n    tidyr::drop_na(USETECH) |> \n    dplyr::group_by(DEGREE, SEX) |>\n    dplyr::summarize(\n        means = base::mean(USETECH),\n        .groups = 'drop'\n    ) |> \n    \n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = DEGREE,\n            y = means,\n            group = SEX\n        )\n    ) +\n    ggplot2::geom_point(\n        ggplot2::aes(\n            color = SEX\n        ),\n        size = 3\n    ) +\n    ggplot2::geom_line(\n        ggplot2::aes(\n            color = SEX\n        ),\n        linewidth = 1,\n        linetype = \"solid\"\n    ) +\n    ggplot2::ylim(0, 100) +\n    ggplot2::labs(\n        x = \"Educational attainment\",\n        y = \"Percent of time spent using technology\"\n        ) +\n    ggokabeito::scale_color_okabe_ito()\n```\n\n::: {.cell-output-display}\n![Means plot of technology use at work by educational attainment and sex](07-analysis-of-variance_files/figure-html/fig-means-plot1-1.png){#fig-means-plot1 width=672}\n:::\n:::\n\n\n::::\n:::::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Using `group` in the aesthetic of the first layer\n:::\n::::{.my-watch-out-container}\nIn this code chunk I have prepared the data.frame, e.g. calculated the means that I used for the graph. But then I needed inside the graph the equivalent of `dplyr::group_by()` as well. Normally the grouping is done implicitly but there are some common cases where the default does not work correctly: \n\n- At first I tried to connect the categorical data with a line. This requires the argument `group = 1`.\n- I needed a `SEX` grouping additionally to the display of `USETECH` by group means.\n\n(See for more detail the help file [Aesthetics: grouping](https://ggplot2.tidyverse.org/reference/aes_group_order.html))\n\n::::\n:::::\n\n###### means plot2\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-means-plot2}\n: Means plot of technology use at work by educational attainment and sex\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean |>\n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = DEGREE,\n            y = USETECH,\n            fill = SEX\n            )\n    ) +\n    ggplot2::stat_summary(\n        fun =  mean,\n        geom = \"point\",\n        size = 3,\n        na.rm = TRUE,\n        ggplot2::aes(\n            color = SEX\n        )\n    ) +\n    ggplot2::stat_summary(\n        fun = mean,\n        geom = \"line\",\n        linewidth = 1,\n        linetype = \"solid\",\n        na.rm = TRUE,\n        ggplot2::aes(\n            group = SEX,\n            color = SEX\n        )\n    ) +\n    ggplot2::ylim(0, 100) +\n    ggplot2::labs(\n        x = \"Educational attainment\",\n        y = \"Percent of time spent using technology\"\n        ) +\n    ggokabeito::scale_color_okabe_ito()\n```\n\n::: {.cell-output-display}\n![Means plot of technology use at work by educational attainment and sex](07-analysis-of-variance_files/figure-html/fig-means-plot2-1.png){#fig-means-plot2 width=672}\n:::\n:::\n\n\n::::\n:::::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Using `ggplot2::stat_summary()`\n:::\n::::{.my-watch-out-container}\nI have still not much experience with `ggplot2::stat_summary()`. I used it the first time in @cnj-chap03-book-revised, but in that occasion I tried to prevent a warning message and applied in other respects the code provided by the book.\n\nThis time I used it for a group summary, e.g. a calculation I would have done with `dplyr::group_by()` and `dplyr::summarize()`.\n::::\n:::::\n\n###### means data1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-means-data1}\n: Comparing means of technology use at work by educational attainment and sex\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_sd_usetech <- gss_2018_clean |> \n  dplyr::group_by(DEGREE, SEX)  |> \n  tidyr::drop_na(USETECH)  |> \n  dplyr::summarize(\n      mean_usetech = base::mean(USETECH),\n      sd_usetech = stats::sd(USETECH),\n      .groups = 'drop'\n      )\nmean_sd_usetech\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 10 × 4\n#>    DEGREE         SEX    mean_usetech sd_usetech\n#>    <fct>          <fct>         <dbl>      <dbl>\n#>  1 < high school  male           25.7       35.4\n#>  2 < high school  female         23.7       37.5\n#>  3 high school    male           43.5       37.8\n#>  4 high school    female         55.9       38.6\n#>  5 junior college male           47.0       36.8\n#>  6 junior college female         70.4       31.7\n#>  7 bachelor       male           68.0       33.1\n#>  8 bachelor       female         67.7       31.2\n#>  9 graduate       male           72.1       29.2\n#> 10 graduate       female         65.9       30.9\n```\n\n\n:::\n:::\n\n***\n\nThe biggest difference in time using technology at the job between male and female is in the group with a junior college degree.\n\nBut instead of using a visual calculation by head it is better to compute it in detail.\n::::\n:::::\n\n###### means data2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-means-data2}\n: Comparing means of technology use at work by educational attainment and sex\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ndiff_sex_degree <- gss_2018_clean |> \n  dplyr::group_by(DEGREE, SEX)  |> \n  tidyr::drop_na(USETECH)  |> \n  dplyr::summarize(\n      mean_usetech = base::mean(USETECH),\n      sd_usetech = stats::sd(USETECH),\n      .groups = 'drop'\n      ) |> \n    dplyr::group_by(DEGREE) |> \n    dplyr::summarize(diffs = diff(mean_usetech))\n\ndiff_means <- dplyr::full_join(\n    x = mean_sd_usetech,\n    y = diff_sex_degree,\n    by = dplyr::join_by(DEGREE)\n)\n\ndiff_means\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 10 × 5\n#>    DEGREE         SEX    mean_usetech sd_usetech  diffs\n#>    <fct>          <fct>         <dbl>      <dbl>  <dbl>\n#>  1 < high school  male           25.7       35.4 -2.04 \n#>  2 < high school  female         23.7       37.5 -2.04 \n#>  3 high school    male           43.5       37.8 12.3  \n#>  4 high school    female         55.9       38.6 12.3  \n#>  5 junior college male           47.0       36.8 23.4  \n#>  6 junior college female         70.4       31.7 23.4  \n#>  7 bachelor       male           68.0       33.1 -0.276\n#>  8 bachelor       female         67.7       31.2 -0.276\n#>  9 graduate       male           72.1       29.2 -6.17 \n#> 10 graduate       female         65.9       30.9 -6.17\n```\n\n\n:::\n:::\n\n***\n\nThe biggest difference in time using technology at the job between male and female is in the group with a junior college degree. The second biggest difference is only half of the first and happens in the high school group. Again about only half is the third difference in graduate, this time the sign is reversed.\n\nThese differences confirms quite well with @fig-means-plot1 and @fig-means-plot2.\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n@fig-means-plot1 and @fig-means-plot2 show that there interaction between the two variable `SEX` and `DEGREE.` because the lines are not parallel. Parallel lines would show that the mean of the continuous variable is consistently higher or lower for certain groups compared to others. Parallel lines indicate that there is no interaction between the two categorical variables. \n\nBut when the means plot --- as in our example --- shows lines that diverge or cross then there is an interaction between the categorical variables. The mean of the continuous variable is different at different levels of one categorical variable depending on the value of the other categorical variable. For example, mean technology use is lower for females compared to males for the lowest and highest educational attainment categories, but female technology use is higher than male technology use for the three other categories of educational attainment. The two variables are working together to influence the value of technology use.\n\n### The two-way ANOVA NHST\n\n\n#### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: {.callout-note}\n- **H0**: The mean time using technology at work is the same across groups of degree and sex. There is no interaction between the variables `DEGREES` and `SEX.`\n- **HA**: The mean time using technology at work is not the same across groups of degree and sex. There is an interaction between the variables `DEGREES` and `SEX.`\n:::\n\n#### NHST Step 2\n\nCompute the test statistic. \n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-two-way-anova}\n: Two-way ANOVA: Using technology by educational attainment and sex\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbase::summary(\n    stats::aov(\n    formula = USETECH ~ DEGREE * SEX,\n    data = gss_2018_clean\n    )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>               Df  Sum Sq Mean Sq F value   Pr(>F)    \n#> DEGREE         4  221301   55325  44.209  < 2e-16 ***\n#> SEX            1   16473   16473  13.163 0.000296 ***\n#> DEGREE:SEX     4   26510    6627   5.296 0.000311 ***\n#> Residuals   1399 1750775    1251                     \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> 936 observations deleted due to missingness\n```\n\n\n:::\n:::\n\n***\n\nThere are three F-statistics for this ANOVA: one for each of the two individual variables (the <a class='glossary' title='Main effect is the relationship between only one of the independent variables and the dependent variable, ignoring the impact of any additional independent variables or interaction effects. (SwR, Glossary)'>main effects</a>), and one for the interaction term.\n\nWhen the interaction term is statistically significant, it is interpreted first and the main effects are only interpreted if a main effect variable influences the outcome alone.\n\n**Does a main effect variable influence the outcome alone?**\n\n- If you cannot say anything about the influence of `SEX` on `USETECH` without mentioning `DEGREE` than there was no main effect of `SEX`.\n- If tech use had started lower for females than males but increased consistently and statistically significantly for both males and females as educational attainment increased, this would be a main effect of DEGREE. But this is in our example not the case.\n\n**Conclusion**\n\nThere is no main effect variable that influences the outcome alone, therefore a repor contains only the interaction term.\n::::\n:::::\n\n\n#### NHST Step 3\n\nReview and interpret the test statistics: \nCalculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\nThe p-value for the interaction term is with 0.000311 very tiny, and so the value of an F-statistic being as large or larger than the F-statistic for the interaction term happens a tiny percentage of the time when the null hypothesis is true.\n\n#### NHST Step 4\n\nConclude and write report.\n\n::: {.callout-tip}\nThere was a statistically significant interaction between degree and sex on mean percent of work time spent using technology [F(4, 1399) = 5.3; p < .001]. The highest mean was 72.1% of time used for technology for males with graduate degrees. The lowest mean was 23.7% of the time for females with less than a high school diploma. The interaction between degree and sex shows that time spent on technology use increases more quickly for females, with both males and females eventually having high tech use in the top two educational attainment groups.\n:::\n\n### Post hoc test for two-way ANOVA\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-two-way-anova-tukey-test}\n: Tukey’s HSD post hoc test for two-way ANOVA of USETECH by DEGREE and SEX\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::TukeyHSD(\n    stats::aov(\n    formula = USETECH ~ DEGREE * SEX,\n    data = gss_2018_clean\n    )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   Tukey multiple comparisons of means\n#>     95% family-wise confidence level\n#> \n#> Fit: stats::aov(formula = USETECH ~ DEGREE * SEX, data = gss_2018_clean)\n#> \n#> $DEGREE\n#>                                    diff       lwr       upr     p adj\n#> high school-< high school    24.8247754 15.244768 34.404783 0.0000000\n#> junior college-< high school 37.6070313 25.329478 49.884584 0.0000000\n#> bachelor-< high school       43.0859568 32.760484 53.411429 0.0000000\n#> graduate-< high school       43.9107249 32.376284 55.445165 0.0000000\n#> junior college-high school   12.7822558  3.459487 22.105024 0.0017563\n#> bachelor-high school         18.2611813 11.719691 24.802671 0.0000000\n#> graduate-high school         19.0859494 10.766152 27.405746 0.0000000\n#> bachelor-junior college       5.4789255 -4.608337 15.566188 0.5733923\n#> graduate-junior college       6.3036936 -5.018002 17.625389 0.5490670\n#> graduate-bachelor             0.8247681 -8.343540  9.993076 0.9991960\n#> \n#> $SEX\n#>                diff      lwr      upr     p adj\n#> female-male 6.80899 3.108699 10.50928 0.0003174\n#> \n#> $`DEGREE:SEX`\n#>                                                   diff         lwr         upr\n#> high school:male-< high school:male         17.8132060   2.7275183  32.8988937\n#> junior college:male-< high school:male      21.3181818  -0.4992077  43.1355713\n#> bachelor:male-< high school:male            42.3151914  25.7902764  58.8401064\n#> graduate:male-< high school:male            46.3538961  27.5496712  65.1581210\n#> < high school:female-< high school:male     -2.0378788 -22.6075109  18.5317533\n#> high school:female-< high school:male       30.1500000  15.0344692  45.2655308\n#> junior college:female-< high school:male    44.7418831  26.3028236  63.1809427\n#> bachelor:female-< high school:male          42.0396406  25.8082011  58.2710800\n#> graduate:female-< high school:male          40.1813241  22.0984520  58.2641962\n#> junior college:male-high school:male         3.5049758 -14.4610385  21.4709901\n#> bachelor:male-high school:male              24.5019854  13.5542915  35.4496792\n#> graduate:male-high school:male              28.5406901  14.3851943  42.6961858\n#> < high school:female-high school:male      -19.8510848 -36.2793820  -3.4227876\n#> high school:female-high school:male         12.3367940   3.6616307  21.0119573\n#> junior college:female-high school:male      26.9286771  13.2619985  40.5953557\n#> bachelor:female-high school:male            24.2264346  13.7269673  34.7259018\n#> graduate:female-high school:male            22.3681181   9.1859540  35.5502821\n#> bachelor:male-junior college:male           20.9970096   1.8065820  40.1874372\n#> graduate:male-junior college:male           25.0357143   3.8508477  46.2205808\n#> < high school:female-junior college:male   -23.3560606 -46.1224714  -0.5896498\n#> high school:female-junior college:male       8.8318182  -9.1592621  26.8228985\n#> junior college:female-junior college:male   23.4237013   2.5622868  44.2851158\n#> bachelor:female-junior college:male         20.7214588   1.7831557  39.6597618\n#> graduate:female-junior college:male         18.8631423  -1.6841193  39.4104039\n#> graduate:male-bachelor:male                  4.0387047 -11.6416301  19.7190396\n#> < high school:female-bachelor:male         -44.3530702 -62.1121183 -26.5940220\n#> high school:female-bachelor:male           -12.1651914 -23.1539720  -1.1764108\n#> junior college:female-bachelor:male          2.4266917 -12.8138117  17.6671952\n#> bachelor:female-bachelor:male               -0.2755508 -12.7548798  12.2037783\n#> graduate:female-bachelor:male               -2.1338673 -16.9414427  12.6737082\n#> < high school:female-graduate:male         -48.3917749 -68.2892584 -28.4942914\n#> high school:female-graduate:male           -16.2038961 -30.3911918  -2.0166004\n#> junior college:female-graduate:male         -1.6120130 -19.2981376  16.0741116\n#> bachelor:female-graduate:male               -4.3142555 -19.6849976  11.0564866\n#> graduate:female-graduate:male               -6.1725720 -23.4870269  11.1418829\n#> high school:female-< high school:female     32.1878788  15.7321731  48.6435845\n#> junior college:female-< high school:female  46.7797619  27.2270154  66.3325084\n#> bachelor:female-< high school:female        44.0775194  26.5912218  61.5638170\n#> graduate:female-< high school:female        42.2192029  23.0019908  61.4364150\n#> junior college:female-high school:female    14.5918831   0.8922699  28.2914963\n#> bachelor:female-high school:female          11.8896406   1.3473395  22.4319416\n#> graduate:female-high school:female          10.0313241  -3.1849820  23.2476303\n#> bachelor:female-junior college:female       -2.7022425 -17.6240305  12.2195454\n#> graduate:female-junior college:female       -4.5605590 -21.4777217  12.3566037\n#> graduate:female-bachelor:female             -1.8583165 -16.3376501  12.6210171\n#>                                                p adj\n#> high school:male-< high school:male        0.0072699\n#> junior college:male-< high school:male     0.0619111\n#> bachelor:male-< high school:male           0.0000000\n#> graduate:male-< high school:male           0.0000000\n#> < high school:female-< high school:male    0.9999995\n#> high school:female-< high school:male      0.0000000\n#> junior college:female-< high school:male   0.0000000\n#> bachelor:female-< high school:male         0.0000000\n#> graduate:female-< high school:male         0.0000000\n#> junior college:male-high school:male       0.9998264\n#> bachelor:male-high school:male             0.0000000\n#> graduate:male-high school:male             0.0000000\n#> < high school:female-high school:male      0.0052315\n#> high school:female-high school:male        0.0003049\n#> junior college:female-high school:male     0.0000000\n#> bachelor:female-high school:male           0.0000000\n#> graduate:female-high school:male           0.0000039\n#> bachelor:male-junior college:male          0.0192892\n#> graduate:male-junior college:male          0.0071871\n#> < high school:female-junior college:male   0.0389231\n#> high school:female-junior college:male     0.8690307\n#> junior college:female-junior college:male  0.0141081\n#> bachelor:female-junior college:male        0.0192858\n#> graduate:female-junior college:male        0.1039186\n#> graduate:male-bachelor:male                0.9983501\n#> < high school:female-bachelor:male         0.0000000\n#> high school:female-bachelor:male           0.0167764\n#> junior college:female-bachelor:male        0.9999688\n#> bachelor:female-bachelor:male              1.0000000\n#> graduate:female-bachelor:male              0.9999867\n#> < high school:female-graduate:male         0.0000000\n#> high school:female-graduate:male           0.0113631\n#> junior college:female-graduate:male        0.9999998\n#> bachelor:female-graduate:male              0.9967894\n#> graduate:female-graduate:male              0.9816675\n#> high school:female-< high school:female    0.0000000\n#> junior college:female-< high school:female 0.0000000\n#> bachelor:female-< high school:female       0.0000000\n#> graduate:female-< high school:female       0.0000000\n#> junior college:female-high school:female   0.0261888\n#> bachelor:female-high school:female         0.0133486\n#> graduate:female-high school:female         0.3233313\n#> bachelor:female-junior college:female      0.9999069\n#> graduate:female-junior college:female      0.9976459\n#> graduate:female-bachelor:female            0.9999951\n```\n\n\n:::\n:::\n\n***\n\n1. The first section under `$DEGREE` was comparing groups of `DEGREE` to each other. \n2. The second section under `$SEX` was comparing males and females to each other. \n3. The third section was the interaction, comparing groups of `DEGREE*SEX` with each other.\n    a) The first row in this last section is high school:male-< high school:male, which compares high school male to less than high school male.\n    b) The difference between the means: diff = 17.81.\n    c) The confidence interval is found around the difference (95% CI: 2.73 to 32.90).\n    d) The p-value for the difference between the means (p = 0.007) is statistically significant\n\nThis indicates that there is a statistically significant (p < .05) difference of 17.81 between the mean percentage time of technology use for males with less than high school compared to males with high school *in the sample*, and that the difference between the mean of these two groups is likely somewhere between 2.73 and 32.90 in the population this sample came from.\n\n\nThere are so many groups with significant differences that it would be more useful to just include the boxplot from the exploratory analysis or the means plot of @fig-means-plot1 and a paragraph about any interesting overall patterns in the comparisons. For instance:\n\n::: {.callout-tip}\nThere is a trend that with higher educational attainment the time spent with technology at work grows. Starting from the same relatively low level of 25% the use of technology with higher degrees rises faster for female than for males until the junior college degree. In the following higher educational attainment groups (bachelor and graduate) the time used with technology decreases for females whereas it continues to increase for males. Both trends cross at the bachelor level with about 68% and reach 66% for female and 72% for males at the graduate level. \n\nThe biggest difference in time using technology at the job between male and female is in the group with a junior college degree (`junior college:female-junior college:male`). \n\nThe second biggest difference is only half of the first and happens in the high school group (`high school:female-< high school:male`). \n\nAll the other differences between female and males at the same degree level are not statistically significant with p <= .05.\n:::\n \n \n::::\n:::::\n\n### Two-way ANOVA assumptions\n\n#### Normality\n\nTesting the normality assumption is more difficult as with the one-way ANOVA, we would need to look at each group. Since there are five degree groups, two sex groups, and 10 degree-by-sex groups (e.g., male and less than high school). Instead of checking normality one group at a time when there are a large number of groups in an ANOVA model, this assumption can be checked by examining the <a class='glossary' title='Residuals are the differences between the observed values and the predicted values. (SwR, Glossary)'>residuals</a>. The residuals are the distances between the value of the outcome for each person and the value of the group mean for that person. When the residuals are normally distributed, this indicates that the values in each group are normally distributed around the group mean.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap07-testing-normality-two-way-anova}\n: Testing normality for two-way ANOVA\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Shapiro-Wilk test\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-test-normality-residuals-shapiro-wilk}\n: Shapiro-Wilk test for normal distribution by groups\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::shapiro.test(\n    aov_test <- stats::aov(\n    formula = USETECH ~ DEGREE * SEX,\n    data = gss_2018_clean\n    )$residuals\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tShapiro-Wilk normality test\n#> \n#> data:  aov_test <- stats::aov(formula = USETECH ~ DEGREE * SEX, data = gss_2018_clean)$residuals\n#> W = 0.95984, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n***\nInstead of providing the result of the model fitting with the `stats::aov()` function I have nested the function into the <a class='glossary' title='The Shapiro-Wilk test is a statistical test to determine or confirm whether a variable has a normal distribution; it is sensitive to small deviations from normality and not useful for sample sizes above 5,000 because it will nearly always find non-normality. (SwR, Glossary)'>Shapiro-Wilk</a> test. The statistically significant p-value means that we have to reject the null hypotheses (\"The distribution is normal\"). So, this test shows that the residuals fail the normality assumption.\n\nTo confirm the result it is helpful to graph the residuals.\n\n::::\n:::::\n\n\n###### Graphing the residuals\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-testing-normality-residuals-graph}\n: Inspecting normality with a graph of the residuals\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov_residuals_df <- tibble::tibble(residuals =\n        stats::aov(\n        formula = USETECH ~ DEGREE * SEX,\n        data = gss_2018_clean\n        )$residuals\n    )\n\np1 <- my_hist_dnorm(\n    df = aov_residuals_df,\n    v = aov_residuals_df$residuals,\n    n_bins = 30,\n    x_label = \"Residuals with 30 bins as in the book\")\n\np2 <- my_hist_dnorm(\n    df = aov_residuals_df,\n    v = aov_residuals_df$residuals,\n    n_bins = 20,\n    x_label = \"Residuals with 20 bins\")\n\ngridExtra::grid.arrange(\n    p1, p2,\n    nrow = 2\n)\n```\n\n::: {.cell-output-display}\n![Distribution of residuals from ANOVA explaining tech use at work based on educational attainment and sex](07-analysis-of-variance_files/figure-html/testing-normality-residuals-graph-1.png){width=672}\n:::\n:::\n\n***\nThe graph is clearly not normal. The insinuated bi-modality in the text of the book is in my opinion an artifact of the chosen number of bins. To compare I have graphed two histograms with 30 and 20 bins.\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n#### Homogeneity of variances\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-two-way-levene-test}\n: Levene’s test for two-way ANOVA\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nglue::glue(\"############### default: `center = mean` ####################\")\ncar::leveneTest(\n    y = USETECH ~ DEGREE * SEX,\n    center = mean,\n    data = gss_2018_clean\n)\n\nglue::glue(\" \")\nglue::glue(\"############### default: `center = median` #################\")\ncar::leveneTest(\n    y = USETECH ~ DEGREE * SEX,\n    center = median,\n    data = gss_2018_clean\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ############### default: `center = mean` ####################\n#> Levene's Test for Homogeneity of Variance (center = mean)\n#>         Df F value    Pr(>F)    \n#> group    9  8.5912 1.289e-12 ***\n#>       1399                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#>  \n#> ############### default: `center = median` #################\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>         Df F value    Pr(>F)    \n#> group    9  7.1573 3.324e-10 ***\n#>       1399                      \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n***\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! What is the center of each group?\n:::\n::::{.my-watch-out-container}\nUsing the means at the center is the original test. The default `center = median` provides a more robust test. But even this more robust test is also statistically significant. So the homogeneity of variance has to be rejected in both cases.\n::::\n:::::\n\n\n\nThe results were statistically significant so the null hypothesis was rejected in this test as well. The equal variances assumption was not met. \n\nThe two-way ANOVA had failed its assumptions.\n::::\n:::::\n\n### Alternatives\n\n#### Friedman test\n\nThe books mentions `stats::friedman.test()`. But it turns out that the test result is not statistically significant. This seems unlikely given all the other information we have about the data distribution.\n\n#### Compute ANOVA with ranks of the otucome variable\n\nWe had already computed the ranks of USETECH for the Dunn’s test earlier in @exm-chap07-dunn-post-hoc-test. We can load the appropriate `gss_2018_clean3.rds` data and use the variable `usetech_rank`.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap07-two-way-ranked-anova}\n: Two-way ANOVA ranked technology use by degree and sex\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\ngss_2018_clean3 <- base::readRDS(\"data/chap07/gss_2018_clean3.rds\")\n\nbase::summary(\n    stats::aov(\n        formula = usetech_rank ~ DEGREE * SEX, \n        data = gss_2018_clean3\n        )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>               Df    Sum Sq Mean Sq F value   Pr(>F)    \n#> DEGREE         4  23270305 5817576   40.26  < 2e-16 ***\n#> SEX            1   1849104 1849104   12.80 0.000359 ***\n#> DEGREE:SEX     4   3120976  780244    5.40 0.000258 ***\n#> Residuals   1399 202148767  144495                     \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> 936 observations deleted due to missingness\n```\n\n\n:::\n:::\n\n***\n\n::: {.callout-tip}\nA two-way ANOVA with ranked technology time use as the outcome found a statistically significant interaction between degree and sex (p < .05). The overall pattern of results indicates that males and females with less than a high school education use technology the least, while those with higher educational attainment use technology the most. Males and females differ a lot in use of technology for those with a junior college degree, with females having a junior college degree having the highest use of technology of all females.\n:::\n\n::::\n:::::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! What's the difference between ranked and not ranked?\n:::\n::::{.my-watch-out-container}\nI have to confess that I do not understand the difference between our (wrong?) tests with assuming that the normality and homogeneity of variance assumptions hold and here now the ranked version of the two-way ANOVA. It seems to me that the results are the same. Even the both graphs are almost identical.\n::::\n:::::\n\n\n#### Box-Cox transformation\n\nAnother suggestion for alternative test is the Box-Cox transformation on the outcome variable. The Box-Cox power transformations were developed to reduce the non-normality of residuals, so they might be useful here. The original paper by Box and Cox explains the transformations [@box1964], and there are numerous tutorials on the use of them [@wikipedia2021a; @zach2020; @glenn.db; @lane2017].\n\n\n\n## Exercises (empty)\n\n\n\n## Glossary\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> ANOVA </td>\n   <td style=\"text-align:left;\"> Analysis of variance is a statistical method used to compare means across groups to determine whether there is a statistically significant difference among the means; typically used when there are three or more means to compare. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Bonferroni </td>\n   <td style=\"text-align:left;\"> Bonferroni post hoc test is a pairwise test used after a statistically significant ANOVA that conducts a t-test for each pair of means but adjusts the threshold for statistical significance to ensure that there is a small enough risk of Type I error; it is generally considered a very conservative post hoc test that only identifies the largest differences between means as statistically significant. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Brown-Forsythe </td>\n   <td style=\"text-align:left;\"> Brown-Forsythe is an alternate F-statistic used for analysis of variance when the assumption of homogeneity of variance is not met; the Brown-Forsythe F-statistic is computed after transforming the values of the outcome to represent the distance from the median. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Ceiling </td>\n   <td style=\"text-align:left;\"> A ceiling effect happens when many observations are at the highest possible value for a variable. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Chi-squared </td>\n   <td style=\"text-align:left;\"> Chi-squared is the test statistic following the chi-squared probability distribution; the chi-squared test statistic is used in inferential tests, including examining the association between two categorical variables and determining statistical significance for a logistic regression model. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Cohen’s d </td>\n   <td style=\"text-align:left;\"> Cohen’s d is a standardized effect size for measuring the difference between two group means. It is frequently used to compare a treatment to a control group. It can be a suitable effect size to include with t-test and ANOVA results. (&lt;a href= \"https://statisticsbyjim.com/basics/cohens-d/\"&gt;Statistics by Jim&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Contrasts </td>\n   <td style=\"text-align:left;\"> Contrasts are sets of numbers used in planned contrasts to specify which means or groups of means to compare to each other, usually to identify statistically significant differences among means after a statistically significant analysis of variance. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Cramér’s V </td>\n   <td style=\"text-align:left;\"> Cramér’s V is an effect size to determine the strength of the relationship between two categorical variables; often reported with the results of a chi-squared. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> CVD </td>\n   <td style=\"text-align:left;\"> Color vision deficiency (CVD) or color blindness (also spelled colour blindness) includes a wide range of causes and conditions and is actually quite complex. It's a condition characterized by an inability or difficulty in perceiving and differentiating certain colors due to abnormalities in the three color-sensing pigments of the cones in the retina. (&lt;a href=\"https://enchroma.com/pages/types-of-color-blindness\"&gt;EnChroma&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Degrees of Freedom </td>\n   <td style=\"text-align:left;\"> Degree of Freedom (df) is the number of pieces of information that are allowed to vary in computing a statistic before the remaining pieces of information are known; degrees of freedom are often used as parameters for distributions (e.g., chi-squared, F). (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Dunn </td>\n   <td style=\"text-align:left;\"> Dunn’s post hoc test is a pairwise comparisons to determine which groups are statistically significantly different from one another following a significant [Kruskal-Wallis] test. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Effect Size </td>\n   <td style=\"text-align:left;\"> Effect size is a measure of the strength of a relationship; effect sizes are important in inferential statistics in order to determine and communicate whether a statistically significant result has practical importance. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Eta-squared </td>\n   <td style=\"text-align:left;\"> Eta-squared is an [effect size] interpreted as the proportion of variability in the continuous outcome variable that is explained by groups in an analysis of variance; recent research suggests that eta-squared is biased and that [omega-squared] may be a less biased alternative following analysis of variance. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> F-Statistic </td>\n   <td style=\"text-align:left;\"> F-statistic is a test statistic comparing explained and unexplained variance in [ANOVA] and linear regression. The F-statistic is a ratio where the variation between the groups is compared to the variation within the groups. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Familywise </td>\n   <td style=\"text-align:left;\"> Familywise error is the alpha or Type I error rate when conducting multiple statistical tests. A large familywise alpha is one of the reasons that analysis of variance is preferable to conducting multiple t-tests when comparing means across more than two groups. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Floor </td>\n   <td style=\"text-align:left;\"> A floor effect happens when a variable has many observations that take the lowest value of the variable, which can indicate that the range of values was insufficient to capture the true variability of the data. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Grand </td>\n   <td style=\"text-align:left;\"> Grand mean is the overall mean of a continuous variable that is used to determine distances from the mean for individuals and groups in ANOVA. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> HSD </td>\n   <td style=\"text-align:left;\"> Tukey’s Honestly Significant Difference (HSD) is a post hoc test to determine which means are statistically significantly different from each other following a significant ANOVA result; Tukey’s HSD compares each pair of means and so is considered a pairwise test, but it is less conservative than the Bonferroni post hoc test. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Kruskal-Wallis </td>\n   <td style=\"text-align:left;\"> Kruskal-Wallis test is used to compare ranks across three or more groups when the normal distribution assumption fails for analysis of variance (ANOVA) (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Kurtosis </td>\n   <td style=\"text-align:left;\"> Kurtosis is a measure of how many observations are in the tails of a distribution; distributions that look bell-shaped, but have a lot of observations in the tails (platykurtic) or very few observations in the tails (leptokurtic) (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Levene </td>\n   <td style=\"text-align:left;\"> Levene’s test is a statistical test to determine whether observed data meet the homogeneity of variances assumption; Levene’s test is used to test this assumption for t-tests and analysis of variance. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Main Effect </td>\n   <td style=\"text-align:left;\"> Main effect is the relationship between only one of the independent variables and the dependent variable, ignoring the impact of any additional independent variables or interaction effects. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Odds Ratio </td>\n   <td style=\"text-align:left;\"> Odds is usually defined in statistics as the probability an event will occur divided by the probability that it will not occur. An odds ratio (OR) is a measure of association between a certain property A and a second property B in a population. Specifically, it tells you how the presence or absence of property A has an effect on the presence or absence of property B. (&lt;a href=\"https://www.statisticshowto.com/probability-and-statistics/probability-main-index/odds-ratio/\"&gt;Statistics How To&lt;/a&gt;). An odds ratio is a ratio of two ratios. They quantify the strength of the relationship between two conditions. They indicate how likely an outcome is to occur in one context relative to another. (&lt;a href=\"https://statisticsbyjim.com/probability/odds-ratio/\"&gt;Statistics by Jim&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Omega-squared </td>\n   <td style=\"text-align:left;\"> Omega-squared is an effect size for determining the strength of a relationship following an analysis of variance ([ANOVA]) statistical test. In contrast to [eta-squared] it is adjusted to account for the positive bias, and is more stable when assumptions are not completely met. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Omnibus </td>\n   <td style=\"text-align:left;\"> An omnibus is a statistical test that identifies that there is some relationship going on between variables, but not what that relationship is. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p-hacking </td>\n   <td style=\"text-align:left;\"> P-hacking is a set of statistical decisions and methodology choices during research that artificially produces statistically significant results. These decisions increase the probability of false positives—where the study indicates an effect exists when it actually does not. P-hacking is also known as data dredging, data fishing, and data snooping. (&lt;a href=\"https://statisticsbyjim.com/hypothesis-testing/p-hacking/\"&gt;Statistics by Jim&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p-value </td>\n   <td style=\"text-align:left;\"> The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Pairwise Comparisons </td>\n   <td style=\"text-align:left;\"> Pairwise comparisons are comparisons between every pair of groups to identify which are statistically significantly different from one another following a statistically significant result in an analysis of variance (ANOVA) or other multigroup analysis. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Planned Comparisons </td>\n   <td style=\"text-align:left;\"> Planned comparisons is a statistical strategy for comparing different groups, often used after a statistically significant analysis of variance to test hypotheses about which group means are statistically significantly different from one another. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Platykurtic </td>\n   <td style=\"text-align:left;\"> Platykurtic is a distribution of a numeric variable that has more observations in the tails than a normal distribution would have; platykurtic distributions often look flatter than a normal distribution. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Residually </td>\n   <td style=\"text-align:left;\"> Residuals are the differences between the observed values and the predicted values. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Shapiro-Wilk </td>\n   <td style=\"text-align:left;\"> The Shapiro-Wilk test is a statistical test to determine or confirm whether a variable has a normal distribution; it is sensitive to small deviations from normality and not useful for sample sizes above 5,000 because it will nearly always find non-normality. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standardized Residuals </td>\n   <td style=\"text-align:left;\"> Standardized residuals are the standardized differences between observed and expected values in a chi-squared analysis; a large standardized residual indicates that the observed and expected values were very different. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Student </td>\n   <td style=\"text-align:left;\"> Student t-test is a statistical test used to test whether the difference between the response of two groups is statistically significant or not. (&lt;a href=\"https://en.wikipedia.org/wiki/Student%27s_t-test\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> T-Test </td>\n   <td style=\"text-align:left;\"> A t-test is a type of statistical analysis used to compare the averages of two groups and determine whether the differences between them are more likely to arise from random chance. (&lt;a href=\"https://en.wikipedia.org/wiki/Student%27s_t-test\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Tukey </td>\n   <td style=\"text-align:left;\"> Tukey’s Honestly Significant Difference (HSD) is a post hoc test to determine which means are statistically significantly different from each other following a significant ANOVA result; Tukey’s HSD compares each pair of means and so is considered a pairwise test, but it is less conservative than the [Bonferroni] post hoc test. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Type-1 </td>\n   <td style=\"text-align:left;\"> Rejecting the null hypothesis when it should be retained is called Type I error or alpha and used as the threshold to determine statistical significance. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Type-2 </td>\n   <td style=\"text-align:left;\"> Retaining the null hypothesis when it should be rejected is calles Type II error or beta. (SwR, Glossary) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n## Session Info {.unnumbered}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\nSession Info\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.3.3 (2024-02-29)\n#>  os       macOS Sonoma 14.4.1\n#>  system   x86_64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2024-04-13\n#>  pandoc   3.1.12.3 @ /usr/local/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package      * version    date (UTC) lib source\n#>  abind          1.4-5      2016-07-21 [1] CRAN (R 4.3.0)\n#>  bayestestR     0.13.2     2024-02-12 [1] CRAN (R 4.3.2)\n#>  car            3.1-2      2023-03-30 [1] CRAN (R 4.3.0)\n#>  carData        3.0-5      2022-01-06 [1] CRAN (R 4.3.0)\n#>  cli            3.6.2      2023-12-11 [1] CRAN (R 4.3.0)\n#>  coda           0.19-4.1   2024-01-31 [1] CRAN (R 4.3.2)\n#>  codetools      0.2-20     2024-03-31 [1] CRAN (R 4.3.3)\n#>  colorblindr    0.1.0      2024-02-14 [1] Github (clauswilke/colorblindr@90d64f8)\n#>  colorspace     2.1-1      2024-01-03 [1] R-Forge (R 4.3.2)\n#>  commonmark     1.9.1      2024-01-30 [1] CRAN (R 4.3.2)\n#>  cowplot        1.1.3.9000 2024-02-14 [1] Github (wilkelab/cowplot@e1334a2)\n#>  crayon         1.5.2      2022-09-29 [1] CRAN (R 4.3.0)\n#>  curl           5.2.1      2024-03-01 [1] CRAN (R 4.3.2)\n#>  datawizard     0.10.0     2024-03-26 [1] CRAN (R 4.3.2)\n#>  digest         0.6.35     2024-03-11 [1] CRAN (R 4.3.2)\n#>  dplyr          1.1.4      2023-11-17 [1] CRAN (R 4.3.0)\n#>  dunn.test      1.3.5      2017-10-27 [1] CRAN (R 4.3.0)\n#>  effectsize     0.8.6      2023-09-14 [1] CRAN (R 4.3.0)\n#>  emmeans        1.10.0     2024-01-23 [1] CRAN (R 4.3.2)\n#>  estimability   1.5        2024-02-20 [1] CRAN (R 4.3.2)\n#>  evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.0)\n#>  fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.0)\n#>  farver         2.1.1      2022-07-06 [1] CRAN (R 4.3.0)\n#>  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#>  forcats        1.0.0      2023-01-29 [1] CRAN (R 4.3.0)\n#>  generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#>  ggokabeito     0.1.0      2021-10-18 [1] CRAN (R 4.3.0)\n#>  ggplot2        3.5.0      2024-02-23 [1] CRAN (R 4.3.2)\n#>  glossary     * 1.0.0.9000 2023-08-12 [1] Github (debruine/glossary@819e329)\n#>  glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.0)\n#>  gridExtra      2.3        2017-09-09 [1] CRAN (R 4.3.0)\n#>  gtable         0.3.4      2023-08-21 [1] CRAN (R 4.3.0)\n#>  here           1.0.1      2020-12-13 [1] CRAN (R 4.3.0)\n#>  highr          0.10       2022-12-22 [1] CRAN (R 4.3.0)\n#>  htmltools      0.5.8      2024-03-25 [1] CRAN (R 4.3.2)\n#>  htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.3.0)\n#>  insight        0.19.10    2024-03-22 [1] CRAN (R 4.3.3)\n#>  jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.0)\n#>  kableExtra     1.4.0      2024-01-24 [1] CRAN (R 4.3.2)\n#>  knitr          1.45       2023-10-30 [1] CRAN (R 4.3.0)\n#>  labeling       0.4.3      2023-08-29 [1] CRAN (R 4.3.0)\n#>  lattice        0.22-6     2024-03-20 [2] CRAN (R 4.3.2)\n#>  lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.3.0)\n#>  lsr            0.5.2      2021-12-01 [1] CRAN (R 4.3.0)\n#>  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#>  markdown       1.12       2023-12-06 [1] CRAN (R 4.3.0)\n#>  MASS           7.3-60.0.1 2024-01-13 [2] CRAN (R 4.3.3)\n#>  Matrix         1.6-5      2024-01-11 [1] CRAN (R 4.3.0)\n#>  moments        0.14.1     2022-05-02 [1] CRAN (R 4.3.0)\n#>  multcomp       1.4-25     2023-06-20 [1] CRAN (R 4.3.0)\n#>  munsell        0.5.0      2018-06-12 [1] CRAN (R 4.3.0)\n#>  mvtnorm        1.2-4      2023-11-27 [1] CRAN (R 4.3.2)\n#>  nortest        1.0-4      2015-07-30 [1] CRAN (R 4.3.0)\n#>  onewaytests    3.0        2023-10-01 [1] CRAN (R 4.3.0)\n#>  parameters     0.21.6     2024-03-18 [1] CRAN (R 4.3.2)\n#>  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#>  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#>  purrr          1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n#>  R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#>  RColorBrewer   1.1-3      2022-04-03 [1] CRAN (R 4.3.0)\n#>  rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.0)\n#>  rmarkdown      2.26       2024-03-05 [1] CRAN (R 4.3.2)\n#>  rprojroot      2.0.4      2023-11-05 [1] CRAN (R 4.3.0)\n#>  rstudioapi     0.16.0     2024-03-24 [1] CRAN (R 4.3.2)\n#>  rversions      2.1.2      2022-08-31 [1] CRAN (R 4.3.0)\n#>  sandwich       3.1-0      2023-12-11 [1] CRAN (R 4.3.0)\n#>  scales         1.3.0      2023-11-28 [1] CRAN (R 4.3.2)\n#>  sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n#>  stringi        1.8.3      2023-12-11 [1] CRAN (R 4.3.0)\n#>  stringr        1.5.1      2023-11-14 [1] CRAN (R 4.3.0)\n#>  survival       3.5-8      2024-02-14 [2] CRAN (R 4.3.3)\n#>  svglite        2.1.3      2023-12-08 [1] CRAN (R 4.3.0)\n#>  systemfonts    1.0.6      2024-03-07 [1] CRAN (R 4.3.2)\n#>  TH.data        1.1-2      2023-04-17 [1] CRAN (R 4.3.0)\n#>  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#>  tidyr          1.3.1      2024-01-24 [1] CRAN (R 4.3.2)\n#>  tidyselect     1.2.1      2024-03-11 [1] CRAN (R 4.3.2)\n#>  utf8           1.2.4      2023-10-22 [1] CRAN (R 4.3.0)\n#>  vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n#>  viridisLite    0.4.2      2023-05-02 [1] CRAN (R 4.3.0)\n#>  wesanderson    0.3.7      2023-10-31 [1] CRAN (R 4.3.0)\n#>  withr          3.0.0      2024-01-16 [1] CRAN (R 4.3.0)\n#>  xfun           0.43       2024-03-25 [1] CRAN (R 4.3.2)\n#>  xml2           1.3.6      2023-12-04 [1] CRAN (R 4.3.0)\n#>  xtable         1.8-4      2019-04-21 [1] CRAN (R 4.3.0)\n#>  yaml           2.3.8      2023-12-11 [1] CRAN (R 4.3.0)\n#>  zoo            1.8-12     2023-04-13 [1] CRAN (R 4.3.0)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.3-x86_64/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n\n\n::::\n:::::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}