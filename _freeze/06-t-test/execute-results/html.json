{
  "hash": "04b7e7856899dabbdd5a5a00fe12148d",
  "result": {
    "engine": "knitr",
    "markdown": "# T-test {#sec-chap06}\n\n\n\n\n\n\n\n\n\n## Achievements to unlock\n\n::: my-objectives\n::: my-objectives-header\nObjectives\n:::\n\n::: my-objectives-container\n**SwR Achievements**\n\n- **Achievement 1**: Understanding the relationship between one categorical variable and one continuous variable using histograms, means, and standard deviations (@sec-chap06-achievement1)\n- **Achievement 2**: Comparing a sample mean to a population mean with a <a class='glossary' title='One-sample t-test, also known as the single-parameter t-test or single-sample t-test, is an inferential statistical test comparing the mean of a numeric variable to a population or hypothesized mean. (SwR, Glossary)'>one-sample t-test</a> (@sec-chap06-achievement2)\n- **Achievement 3**: Comparing two unrelated sample means with an <a class='glossary' title='Independent-samples t-test or unpaired sample t-test is an inferential test comparing two independent means. (SwR, Glossary)'>independent-samples t-test</a> (@sec-chap06-achievement3)\n- **Achievement 4**: Comparing two related sample means with a <a class='glossary' title='Dependent-samples test or paired-samples t-test is an inferential test comparing two related means . (SwR, Glossary)'>dependent-samples t-test</a> (@sec-chap06-achievement4)\n- **Achievement 5**: Computing and interpreting an <a class='glossary' title='Effect size is a measure of the strength of a relationship; effect sizes are important in inferential statistics in order to determine and communicate whether a statistically significant result has practical importance. (SwR, Glossary)'>effect size</a> for significant t-tests (@sec-chap06-achievement5)\n- **Achievement 6**: Examining and checking the underlying assumptions for using the t-test (@sec-chap06-achievement6)\n- **Achievement 7**: Identifying and using alternate tests when t-test assumptions are not met (@sec-chap06-achievement7)\n\n:::\n:::\n\n\n## The blood pressure predicament\n\n- **Systolic blood pressure** is measured in millimeters of mercury, or mmHG, and ranges from 74 to 238.\n- **Diastolic blood pressure** is also measured in mmHG and ranges from 0 to 120.\n\n## Resources & Chapter Outline\n\n### Data, codebook, and R packages {#sec-chap04-data-codebook-packages}\n\n::: my-resource\n::: my-resource-header\nData, codebook, and R packages for learning about t-test\n:::\n\n::: my-resource-container\n\n**Data**\n\nTwo options for accessing the data:\n\n- Download the data set `nhanes_2015–2016_ch6.csv` from <https://edge.sagepub.com/harris1e>. \n- Follow the instructions in Box 6.1 to import the data directly with the halp of {**NHANES**} from the Internet into R.\n\n**Codebook**\n\nTwo options for accessing the codebook:\n\n- Download the codebook files `nhanes_demographics_20152016_codebook.html` and `nhanes_examination_20152016_codebook.html` from <https://edge.sagepub.com/harris1e>.  \n- Use the online version of the codebook on the NHANES website (https://www.cdc.gov/nchs/nhanes/index.htm)\n\n**Packages**\n\n1. Packages used with the book (sorted alphabetically)\n\n-   {**BSDA**} @pak-BDSA (Alan T. Arnholt) \n-   {**car**} @pak-car (John Fox)\n-   {**lsr**} @pak-lsr (Danielle Navarro)  \n-   {**rcompanion**} @pak-rcompanion (Salvatore Mangiafico)\n-   {**RNHANES**} @pak-RNHANES (Herb Susmann) \n-   {**tidyverse**}: @pak-tidyverse (Hadley Wickham)\n\n    \n2. My additional packages new introduced (sorted alphabetically)\n\n- {**datawizard**} @pak-datawizard (Etienne Bacher)\n- {**e1071**} @pak-e1071 (David Meyer)\n- {**effectsize**} @pak-effectsize (Mattan S. Ben-Shachar)\n- {**moments**} @pak-moments (Lukasz Komsta)\n- {**misty**} @pak-misty (Takuya Yanagida)\n- {**nhanesA**} @pak-nhanesA (Christopher Endres)\n- {**nortest**} @pak-nortest (Uwe Ligges)\n- {**psych**} @pak-psych (William Revelle)\n- {**rcompanion**} @pak-rcompanion (Salvatore Mangiafico)\n\n:::\n:::\n\n### Get data\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-get-data}\n: Numbered Example Title\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### NHANES data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-get-nhanes-data}\n: Get NHANES data for blood pressure examination with demographics variable for 2015-2016\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## run one once manually #########\n\n## list EXAM tables for 2016 to get file names\nexam_tables_2016 <- nhanesA::nhanesTables('EXAM', 2016)\n\n## list variables in BPX_I (Blood Pressure file)\nbpx_i_variables <- nhanesA::nhanesTableVars('EXAM', 'BPX_I')\n\nbpx_i <- nhanesA::nhanes('BPX_I')\ndemo_i <-  nhanesA::nhanes('DEMO_I')\n\nbpx_2016 <- dplyr::full_join(demo_i, bpx_i, by = \"SEQN\")\n\nsave_data_file(\"chap06\", bpx_2016, \"bpx_2016.rds\")\n```\n:::\n\n\n(*For this R code chunk is no output available. For the raw data see *)\n\n::::\n:::::\n\n\n###### NHANES codebook\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-get-nhanes-codebook}\n: Get NHANES codebook for blood pressure examination with demographics variable for 2015-2016\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncb_systolic <- nhanesA::nhanesCodebook(\"BPX_I\", \"BPXSY1\")\ncb_diastolic <- nhanesA::nhanesCodebook(\"BPX_I\", \"BPXDI1\")\n```\n:::\n\n\n***\n\n(*For this R code chunk is no output available. For the raw data see*)\n\nBesides to call the appropriate website for 2015-2016 [examination codebook](nhanes_examination_20152016_codebook.html) and [demographic codebook](nhanes_demographics_20152016_codebook.html) there is also the option to download information via {**nhanesA**}.\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n***\n\n\n\n\n### Show raw data \n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-show-raw-data}\n: Numbered Example Title\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Blood pressure data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-glance-nhanes-data}\n: Glance at NHANES data for blood pressure examination with demographics variable for 2015-2016\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#lst-glance-nhanes-data}\n\n::: {.cell}\n\n```{.r .cell-code}\nbpx_2016 <- base::readRDS(\"data/chap06/bpx_2016.rds\")\n\nskimr::skim(bpx_2016)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |         |\n|:------------------------|:--------|\n|Name                     |bpx_2016 |\n|Number of rows           |9971     |\n|Number of columns        |67       |\n|_______________________  |         |\n|Column type frequency:   |         |\n|factor                   |45       |\n|numeric                  |22       |\n|________________________ |         |\n|Group variables          |None     |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                                 |\n|:-------------|---------:|-------------:|:-------|--------:|:------------------------------------------|\n|RIDSTATR      |         0|          1.00|FALSE   |        2|Bot: 9544, Int: 427                        |\n|RIAGENDR      |         0|          1.00|FALSE   |        2|Fem: 5079, Mal: 4892                       |\n|RIDRETH1      |         0|          1.00|FALSE   |        5|Non: 3066, Non: 2129, Mex: 1921, Oth: 1547 |\n|RIDRETH3      |         0|          1.00|FALSE   |        6|Non: 3066, Non: 2129, Mex: 1921, Oth: 1308 |\n|RIDEXMON      |       427|          0.96|FALSE   |        2|May: 4950, Nov: 4594                       |\n|DMQMILIZ      |      3822|          0.62|FALSE   |        2|No: 5622, Yes: 527                         |\n|DMQADFC       |      9444|          0.05|FALSE   |        3|No: 267, Yes: 258, Ref: 2                  |\n|DMDBORN4      |         0|          1.00|FALSE   |        3|Bor: 7733, Oth: 2236, Don: 2               |\n|DMDCITZN      |         2|          1.00|FALSE   |        4|Cit: 8785, Not: 1168, Ref: 9, Don: 7       |\n|DMDYRSUS      |      7735|          0.22|FALSE   |       11|20 : 384, 10 : 285, 30 : 281, 1 y: 273     |\n|DMDEDUC3      |      7324|          0.27|FALSE   |       19|1st: 252, Nev: 239, 2nd: 237, 3rd: 220     |\n|DMDEDUC2      |      4252|          0.57|FALSE   |        6|Som: 1692, Col: 1422, Hig: 1236, Les: 688  |\n|DMDMARTL      |      4252|          0.57|FALSE   |        8|Mar: 2886, Nev: 1048, Div: 614, Liv: 555   |\n|RIDEXPRG      |      8683|          0.13|FALSE   |        3|The: 1125, Can: 93, Yes: 70                |\n|SIALANG       |         0|          1.00|FALSE   |        2|Eng: 8584, Spa: 1387                       |\n|SIAPROXY      |         1|          1.00|FALSE   |        2|No: 6281, Yes: 3689                        |\n|SIAINTRP      |         0|          1.00|FALSE   |        2|No: 9514, Yes: 457                         |\n|FIALANG       |       329|          0.97|FALSE   |        2|Eng: 8430, Spa: 1212                       |\n|FIAPROXY      |       329|          0.97|FALSE   |        2|No: 9633, Yes: 9                           |\n|FIAINTRP      |       329|          0.97|FALSE   |        2|No: 9237, Yes: 405                         |\n|MIALANG       |      2994|          0.70|FALSE   |        2|Eng: 6382, Spa: 595                        |\n|MIAPROXY      |      2993|          0.70|FALSE   |        2|No: 6921, Yes: 57                          |\n|MIAINTRP      |      2993|          0.70|FALSE   |        2|No: 6632, Yes: 346                         |\n|AIALANGA      |      4009|          0.60|FALSE   |        3|Eng: 5218, Spa: 638, Asi: 106              |\n|DMDHHSIZ      |         0|          1.00|FALSE   |        7|4: 2061, 2: 1723, 3: 1719, 5: 1672         |\n|DMDFMSIZ      |         0|          1.00|FALSE   |        7|4: 2011, 5: 1635, 3: 1634, 2: 1510         |\n|DMDHHSZA      |         0|          1.00|FALSE   |        4|0: 6298, 1: 2147, 2: 1199, 3 o: 327        |\n|DMDHHSZB      |         0|          1.00|FALSE   |        5|0: 4715, 1: 1990, 2: 1833, 3: 822          |\n|DMDHHSZE      |         0|          1.00|FALSE   |        4|0: 7151, 1: 1663, 2: 1099, 3 o: 58         |\n|DMDHRGND      |         0|          1.00|FALSE   |        2|Mal: 5053, Fem: 4918                       |\n|DMDHRBR4      |       396|          0.96|FALSE   |        4|Bor: 6359, Oth: 3207, Ref: 5, Don: 4       |\n|DMDHREDU      |       396|          0.96|FALSE   |        6|Som: 2908, Col: 2331, Hig: 2015, 9-1: 1200 |\n|DMDHRMAR      |        62|          0.99|FALSE   |        8|Mar: 5681, Nev: 1305, Liv: 1017, Div: 977  |\n|DMDHSEDU      |      4745|          0.52|FALSE   |        7|Col: 1629, Som: 1462, Hig: 980, Les: 619   |\n|INDHHIN2      |       345|          0.97|FALSE   |       16|$10: 1634, $25: 1017, $35: 960, $75: 920   |\n|INDFMIN2      |       329|          0.97|FALSE   |       16|$10: 1548, $25: 1038, $35: 934, $75: 885   |\n|PEASCCT1      |      9735|          0.02|FALSE   |        3|Tim: 178, Oth: 46, SP : 12                 |\n|BPAARM        |      2573|          0.74|FALSE   |        3|Rig: 7361, Lef: 36, Cou: 1                 |\n|BPACSZ        |      2585|          0.74|FALSE   |        5|Lar: 3368, Adu: 2461, Thi: 1068, Chi: 488  |\n|BPXPULS       |       657|          0.93|FALSE   |        2|Reg: 9113, Irr: 201                        |\n|BPXPTY        |      2595|          0.74|FALSE   |        2|Rad: 7348, Bra: 28                         |\n|BPAEN1        |      2826|          0.72|FALSE   |        2|No: 7142, Yes: 3                           |\n|BPAEN2        |      2658|          0.73|FALSE   |        2|No: 7130, Yes: 183                         |\n|BPAEN3        |      2695|          0.73|FALSE   |        2|No: 7070, Yes: 206                         |\n|BPAEN4        |      9647|          0.03|FALSE   |        2|No: 248, Yes: 76                           |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|     mean|       sd|       p0|      p25|      p50|      p75|     p100|hist  |\n|:-------------|---------:|-------------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|:-----|\n|SEQN          |         0|          1.00| 88717.00|  2878.52| 83732.00| 86224.50| 88717.00| 91209.50|  93702.0|▇▇▇▇▇ |\n|SDDSRVYR      |         0|          1.00|     9.00|     0.00|     9.00|     9.00|     9.00|     9.00|      9.0|▁▁▇▁▁ |\n|RIDAGEYR      |         0|          1.00|    31.90|    24.77|     0.00|     9.00|    27.00|    53.00|     80.0|▇▃▃▃▃ |\n|RIDAGEMN      |      9276|          0.07|    10.79|     7.02|     0.00|     5.00|    10.00|    17.00|     24.0|▇▇▆▆▅ |\n|RIDEXAGM      |      5911|          0.41|   104.53|    68.97|     0.00|    41.00|   100.00|   162.00|    239.0|▇▆▆▅▅ |\n|DMDHRAGE      |         0|          1.00|    46.18|    15.83|    18.00|    34.00|    44.00|    57.00|     80.0|▅▇▆▃▃ |\n|WTINT2YR      |         0|          1.00| 31740.15| 32929.54|  3293.93| 12878.50| 20160.47| 33257.36| 233755.8|▇▁▁▁▁ |\n|WTMEC2YR      |         0|          1.00| 31740.15| 34105.57|     0.00| 12550.53| 20281.32| 33708.15| 242386.7|▇▁▁▁▁ |\n|SDMVPSU       |         0|          1.00|     1.49|     0.50|     1.00|     1.00|     1.00|     2.00|      2.0|▇▁▁▁▇ |\n|SDMVSTRA      |         0|          1.00|   126.27|     4.24|   119.00|   123.00|   126.00|   130.00|    133.0|▇▇▇▇▇ |\n|INDFMPIR      |      1052|          0.89|     2.27|     1.58|     0.00|     0.97|     1.82|     3.48|      5.0|▇▇▅▃▆ |\n|BPXCHR        |      8033|          0.19|   106.56|    21.76|    58.00|    90.00|   104.00|   120.00|    190.0|▃▇▅▂▁ |\n|BPXPLS        |      2595|          0.74|    74.61|    12.23|    36.00|    66.00|    74.00|    82.00|    142.0|▁▇▃▁▁ |\n|BPXML1        |      2600|          0.74|   146.60|    18.56|   110.00|   130.00|   140.00|   160.00|    260.0|▇▅▁▁▁ |\n|BPXSY1        |      2826|          0.72|   120.54|    18.62|    72.00|   108.00|   118.00|   130.00|    236.0|▂▇▂▁▁ |\n|BPXDI1        |      2826|          0.72|    66.18|    14.29|     0.00|    58.00|    66.00|    76.00|    120.0|▁▁▇▅▁ |\n|BPXSY2        |      2658|          0.73|   120.32|    18.62|    76.00|   108.00|   118.00|   130.00|    238.0|▃▇▂▁▁ |\n|BPXDI2        |      2658|          0.73|    66.06|    14.37|     0.00|    58.00|    66.00|    76.00|    144.0|▁▂▇▁▁ |\n|BPXSY3        |      2695|          0.73|   119.95|    18.29|    76.00|   108.00|   116.00|   130.00|    226.0|▃▇▂▁▁ |\n|BPXDI3        |      2695|          0.73|    65.99|    14.56|     0.00|    58.00|    66.00|    76.00|    140.0|▁▂▇▁▁ |\n|BPXSY4        |      9647|          0.03|   129.10|    22.88|    82.00|   112.00|   128.00|   142.50|    212.0|▃▇▅▁▁ |\n|BPXDI4        |      9647|          0.03|    70.38|    18.62|     0.00|    62.00|    72.00|    80.00|    132.0|▁▁▇▃▁ |\n\n\n:::\n\n```{.r .cell-code}\nglance_data(bpx_2016)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>     obs  SEQN SDDSRVYR                          RIDSTATR RIAGENDR RIDAGEYR\n#> 1     1 83732        9 Both interviewed and MEC examined     Male       62\n#> 2   356 84087        9 Both interviewed and MEC examined     Male       36\n#> 3  1252 84983        9                  Interviewed only   Female       10\n#> 4  2369 86100        9 Both interviewed and MEC examined     Male       33\n#> 5  3954 87685        9 Both interviewed and MEC examined   Female        1\n#> 6  5273 89004        9 Both interviewed and MEC examined   Female        0\n#> 7  7700 91431        9                  Interviewed only   Female       47\n#> 8  8826 92557        9 Both interviewed and MEC examined     Male       71\n#> 9  9290 93021        9 Both interviewed and MEC examined   Female       18\n#> 10 9971 93702        9 Both interviewed and MEC examined   Female       24\n#>    RIDAGEMN                            RIDRETH1           RIDRETH3\n#> 1        NA                  Non-Hispanic White Non-Hispanic White\n#> 2        NA Other Race - Including Multi-Racial Non-Hispanic Asian\n#> 3        NA                  Non-Hispanic Black Non-Hispanic Black\n#> 4        NA                    Mexican American   Mexican American\n#> 5        22 Other Race - Including Multi-Racial Non-Hispanic Asian\n#> 6         3                    Mexican American   Mexican American\n#> 7        NA                      Other Hispanic     Other Hispanic\n#> 8        NA Other Race - Including Multi-Racial Non-Hispanic Asian\n#> 9        NA                    Mexican American   Mexican American\n#> 10       NA                  Non-Hispanic White Non-Hispanic White\n#>                       RIDEXMON RIDEXAGM DMQMILIZ DMQADFC\n#> 1  November 1 through April 30       NA       No    <NA>\n#> 2  November 1 through April 30       NA       No    <NA>\n#> 3                         <NA>       NA     <NA>    <NA>\n#> 4  November 1 through April 30       NA       No    <NA>\n#> 5     May 1 through October 31       23     <NA>    <NA>\n#> 6  November 1 through April 30        3     <NA>    <NA>\n#> 7                         <NA>       NA       No    <NA>\n#> 8     May 1 through October 31       NA       No    <NA>\n#> 9  November 1 through April 30      226       No    <NA>\n#> 10    May 1 through October 31       NA       No    <NA>\n#>                                  DMDBORN4                           DMDCITZN\n#> 1  Born in 50 US states or Washington, DC Citizen by birth or naturalization\n#> 2                                  Others            Not a citizen of the US\n#> 3  Born in 50 US states or Washington, DC Citizen by birth or naturalization\n#> 4                                  Others            Not a citizen of the US\n#> 5  Born in 50 US states or Washington, DC Citizen by birth or naturalization\n#> 6  Born in 50 US states or Washington, DC Citizen by birth or naturalization\n#> 7  Born in 50 US states or Washington, DC Citizen by birth or naturalization\n#> 8                                  Others Citizen by birth or naturalization\n#> 9  Born in 50 US states or Washington, DC Citizen by birth or naturalization\n#> 10 Born in 50 US states or Washington, DC Citizen by birth or naturalization\n#>                                   DMDYRSUS             DMDEDUC3\n#> 1                                     <NA>                 <NA>\n#> 2  15 year or more, but less than 20 years                 <NA>\n#> 3                                     <NA>            3rd grade\n#> 4  10 year or more, but less than 15 years                 <NA>\n#> 5                                     <NA>                 <NA>\n#> 6                                     <NA>                 <NA>\n#> 7                                     <NA>                 <NA>\n#> 8    1 year or more, but less than 5 years                 <NA>\n#> 9                                     <NA> High school graduate\n#> 10                                    <NA>                 <NA>\n#>                                              DMDEDUC2            DMDMARTL\n#> 1                           College graduate or above             Married\n#> 2                           College graduate or above Living with partner\n#> 3                                                <NA>                <NA>\n#> 4  9-11th grade (Includes 12th grade with no diploma)             Married\n#> 5                                                <NA>                <NA>\n#> 6                                                <NA>                <NA>\n#> 7                           College graduate or above           Separated\n#> 8  9-11th grade (Includes 12th grade with no diploma)             Married\n#> 9                                                <NA>                <NA>\n#> 10                          College graduate or above       Never married\n#>                                    RIDEXPRG SIALANG SIAPROXY SIAINTRP FIALANG\n#> 1                                      <NA> English       No       No English\n#> 2                                      <NA> English       No       No English\n#> 3                                      <NA> English      Yes       No English\n#> 4                                      <NA> Spanish       No       No Spanish\n#> 5                                      <NA> English      Yes      Yes English\n#> 6                                      <NA> English      Yes       No English\n#> 7                                      <NA> English       No       No English\n#> 8                                      <NA> English       No      Yes    <NA>\n#> 9                                      <NA> English       No       No English\n#> 10 The participant was not pregnant at exam English       No       No English\n#>    FIAPROXY FIAINTRP MIALANG MIAPROXY MIAINTRP AIALANGA\n#> 1        No       No English       No       No  English\n#> 2        No       No English       No       No  English\n#> 3        No       No    <NA>     <NA>     <NA>     <NA>\n#> 4        No       No English       No       No  Spanish\n#> 5        No      Yes    <NA>     <NA>     <NA>     <NA>\n#> 6        No       No    <NA>     <NA>     <NA>     <NA>\n#> 7        No       No    <NA>     <NA>     <NA>     <NA>\n#> 8      <NA>     <NA> English       No      Yes     <NA>\n#> 9        No       No English       No       No  English\n#> 10       No       No English       No       No  English\n#>                             DMDHHSIZ                       DMDFMSIZ  DMDHHSZA\n#> 1                                  2                              2         0\n#> 2                                  4                              4         0\n#> 3                                  2                              2         0\n#> 4                                  6                              6         1\n#> 5                                  5                              5 3 or more\n#> 6  7 or more people in the Household 7 or more people in the Family         2\n#> 7                                  4                              4         1\n#> 8                                  3                              3         0\n#> 9                                  4                              1         0\n#> 10                                 3                              1         0\n#>    DMDHHSZB DMDHHSZE DMDHRGND DMDHRAGE                               DMDHRBR4\n#> 1         0        1     Male       62 Born in 50 US states or Washington, DC\n#> 2         2        0     Male       36                                 Others\n#> 3         1        0   Female       35 Born in 50 US states or Washington, DC\n#> 4         3        0   Female       36                                 Others\n#> 5         0        0     Male       37                                 Others\n#> 6         1        0     Male       37                                 Others\n#> 7         0        0   Female       23 Born in 50 US states or Washington, DC\n#> 8         0        1     Male       71                                 Others\n#> 9         0        0   Female       26                                   <NA>\n#> 10        0        0   Female       22 Born in 50 US states or Washington, DC\n#>                                              DMDHREDU            DMDHRMAR\n#> 1                           College Graduate or above             Married\n#> 2                           College Graduate or above Living with partner\n#> 3                           Some College or AA degree       Never married\n#> 4  9-11th Grade (Includes 12th grade with no diploma)             Married\n#> 5                  High School Grad/GED or Equivalent             Married\n#> 6                                 Less Than 9th Grade             Married\n#> 7                  High School Grad/GED or Equivalent Living with partner\n#> 8  9-11th Grade (Includes 12th grade with no diploma)             Married\n#> 9                                                <NA>       Never married\n#> 10                          College Graduate or above       Never married\n#>                                              DMDHSEDU   WTINT2YR   WTMEC2YR\n#> 1                  High School Grad/GED or Equivalent 134671.370 135629.507\n#> 2                                                <NA>  18448.501  18550.188\n#> 3                                                <NA>  14840.527      0.000\n#> 4  9-11th Grade (Includes 12th grade with no diploma)  26689.378  27745.104\n#> 5                  High School Grad/GED or Equivalent   8860.151   8793.912\n#> 6                                 Less Than 9th Grade   5454.854   5610.739\n#> 7                                                <NA>  19288.330      0.000\n#> 8                                                <NA>  19926.979  25066.422\n#> 9                                                <NA>  16127.645  16401.812\n#> 10                                               <NA> 107361.907 105080.445\n#>    SDMVPSU SDMVSTRA           INDHHIN2           INDFMIN2 INDFMPIR PEASCCT1\n#> 1        1      125 $65,000 to $74,999 $65,000 to $74,999     4.39     <NA>\n#> 2        1      133  $100,000 and Over  $100,000 and Over     5.00     <NA>\n#> 3        2      132 $15,000 to $19,999 $15,000 to $19,999     0.94     <NA>\n#> 4        1      128 $15,000 to $19,999 $15,000 to $19,999     0.48     <NA>\n#> 5        1      122 $15,000 to $19,999 $15,000 to $19,999     0.63     <NA>\n#> 6        2      121 $55,000 to $64,999 $55,000 to $64,999     1.50     <NA>\n#> 7        1      121 $35,000 to $44,999 $35,000 to $44,999     1.65     <NA>\n#> 8        1      129               <NA>               <NA>       NA     <NA>\n#> 9        2      128 $45,000 to $54,999 $ 5,000 to $ 9,999     0.59     <NA>\n#> 10       2      119 $65,000 to $74,999 $35,000 to $44,999     3.54     <NA>\n#>    BPXCHR BPAARM        BPACSZ BPXPLS BPXPULS BPXPTY BPXML1 BPXSY1 BPXDI1\n#> 1      NA  Right Large (15X32)     76 Regular Radial    150    128     70\n#> 2      NA  Right Adult (12X22)     46 Regular Radial    160    122     80\n#> 3      NA   <NA>          <NA>     NA    <NA>   <NA>     NA     NA     NA\n#> 4      NA  Right Large (15X32)     68 Regular Radial    150    116     70\n#> 5     110   <NA>          <NA>     NA Regular   <NA>     NA     NA     NA\n#> 6     122   <NA>          <NA>     NA Regular   <NA>     NA     NA     NA\n#> 7      NA   <NA>          <NA>     NA    <NA>   <NA>     NA     NA     NA\n#> 8      NA  Right Adult (12X22)     72 Regular Radial    140    112     80\n#> 9      NA  Right  Child (9X17)     74 Regular Radial    120    100     74\n#> 10     NA  Right Adult (12X22)     80 Regular Radial    150    118     66\n#>    BPAEN1 BPXSY2 BPXDI2 BPAEN2 BPXSY3 BPXDI3 BPAEN3 BPXSY4 BPXDI4 BPAEN4\n#> 1      No    124     64     No    116     62     No     NA     NA   <NA>\n#> 2      No    128     82     No    122     86     No     NA     NA   <NA>\n#> 3    <NA>     NA     NA   <NA>     NA     NA   <NA>     NA     NA   <NA>\n#> 4      No    124     66     No    116     66     No     NA     NA   <NA>\n#> 5    <NA>     NA     NA   <NA>     NA     NA   <NA>     NA     NA   <NA>\n#> 6    <NA>     NA     NA   <NA>     NA     NA   <NA>     NA     NA   <NA>\n#> 7    <NA>     NA     NA   <NA>     NA     NA   <NA>     NA     NA   <NA>\n#> 8      No    118     78     No    122     80     No     NA     NA   <NA>\n#> 9      No    100     66     No     98     68     No     NA     NA   <NA>\n#> 10     No    114     68     No    124     64     No     NA     NA   <NA>\n```\n\n\n:::\n:::\n\nGlance at NHANES data for blood pressure examination with demographics variable for 2015-2016\n\n:::\n\n::::\n:::::\n\n\n\n###### Blood pressure codebook\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-glance-nhanes-codebook}\n: Glance at NHANES codebook for systolic & diastolic blood pressure (2015-2016)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglue::glue(\"*********************** Systolic blood pressure ******************\")\ncb_systolic\nglue::glue(\" \")\nglue::glue(\"*********************** Diastolic blood pressure ******************\")\ncb_diastolic\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> *********************** Systolic blood pressure ******************\n#> $`Variable Name:`\n#> [1] \"BPXSY1\"\n#> \n#> $`SAS Label:`\n#> [1] \"Systolic: Blood pres (1st rdg) mm Hg\"\n#> \n#> $`English Text:`\n#> [1] \"Systolic: Blood pressure (first reading) mm Hg\"\n#> \n#> $`Target:`\n#> [1] \"Both males and females 8 YEARS -\\r 150 YEARS\"\n#> \n#> $BPXSY1\n#> # A tibble: 2 × 5\n#>   `Code or Value` `Value Description` Count Cumulative `Skip to Item`\n#>   <chr>           <chr>               <int>      <int> <lgl>         \n#> 1 72 to 236       Range of Values      7145       7145 NA            \n#> 2 .               Missing              2399       9544 NA            \n#> \n#>  \n#> *********************** Diastolic blood pressure ******************\n#> $`Variable Name:`\n#> [1] \"BPXDI1\"\n#> \n#> $`SAS Label:`\n#> [1] \"Diastolic: Blood pres (1st rdg) mm Hg\"\n#> \n#> $`English Text:`\n#> [1] \"Diastolic: Blood pressure (first reading) mm Hg\"\n#> \n#> $`Target:`\n#> [1] \"Both males and females 8 YEARS -\\r 150 YEARS\"\n#> \n#> $BPXDI1\n#> # A tibble: 2 × 5\n#>   `Code or Value` `Value Description` Count Cumulative `Skip to Item`\n#>   <chr>           <chr>               <int>      <int> <lgl>         \n#> 1 0 to 120        Range of Values      7145       7145 NA            \n#> 2 .               Missing              2399       9544 NA\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n***\n\n\n\n### Recode data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-clean-data}\n: Clean NHANES blood pressure data (2015-2016)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## load bpx_2016 #######\nbpx_2016 <- base::readRDS(\"data/chap06/bpx_2016.rds\")\n\nbp_clean <-  bpx_2016 |> \n    dplyr::rename(\n        systolic = BPXSY1,\n        systolic2 = BPXSY2,\n        sex = RIAGENDR\n        ) |> \n    dplyr::mutate(diff_syst = systolic - systolic2) |> \n    dplyr::relocate(c(systolic, systolic2, diff_syst), .before = sex)\n\nsave_data_file(\"chap06\", bp_clean, \"bp_clean.rds\")\n```\n:::\n\n\n***\n(*For this R code chunk is no output available*)\n::::\n:::::\n\n\n## Achievement 1: Relationship between one categorical and one continuous variable {#sec-chap06-achievement1}\n\nFor this first achievement we are going to look into the relationship between one categorical variable and one continuous variable using histograms, means, and standard deviations.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-descriptive}\n: Description of blood pressure data from NHANES 2015-2016\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Histogram 1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-systolic-histo1}\n: Histogram of systolic blood pressure (NHANES 2015-2016)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## load bpx_2016 #######\nbpx_2016 <- base::readRDS(\"data/chap06/bpx_2016.rds\")\n\n## graph systolic blood pressure variable BPXSY1 (Figure 6.1)\nsys_histo <- bpx_2016  |>  \n    ggplot2::ggplot(\n        ggplot2::aes(x = BPXSY1)\n        ) + \n    ggplot2::geom_histogram(\n        fill =  \"mediumpurple\",\n        color = \"white\",\n        bins = 30,\n        na.rm = TRUE\n        ) + \n    ggplot2::theme_bw() + \n    ggplot2::labs(\n        x = \"Systolic blood pressure (mmHg)\", \n        y = \"NHANES participants\"\n        ) \n\nsys_histo\n```\n\n::: {.cell-output-display}\n![Histogram of systolic blood pressure (NHANES 2015-2016)](06-t-test_files/figure-html/systolic-histo1-1.png){width=672}\n:::\n:::\n\n***\n\nThis is the replication of the book’s Figure 6.1.\n\nThe graph is not exactly normally distributed; it has a little right skew.  The <a class='glossary' title='Quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities (Wikipedia)'>quantile</a> values (0%  25%  50%  75%  100%) are 72, 108, 118, 130, 236. The middle 50% lies in the range between 108 and 130 mmHG. You can't see the highest values because their frequencies are too small.  \n::::\n:::::\n\n\n###### Histogram 2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-systolic-histo2}\n: Histogram of systolic blood pressure with risk factors (NHANES 2015-2016)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## graph systolic blood pressure with risk factors (Figure 6.2)\nsys_histo2 <- bpx_2016 |>  \n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = BPXSY1, \n            fill = BPXSY1 > 120)\n        ) + \n    ggplot2::geom_histogram(\n        color = \"white\",\n        bins = 30,\n        na.rm = TRUE) + \n    ggplot2::theme_bw() + \n    ggplot2::labs(\n        x = \"Systolic blood pressure (mmHg)\", \n        y = \"NHANES participants\"\n        ) +\n    ggplot2::scale_fill_manual(\n        values = c(\"mediumpurple\", \"grey\"),\n        labels = c(\"Normal range\",\n                   \"at-risk or high\"),\n        name = \"Systolic\\nBlood Pressure\"\n    )\n\nsys_histo2\n```\n\n::: {.cell-output-display}\n![Histogram of systolic blood pressure with risk factors (NHANES 2015-2016)](06-t-test_files/figure-html/systolic-histo2-1.png){width=672}\n:::\n:::\n\n***\n\nThis is the replication of the book’s Figure 6.2.\n::::\n:::::\n\n###### Experiment\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-systolic-histo3}\n: Blood pressure histogram with several colors according to their medical conditions\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## graph systolic blood pressure differentiated\nsys_histo3 <- bpx_2016 |>  \n    dplyr::select(BPXSY1) |> \n    dplyr::mutate(sys = dplyr::case_when(\n        BPXSY1 < 105 ~ \"0\",\n        BPXSY1 >= 105 & BPXSY1 < 120 ~ \"1\",\n        BPXSY1 >= 120 & BPXSY1 < 130 ~ \"2\",\n        BPXSY1 >= 130 & BPXSY1 < 140 ~ \"3\",\n        BPXSY1 >= 140 ~ \"4\"\n        )\n    ) |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = BPXSY1, fill = sys)\n        ) + \n    ggplot2::geom_histogram(\n        color = \"white\",\n        binwidth = 2,\n        na.rm = TRUE) + \n    ggplot2::theme_bw() + \n    ggplot2::theme(legend.position = \"bottom\") +\n    ggplot2::labs(\n        x = \"Systolic blood pressure (mmHg)\", \n        y = \"NHANES participants\"\n        ) +\n    ggplot2::scale_fill_manual(\n        values = c(\n            \"0\" = \"grey\", \n            \"1\" = \"mediumpurple\", \n            \"2\" = \"yellow\", \n            \"3\" = \"darkorange\", \n            \"4\" = \"red\"\n            ),\n        labels = c(\"Low\",\n                   \"Optimal\",\n                   \"Normal\",\n                   \"At-risk\",\n                   \"High\"\n                  ),\n        name = \"Systolic\\nBlood Pressure\"\n    ) +\n    ggplot2::xlim(70, 240)\n\nsys_histo3\n```\n\n::: {.cell-output-display}\n![](06-t-test_files/figure-html/systolic-histo3-1.png){width=672}\n:::\n:::\n\n***\nHere I have experimented to colorize the histogram with different colors. I took as borders the medical condition for isolated blood pressure measures:\n\n- Low: < 105\n- Optimal: >= 105 & < 120\n- Normal: >= 120 & < 130\n- At Risk: >= 130 & < 140\n- High: >= 140\n\nIn this case I can’t use the color directly as `fill` variable into the `ggplots::aes()` function. Besides I learned two other solve two other issues:\n\n- The sequence of colors are aligned to the values alphabetically. Therefore I had to take characters that are sorted in the correct order. I took c(\"0\", \"1\", \"2\", \"3\", \"4\") but c(\"a\", \"b\", \"c\", \"d\" ,\"e\") would have worked too.\n- Wider bins brought the problem that the color has changed in the middle of the bar length. I did not know how to solve this issue generally, for instance with providing `breaks` or to provide borders conforming to the medical status. Only `binwidth` of 1 and 2 worked, 3 already showed the problem. Other people had the same problem, see for instance the section \"Example 2: Draw Histogram with Different Colors Using ggplot2 Package\" in [Draw Histogram with Different Colors in R (2 Examples)](https://statisticsglobe.com/draw-histogram-with-different-colors-in-r) [@schorkn.d].\n\n::::\n:::::\n\n###### Histogram 3\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-diastolic-histo}\n: Histogram of diastolic blood pressure with risk factors (NHANES 2015-2016)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## graph systolic blood pressure with risk factors (Figure 6.3)\ndia_histo <- bpx_2016 |>  \n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = BPXDI1, \n            fill = BPXDI1 > 80)\n        ) + \n    ggplot2::geom_histogram(\n        color = \"white\",\n        bins = 30,\n        na.rm = TRUE) + \n    ggplot2::theme_bw() + \n    ggplot2::labs(\n        x = \"Diastolic blood pressure (mmHg)\", \n        y = \"NHANES participants\"\n        ) +\n    ggplot2::scale_fill_manual(\n        values = c(\"mediumpurple\", \"grey\"),\n        labels = c(\"Normal range\",\n                   \"at-risk or high\"),\n        name = \"Diastolic\\nBlood Pressure\"\n    )\n\ndia_histo\n```\n\n::: {.cell-output-display}\n![Histogram of diastolic blood pressure with risk factors (NHANES 2015-2016)](06-t-test_files/figure-html/diastolic-histo-1.png){width=672}\n:::\n:::\n\n***\n\nThis is the replication of the book’s Figure 6.3.\n::::\n:::::\n\n###### `mean()` & `sd()`\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-systolic-mean-sd}\n: Mean and standard deviation of systolic blood pressure in the NHANES data sample (2015-2016)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbpx_stats <- \n    bpx_2016 |> \n        tidyr::drop_na(BPXSY1) |> \n        dplyr::summarize(\n            mean = base::mean(BPXSY1),\n            sd = stats::sd(BPXSY1),\n            n = dplyr::n()\n            )\nbpx_stats\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       mean       sd    n\n#> 1 120.5394 18.61692 7145\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n## Achievement 2: One-Sample t-test {#sec-chap06-achievement2}\n\n### Introduction\n\nWe have a mean of the systolic blood pressure a little bit above 120.54 mmHG. This is almost exactly the upper cutoff value of 120 mmHG for the \"normal range\" of blood pressure. Is this only valid for the sample of also for the whole population? Have about half of the US people high systolic blood pressure, e.g. more than 120mmHG? The question can be answered with a <a class='glossary' title='One-sample t-test, also known as the single-parameter t-test or single-sample t-test, is an inferential statistical test comparing the mean of a numeric variable to a population or hypothesized mean. (SwR, Glossary)'>one-sample t-test</a>. The one sample t-test compares a sample mean to a *hypothesized or population* mean.\n\n:::::{.my-important}\n:::{.my-important-header}\nThere are three different t-tests\n:::\n::::{.my-important-container}\n\n- **One-sample t-test**: compares a mean to a population or hypothesized value \n- **Independent-samples t-test**: compares the means of two unrelated groups \n- **Dependent-samples t-test**: compares the means of two related groups\n\n::::\n:::::\n\nThe t-distribution has a bell shape like the normal distribution. But unlike the normal distribution its variance is not known but approximated with its only parameter <a class='glossary' title='Degree of Freedom (df) is the number of pieces of information that are allowed to vary in computing a statistic before the remaining pieces of information are known; degrees of freedom are often used as parameters for distributions (e.g., chi-squared, F). (SwR, Glossary)'>degrees of freedom</a> (df). `df` is calculated by the number of observations minus one ($n-1$). With higher degrees of freedom the t-distribution will get closer to the normal distribution. Often the number 30 is recommended as the cutting point where t-distribution and normal distribution are equivalent.\n\n\nI am following @prp-chap05-nhst from @sec-chap05-achievement5.\n\n\n### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\nTwo considerations:\n\n1. The Null relates most of the times to a situation where no change occurs. In this case that there is no difference in the means of the systolic blood pressure. This is different to the assumption that the mean difference is not higher than 120 mmHG!\n2. In the one-sample t-test we are comparing sample mean with population mean. In this case the NHANES sample from the 2015-2016 data with the population mean of the US population.\n\n::: {.callout-note}\n- **H0**: There is no difference in the mean systolic blood pressure in the US population and the cutoff for normal blood pressure of 120 mmHG in the NHANES 2015-2016 data set.\n- **HA**: There is a difference in the mean systolic blood pressure in the US population and the cutoff for normal blood pressure of 120 mmHG in the NHANES 2015-2016 data set.\n:::\n\n### NHST Step 2\n\nCompute the test statistic. The one-sample t-test uses the <a class='glossary' title='The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (Statistics How-To)'>t-statistic</a> (sort of like a z-statistic) \n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap06-t-statistic}\n: t-test formula\n::::::\n:::\n::::{.my-theorem-container}\n\n$$\nt = \\frac{m_{x} - \\mu_{x}}{\\frac{s_x}{\\sqrt{n_{x}}}}\n$$ {#eq-chap06-t-statistic}\n\n- $m_{x}$ represents the mean of the variable x, the variable to be tested, \n- $\\mu_{x}$ is the *population mean or hypothesized value* of the variable, \n- $s_{x}$ is the sample standard deviation of x, and \n- $n_{x}$ is the sample size\n::::\n:::::\n\nThe formula is very similar as the <a class='glossary' title='A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (StatisticsHowTo)'>Z-score</a> statistic in @eq-chap04-z-score. The only difference is that in the above <a class='glossary' title='The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (Statistics How-To)'>t-statistic</a> the denominator is the <a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a> rather than the <a class='glossary' title='The standard error (SE) of a statistic is the standard deviation of its [sampling distribution]. If the statistic is the sample mean, it is called the standard error of the mean (SEM). (Wikipedia) The standard error is a measure of variability that estimates how much variability there is in a population based on the variability in the sample and the size of the sample. (SwR, Glossary)'>standard error</a>.\n\n- `z` shows how many sample standard deviations some value is away from the mean.\n- `t` shows how many standard errors (i.e., population standard deviations) some value is away from the mean.\n\n$$\nt = \\frac{120.5394 - 120}{\\frac{18.61692}{\\sqrt{7145}}} = 2.45\n$$\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-t-statistic-systolic}\n: t-statistic of systolic blood pressure aof NHANES sample with hypothesized population mean\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n(\n    t_test_systolic <- stats::t.test(\n        bpx_2016$BPXSY1,\n        alternative = \"two.sided\",\n        mu = 120)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tOne Sample t-test\n#> \n#> data:  bpx_2016$BPXSY1\n#> t = 2.4491, df = 7144, p-value = 0.01435\n#> alternative hypothesis: true mean is not equal to 120\n#> 95 percent confidence interval:\n#>  120.1077 120.9711\n#> sample estimates:\n#> mean of x \n#>  120.5394\n```\n\n\n:::\n\n```{.r .cell-code}\n## for later use #########\nt_sys = t_test_systolic[[\"statistic\"]][[\"t\"]]\ndf_sys = t_test_systolic[[\"parameter\"]][[\"df\"]]\nnull_sys = t_test_systolic[[\"null.value\"]][[\"mean\"]]\nestimate_sys = t_test_systolic[[\"estimate\"]][[\"mean of x\"]]\np_value_sys = t_test_systolic[[\"p.value\"]]\nse_sys = t_test_systolic[[\"stderr\"]]\n```\n:::\n\n\n***\n\nI have stored the result of the t-test into variables for later use.\n\n\n\n::::\n:::::\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-chap06-t-test-output}\n: Explications of the t-test output\n::::::\n:::\n::::{.my-assessment-container}\n\n**1. Line**: Data (variable) used.\n**2. Line**: \n    - `t`: value of the t-test statistic.\n    - `df`: degrees of freedom, with t-statistic = subtracting 1 from sample size = $n - 1$.\n    - `p-value`: The probability of the sample coming from a population where the null hypothesis is true. \n**3. Line**: wording of the alternative hypothesis.\n**4. Line**: Chosen confidence interval.\n**5. Line**: The lower and upper boundary of the confidence interval.\n**6. Line**: Sample estimates that has to be compared to the value of the null hypothesis.\n\n::::\n:::::\n\n### NHST Step 3\n\nReview and interpret the test statistics: Calculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\nThe following examples replicates Figure 6.4, 6.5 and 6.6. I will break down the final code into several steps following the nice article [Visualizing Sampling Distributions: Learn how to add areas under the curve in sampling distributions]https://ggplot2tutor.com/tutorials/sampling_distributions [@burkhart2021].\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-prob-dist-t-test}\n: Probability distribution of t-test statistic\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### t (df=1)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-two-t-prob-dist}\n: Student t distributions with 1 degree of freedom (df)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot() +\n    ggplot2::xlim(-7, 7) +\n\n## or as an alternative\n# ggplot2::ggplot(tibble::tibble(x = c(-7, 7)), \n#          ggplot2::aes(x)) +\n    \n    ggplot2::stat_function(\n             fun = stats::dt,\n             args = list(df = 1),\n             geom = \"line\",\n             linewidth = 0.7\n         ) +\n    ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Student t distributions with 1 degree of freedom (df)](06-t-test_files/figure-html/fig-t-prob-dist-1.png){#fig-t-prob-dist width=672}\n:::\n:::\n\n\n::::\n:::::\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-chap06-plot-dist}\n: Plotting a distribution with {**ggplot2**}\n::::::\n:::\n::::{.my-procedure-container}\n\n1. As there is no data (just the formula for the function) we need to specify the x-limits.\n2. `ggplot2::stat_function` draws the function. We can specify the function extra or create an anonymous function or --- as I have done here --- use a function from an R package. Note that there is no parenthesis behind the function name.\n\n::::\n:::::\n\n\n###### comparing t\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-compare-t-prob-dist}\n: Student t distributions with 1 and 7144 degree of freedom (df) and normal distribution compared\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot(tibble::tibble(x = c(-7, 7)), \n         ggplot2::aes(x)) +\n         ggplot2::stat_function(\n             fun = stats::dt,\n             args = list(df = 1),\n             geom = \"line\",\n             linewidth = 0.7,\n             ggplot2::aes(linetype = \"1\")\n         ) +\n         ggplot2::stat_function(\n             fun = stats::dt,\n             args = list(df = 7144),\n             geom = \"line\",\n             linewidth = 0.7,\n             ggplot2::aes(linetype = \"5\")\n         ) + \n         ggplot2::stat_function(\n             fun = stats::dt,\n             args = list(df = 30),\n             geom = \"line\",\n             linewidth = 0.7,\n             ggplot2::aes(linetype = \"3\")\n         ) +\n         ggplot2::stat_function(\n            fun = stats::dnorm,\n            geom = \"line\",\n            linewidth = 0.7,\n            ggplot2::aes(color = \"red\")\n         ) +\n         ggplot2::scale_linetype_discrete(\n             name = \"t dist\",\n             labels = c(\"df = 1\", \"df = 30\", \"df = 7144\")\n         ) +\n         ggplot2::scale_color_discrete(\n             name = \"normal dist\",\n             labels = \"mean = 0, sd = 1\"\n         ) +\n         ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Student t distributions with 1 and 7144 degree of freedom (df) and normal distribution compared](06-t-test_files/figure-html/fig-compare-t-prob-dist-1.png){#fig-compare-t-prob-dist width=672}\n:::\n:::\n\n***\n\nThe plot shows two things:\n\n1. There is a big difference between a t distribution with df = 1 and df = 30.\n2. There is no visible difference between t with df = 30, df = 7144 and a normal distribution.\n\n\n::::\n:::::\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-chap06-plot-several-dist}\n: Plotting several distributions with {**ggplot2**}\n::::::\n:::\n::::{.my-procedure-container}\n\n1. Use for every distribution `ggplot2::stat_function()`.\n2. Put the aesthetic into an `ggplot2::aes()` function.\n3. Add for each legend a corresponding scale with name and labels.\n\n::::\n:::::\n\n\n###### t systolic\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-code-name-b}\n: t-distribution (df = 7,144) shaded for values of 2.4491 or higher (replicating Figure 6.5)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot() +\n    ggplot2::xlim(-4, 4) +\n    ggplot2::stat_function(\n             fun = stats::dt,\n             args = list(df = df_sys),\n             geom = \"line\",\n             linewidth = 0.7\n             ) +\n    ggplot2::stat_function(\n             fun = stats::dt,\n             args = list(df = df_sys),\n             geom = \"area\",\n             xlim = c(t_sys, 4),\n             ggplot2::aes(fill = \n                paste(\"t >=\", round(t_sys, 3))\n                ) \n             ) +\n    ggplot2::theme_bw() +\n    ggplot2::scale_fill_manual(\n        name = \"\",\n        values = \"steelblue\"\n    )\n```\n\n::: {.cell-output-display}\n![t-distribution (df = 7,144) shaded for values of 2.4491 or higher (replicating Figure 6.5)](06-t-test_files/figure-html/fig-t-test-systolic-1.png){#fig-t-test-systolic width=672}\n:::\n:::\n\n***\n\nThe plot shows that the t-value of 2.499 is very unlikely if the null hypotheses were true, e.g. if the sample comes from a population with a systolic blodd pressure mean of 240.\n\n\n::::\n:::::\n\n###### 2.5% shaded\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-two-sided-shaded}\n: t-distribution (df = 7,144) with 2.5% shaded in each tail of the distribution (replicating Figure 6.6)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot() +\n    ggplot2::xlim(-4, 4) +\n    ggplot2::stat_function(\n             fun = stats::dt,\n             args = list(df = df_sys),\n             geom = \"line\",\n             linewidth = 0.7\n             ) +\n    ggplot2::stat_function(\n             fun = stats::dt,\n             args = list(df = df_sys),\n             geom = \"area\",\n             xlim = c(1.96, 4),\n             ggplot2::aes(fill = \n                paste(\"Rejection region\")\n                ) \n             ) +\n    ggplot2::stat_function(\n         fun = stats::dt,\n         args = list(df = df_sys),\n         geom = \"area\",\n         xlim = c(-4, -1.96),\n         ggplot2::aes(fill = \n            paste(\"Rejection region\")\n            ) \n         ) +\n    ggplot2::theme_bw() +\n    ggplot2::scale_fill_manual(\n        name = \"\",\n        values = \"purple3\"\n    ) +\n    ggplot2::labs(\n        x = \"t-statistic\",\n        y = \"Probability density\"\n    )\n```\n\n::: {.cell-output-display}\n![t-distribution (df = 7,144) with 2.5% shaded in each tail of the distribution (replicating Figure 6.6)](06-t-test_files/figure-html/fig-two-sided-shaded-1.png){#fig-two-sided-shaded width=672}\n:::\n:::\n\n***\n\nThe plot shows the rejection regions for the probability of 95%.\n\n$$\n\\begin{align*}\np_{low}(x > 1.96) \\approx 0.025 \\\\\np_{high}(x < 1.96) \\approx 0.975 \\\\\np_{high} - p_{low} = \\\\\n0.975 - 0.025 = 0.95\n\\end{align*}\n$$ \n\n::::\n:::::\n\n###### t systolic & 2.5% shaded overlaid\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-t-two-sided-shaded}\n: t-distribution (df = 7,144) with 2.5% shaded in each tail of the distribution (replicating Figure 6.6)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot() +\n    ggplot2::xlim(-4, 4) +\n    ggplot2::stat_function(\n             fun = stats::dt,\n             args = list(df = df_sys),\n             geom = \"line\",\n             linewidth = 0.7\n             ) +\n    ggplot2::stat_function(\n             fun = stats::dt,\n             args = list(df = df_sys),\n             geom = \"area\",\n             xlim = c(1.96, 4),\n             alpha = 0.5,\n             ggplot2::aes(fill = \n                paste(\"Rejection region\"),\n                ) \n             ) +\n    ggplot2::stat_function(\n         fun = stats::dt,\n         args = list(df = df_sys),\n         geom = \"area\",\n         xlim = c(-4, -1.96),\n        alpha = 0.5,\n         ggplot2::aes(fill = \n            paste(\"Rejection region\"),\n            ),\n         ) +\n    ggplot2::stat_function(\n     fun = stats::dt,\n     args = list(df = df_sys),\n     geom = \"area\",\n     xlim = c(t_sys, 4),\n     alpha = 0.5,\n     ggplot2::aes(fill = \n        paste(\"t >=\", round(t_sys, 3))\n        ) \n     ) +\n    ggplot2::theme_bw() +\n    ggplot2::scale_fill_manual(\n        name = \"\",\n        values = c(\"purple3\", \"red\")\n    ) +\n    ggplot2::labs(\n        x = \"t-statistic\",\n        y = \"Probability density\"\n    )\n```\n\n::: {.cell-output-display}\n![t-distribution (df = 7,144) with 2.5% shaded in each tail of the distribution (replicating Figure 6.6)](06-t-test_files/figure-html/fig-t-two-sided-shaded-1.png){#fig-t-two-sided-shaded width=672}\n:::\n:::\n\n***\n\nThe plot shows that the t-value is in the <a class='glossary' title='Rejection region is the area under the curve of a sampling distribution where the probability of obtaining a value is very small, often below 5%; the rejection region is in the end of the tail or tails of the distribution. (SwR, Glossary)'>rejection area</a>, e.g., the null has to be rejected.\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n### NHST Step 4\n\nConclude and write the report.\n\nEven though the difference between the mean systolic blood pressure of 120.54 and the hypothesized value of 120 is small, it is statistically significant. The probability of this sample that it comes from a population where the mean systolic blood pressure is actually 120 is just 1.4%. This sample is likely to be from a population with a higher mean blood pressure.\n\n::: {.callout-tip}\nThe mean systolic blood pressure in a sample of 7145 people was 120.54 (sd = 18.62). A one-sample t-test found this mean to be statistically significantly different from the hypothesized mean of 120 [t(7144) = 2.449; p = 0.014]. The sample likely came from a population with a mean systolic blood pressure not equal to 120.\n:::\n\n## Achievement 3: Independent-samples t-test {#sec-chap06-achievement3}\n\nInstead of comparing one mean to a hypothesized or population mean, the <a class='glossary' title='Independent-samples t-test or unpaired sample t-test is an inferential test comparing two independent means. (SwR, Glossary)'>independent-samples t-test</a> compares the means of two groups to each other.\n\nWe could for instance be interested to see if the blood pressure for persons of different sex are the same. Or the question statistically formulated: Do males and females in the sample come from a population where males and females have the same mean systolic blood pressure?\n\nI am not going into the details of achievement 3 because there is no much difference between the procedure for the one-sample t-test and the independent-samples t-test. Essentially there are only two differences:\n\n### Formula\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap06-t-independent-test}\n: Independent-samples t-test formula\n::::::\n:::\n::::{.my-theorem-container}\n\n$$\nt = \\frac{m_{1} - m_{2}}{\\sqrt{\\frac{s_1^2}{n_{1}} + \\frac{s_2^2}{n_{2}}}}\n$$ {#eq-chap06-t-independent-test}\n\n- $m_{1}$ represents the mean of one group, \n- $m_{2}$ represents the mean of another group, \n- $s_{1}^2$ is the variance of the first group,\n- $s_{2}^2$ is the variance of the second group,\n- $n_{1}$ is the size of the first group,\n- $n_{2}$ is the size of the second group.\n::::\n:::::\n\n### Computing\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-t-indpendent-test}\n: Independent-samples t-test for systolic blood pressure of males and females\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {#lst-chap06-t-indpendent-test}\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean = base::readRDS(\"data/chap06/bp_clean.rds\")\n\nbp_clean |> \n    tidyr::drop_na(systolic) |> \n    dplyr::group_by(sex) |> \n    dplyr::summarize(\n        mean_systolic = mean(systolic),\n        var_systolic = var(systolic),\n        sample_size = dplyr::n()\n    )\n\ntwo_sample_t <- t.test(formula = \n           bp_clean$systolic ~ bp_clean$sex)\ntwo_sample_t\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 4\n#>   sex    mean_systolic var_systolic sample_size\n#>   <fct>          <dbl>        <dbl>       <int>\n#> 1 Male            122.         329.        3498\n#> 2 Female          119.         358.        3647\n#> \n#> \tWelch Two Sample t-test\n#> \n#> data:  bp_clean$systolic by bp_clean$sex\n#> t = 7.3135, df = 7143, p-value = 2.886e-13\n#> alternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n#> 95 percent confidence interval:\n#>  2.347882 4.067432\n#> sample estimates:\n#>   mean in group Male mean in group Female \n#>             122.1767             118.9690\n```\n\n\n:::\n:::\n\n\nIndependent-samples t-test for systolic blood pressure of males and females\n:::\n::::\n:::::\n\nIt is important to note that category variables like `sex` do not work with the default `x, y` version of the t-test. Therefore we have to apply the method with class `formula`.\n\nIn R formulae a single variable on the left-hand side is followed by the tilde sign `~` and one ore more objects that predict or explain the left-hand side.\n\n> In a lot of statistical tests, the object on the left-hand side of the formula is the <a class='glossary' title='Outcome is the variable being explained or predicted by a model; in linear and logistic regression, the outcome variable is on the left-hand side of the equal sign. (SwR, Glossary)'>outcome</a> or dependent variable while the object(s) on the right-hand side of the formula are the <a class='glossary' title='Predictor variable – also known sometimes as the independent or explanatory variable – is the counterpart to the response or dependent variable. Predictor variables are used to make predictions for dependent variables. (DeepAI, MiniTab)'>predictors</a> or independent variables. (SwR)\n\n> It is commonly used to generate *design matrices* for modeling function (e.g. `lm`) [@kuhn2017].\n\nIn our case, systolic blood pressure is the *outcome* being explained by the *predictor* of sex.\n\nIn R the default t-test for independent samples is the <a class='glossary' title='Welch’s t-test is a variation on the Student’s t-test that does not assume equal variances in group (SwR, Glossary).'>Welch’s t-test</a> and not the student t-test. \n\n> Welch’s t-test is slightly different from the original formula for t, which used pooled variance in the denominator. <a class='glossary' title='Pooled variance is the assumption that the variances in two groups are equal, so these variances are combined (‘pooled’) (SwR, Glossary).'>Pooled variance</a> assumes that the variances in the two groups are equal and combines them. (SwR)\n\nThere is an scientific article explaining why Welch's t-test should be used in any case, even if the assumption of homogeneity of variance is met: \n\n> We show that the Welch’s t-test provides a better control of Type 1 error rates when the assumption of homogeneity of variance is not met, and it loses little robustness compared to Student’s t-test when the assumptions are met. We argue that Welch’s t-test should be used as a default strategy. [@delacre2017; see also: @delacre2022]\n\nJust to conclude this abbreviated section I quote the final summary reporting the independent t-test results.\n\n::: {.callout-tip}\n> There was a statistically significant difference [t(7143) = 7.31; p < .05] in mean systolic blood pressure between males (m = 122.18) and females (m = 118.97) in the sample. The sample was taken from the U.S. population, indicating that males in the United States likely have a different mean systolic blood pressure than females in the United States. The difference between male and female mean systolic blood pressure was 3.21 in the sample; in the population this sample came from, the difference between male and female mean blood pressure was likely to be between 2.35 and 4.07 (d = 3.21; 95% CI: 2.35–4.07). (SwR)\n:::\n\n\n## Achievement 4: Dependent-samples t-test {#sec-chap06-achievement4}\n\nAgain: I am not going to summarize this section because it resembles achievement 2 (one-sample t-test) and achievement 3 (independent.samples t-test).\n\n### Formula\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap06-t-dependent-test}\n: Independent-samples t-test formula\n::::::\n:::\n::::{.my-theorem-container}\n\n$$\nt = \\frac{m_{d} - 0}{\\sqrt{\\frac{s_d}{n_{d}}}}\n$$ {#eq-chap06-t-dependent-test}\n\n- $m_{d}$ represents the mean of differences between to measures, \n- $s_{d}^2$ is the variance of the mean differences between to measures,\n- $n_{d}$ is the sample size,\n- $0$ subtracting represents the null hypothesis; zero is the mean difference if the two measures were exactly the same.\n::::\n:::::\n\n### Computing\n\n:::::{.my-important}\n:::{.my-important-header}\nGeneral advice before starting a test statistics\n:::\n::::{.my-important-container}\nAlways look at some visuals and descriptive statistics before you are starting the test procedure and following the NHST procedure as outlined in @prp-chap05-nhst.\n::::\n:::::\n\nThis shows that the *difference of the mean* between two measures is very small (0.55 mmHG). But it turned out that this value is highly statistically significant. But from a clinical point of view it is irrelevant!\n\n:::::{.my-important}\n:::{.my-important-header}\nStatistically significant != meaningful!\n:::\n::::{.my-important-container}\nAll our three t-tests result in small but statistically significant values. This is an important reminder that statistically significant p-values are not necessarily of relevance.\n\nBy the way: The reason for our small but statistically significant values are very large samples.\n::::\n:::::\n\n\nThe computation in R is the same as in @lst-chap06-t-indpendent-test. Again apply the formula version of Welch’s t-test with the only difference to add the argument `paired = TRUE`.\n\n## Achievement 5: Effect size {#sec-chap06-achievement5}\n\n### Introduction\n\nWe haven seen that even the very small difference of 0.54 mmHG systolic blood pressure is with a large sample size statistically significant. But this small difference is clinically not relevant. To judge the importance of some statistically significant results we need effect sizes as another criteria. \n\nThe proportion of people that believe that effect sizes are even more important than p-values is rising. P-values only report whether a difference or relationship from a sample is likely to be true in the population, while effect sizes provide information about the strength or size of a difference or relationship.\n\nIn @sec-chap05 we discussed for the Chi-squared test as effect sizes \n\n- Cramèrs V (@sec-chap05-cramers-v)\n- Phi coefficient $\\phi$ (@sec-chap05-phi-coefficient) and\n- Odds ratio (@sec-chap05-odds-ratio)\n\nFor t-test the appropriate effect size measure is <a class='glossary' title='Cohen’s d is a standardized effect size for measuring the difference between two group means. It is frequently used to compare a treatment to a control group. It can be a suitable effect size to include with t-test and ANOVA results. (Statistics by Jim)'>Cohen’s d</a>. \n\n:::::{.my-resource}\n:::{.my-resource-header}\nDelving into effects size assessment\n:::\n::::{.my-resource-container}\n\nDerived from the importance of effect size parameter I need to know more how to use and interpret different kind of effect sizes. What follows are some resources\n\n- [Effect Size](https://en.wikipedia.org/wiki/Effect_size) (Wikipedia)\n- [Effect size](https://www.psy.gla.ac.uk/~steve/best/effect.html) [@draper2023]\n- [Effect size calculator](https://www.ai-therapy.com/psychology-statistics/effect-size-calculator) (AI-Therapie Statistics)\n- [Rules of thumb on magnitudes of effect sizes](https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize) (MRC Cognition and Brain Sciences Unit, University of Cambridge)\n- [Computation of Effect Sizes](https://www.psychometrica.de/effect_size.html) [@lenhard2022]\n\n::::\n:::::\n\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap06-cohens-d}\n: Formula for Cohen’s d one-sample t-test\n::::::\n:::\n::::{.my-theorem-container}\n$$\nd = \\frac{m_{x} - \\mu_{x}}{s_{x}}\n$$ {#eq-chap06-cohens-d-one-sample}\n\n$m_{x}$ = sample mean for $x$\n$\\mu_{x}$ = hypothesized or population mean\n$s_{x}$ = sample standard deviation for $x$\n::::\n:::::\n\nThe formula is similar to the already well-known z-score calculation (@eq-chap04-z-score).\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-chap06-cohens-d}\n: Classification of Cohen’s d values\n::::::\n:::\n::::{.my-assessment-container}\n\n- **Small effect size**: Cohen’s d = .2 to d < .5 \n- **Medium effect size**: Cohen’s d = .5 to d < .8 \n- **Large effect size**: Cohen’s d ≥ .8\n\n::::\n:::::\n\n### Cohen’s d computation\n\n:::::{.my-resource}\n:::{.my-resource-header}\nPackages with Cohen’s d functions\n:::\n::::{.my-resource-container}\n\n- {**lsr**}: The recommendation from the book (see @pak-lsr)\n- {**effectsize**}: Indices of effect sizes (see @pak-effectsize)\n- {**rstatix**}: (see @pak-rstatix)\n\nThee is another package {**effsize**} that I have not used. I had problems to use it, because it has no argument for $\\mu$ but it also has not many downloads.\n\nThe packages {**effectsize**} and {**rstatix**} are important as they have many other computation for effect size parameters.\n\n\n\n::: {#tbl-donwload-numbers-cohens-d-packages .cell tbl-cap='Download average numbers of packages with Cohen’s d functions'}\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 4 × 4\n#>   package    average from       to        \n#>   <chr>        <dbl> <date>     <date>    \n#> 1 rstatix       5551 2024-03-21 2024-03-27\n#> 2 effectsize    1479 2024-03-21 2024-03-27\n#> 3 lsr            452 2024-03-21 2024-03-27\n#> 4 effsize        289 2024-03-21 2024-03-27\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n### Cohen’s d for one-sample t-tests\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-cohens-d}\n: Computation of Cohen’s d for one-sample t-tests\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Manual\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-cohens-d-manual}\n: Computation of Cohen’s d for one-sample t-tests manually (by hand)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean <-  base::readRDS(\"data/chap06/bp_clean.rds\")\n\n(estimate_sys - null_sys) / sd(bp_clean$systolic, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.02897354\n```\n\n\n:::\n:::\n\n\nThe effect size is very small, it is not even close the starting value for \"small effect size\".\n\n::: {.callout-tip}\nThe mean systolic blood pressure in a sample of 7,145 people was 120.54 (sd = 18.62). A one-sample t-test found this mean to be statistically significantly different from the hypothesized mean of 120 [t(7144) = 2.45; p = 0.014]. The sample likely came from a population with a mean systolic blood pressure not equal to 120. While the sample mean was statistically significantly different from 120, is has a very small effect size (Cohen’s d = .03).\n:::\n\n::::\n:::::\n\n\n###### lsr\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-cohens-d-one-sample-lsr}\n: Compute Cohen’s d with for one-sample t-tests {**lsr**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlsr::cohensD(bp_clean$systolic, mu = 120)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.02897354\n```\n\n\n:::\n:::\n\n***\n\n\n\n\n::::\n:::::\n\n\n###### effectsize\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-cohens-d-one-sample-effectsize}\n: Compute Cohen’s d for one-sample t-tests with {**effectsize**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\neffectsize::cohens_d(\n    x = bp_clean$systolic,\n    mu = 120\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Missing values detected. NAs dropped.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Cohen's d |       95% CI\n#> ------------------------\n#> 0.03      | [0.01, 0.05]\n#> \n#> - Deviation from a difference of 120.\n```\n\n\n:::\n:::\n\n***\n\nThe {**effectsize**} packages discusses two alternatives for Cohen’s d:\n\n- **Hedges' g** provides a correction for small-sample bias (using the exact method) to Cohen's d. For sample sizes > 20, the results for both statistics are roughly equivalent. \n- **Glass’s delta** is appropriate when the standard deviations are significantly different between the populations, as it uses only the second group's standard deviation.\n::::\n:::::\n\n###### rstatix\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-cohens-d-one-sample-rstatix}\n: Compute Cohen’s d for one-sample t-tests with {**rstatix**}\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nrstatix::cohens_d(\n    data = bp_clean,\n    formula = systolic ~ 1,\n    mu = 120\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 6\n#>   .y.      group1 group2     effsize     n magnitude \n#> * <chr>    <chr>  <chr>        <dbl> <int> <ord>     \n#> 1 systolic 1      null model  0.0290  7145 negligible\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n### Cohen’s d for independent-samples t-tests\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap06-dependent-samples-cohens-d}\n: Formula for independent-samples cohen’s d\n::::::\n:::\n::::{.my-theorem-container}\n\n$$\nd = \\frac{m_{1}-{m_{2}}}{\\sqrt{\\frac{s_{1}^2 + s_{2}^2}{2}}}\n$$ {#eq-chap06-cohens-d-independent-samples}\n\n***\n\n- $m_{1}, m_{2}$: sample means \n- $s_{1}^2, s_{2}^2$: sample variances\n::::\n:::::\n\n\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-cohens-d}\n: Computation of Cohen’s d for independent-samples t-tests\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Manual\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-cohens-d-independent-samples-manual}\n: Computation of Cohen’s d for independent-samples t-tests manually (by hand)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean <-  base::readRDS(\"data/chap06/bp_clean.rds\")\n\nbp_ind_samples <- \n    bp_clean |> \n    tidyr::drop_na(systolic) |> \n    dplyr::group_by(sex) |> \n    dplyr::summarize(\n        mean_sys_sex = mean(systolic),\n        var_sys_sex = var(systolic)\n    )\nbp_ind_samples\n\nm1 <- bp_ind_samples[[1,2]] \nm2 <- bp_ind_samples[[2,2]] \nvar1 <- bp_ind_samples[[1,3]]\nvar2 <- bp_ind_samples[[2,3]]\n\n(m1 - m2) / (sqrt((var1 + var2) / 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 3\n#>   sex    mean_sys_sex var_sys_sex\n#>   <fct>         <dbl>       <dbl>\n#> 1 Male           122.        329.\n#> 2 Female         119.        358.\n#> [1] 0.1730045\n```\n\n\n:::\n:::\n\n\nThe effect size is very small.\n\n::::\n:::::\n\n\n\n\n\n###### lsr\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-cohens-d-independent-samples-lsr}\n: Compute Cohen’s d of independent samples with {**lsr**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlsr::cohensD(\n    x = systolic ~ sex,\n    data = bp_clean,\n    method = \"unequal\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.1730045\n```\n\n\n:::\n:::\n\n***\n\n1. Instead of a vector variable we are using the formula interface.\n2. Because we are using the Welch's t-test we change the default method \"pooled\", to \"unequal\", e.g., we are not assuming equal variances.\n\n\n::::\n:::::\n\n\n###### effectsize\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-cohens-d-independent-samples-effectsize}\n: Compute Cohen’s d for independent-samples t-tests with {**effectsize**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\neffectsize::cohens_d(\n    x = bp_clean$systolic,\n    y = bp_clean$sex,\n    pooled_sd = FALSE,\n    paired = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Missing values detected. NAs dropped.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Cohen's d |       95% CI\n#> ------------------------\n#> 0.17      | [0.13, 0.22]\n#> \n#> - Estimated using un-pooled SD.\n```\n\n\n:::\n:::\n\n***\n\nThe {**effectsize**} packages discusses two alternatives for Cohen’s d:\n\n- **Hedges' g** provides a correction for small-sample bias (using the exact method) to Cohen's d. For sample sizes > 20, the results for both statistics are roughly equivalent. \n- **Glass’s delta** is appropriate when the standard deviations are significantly different between the populations, as it uses only the second group's standard deviation.\n::::\n:::::\n\n###### rstatix\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-cohens-d-independent-samples-rstatix}\n: Compute Cohen’s d for independent-samples t-tests with {**rstatix**}\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nrstatix::cohens_d(\n    data = bp_clean,\n    formula = systolic ~ sex\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 7\n#>   .y.      group1 group2 effsize    n1    n2 magnitude \n#> * <chr>    <chr>  <chr>    <dbl> <int> <int> <ord>     \n#> 1 systolic Male   Female   0.173  3498  3647 negligible\n```\n\n\n:::\n:::\n\n***\n\n- We have used the default value for `paired = FALSE` because we have an independent samples t-test.\n- We have used the default value for `var.equal = FALSE` because we have used the Welch’s t-test taht does not assume equal variances.\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n### Cohen’s d for dependent-samples t-tests\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap06-dependent-samples-cohens-d}\n: Formula for dependent-samples cohen’s d\n::::::\n:::\n::::{.my-theorem-container}\n\n$$\nd = \\frac{m_{d}-0}{s_{d}}\n$$ {#eq-chap06-cohens-d-dependent-samples}\n\n***\n- $m_{d}$: mean difference between the two measures (for instance in our case, systolic and systolic2)\n- $s_{d}$: standard deviation of the differences between the two measures\n::::\n:::::\n\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-cohens-d-dependent-samples}\n: Computation of Cohen’s d for dependent-samples t-tests\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Manual\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-cohens-d-dependent-samples-manual}\n: Computation of Cohen’s d for dependent-samples t-tests manually (by hand)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean <-  base::readRDS(\"data/chap06/bp_clean.rds\")\n\nbp_ind_samples <- bp_clean |> \n    tidyr::drop_na(diff_syst) |> \n    dplyr::summarize(\n        mean_diff = mean(diff_syst),\n        sd_diff = sd(diff_syst)\n    )\nbp_ind_samples\n\n\n(bp_ind_samples$mean_diff - 0) / bp_ind_samples$sd_diff\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   mean_diff  sd_diff\n#> 1 0.5449937 4.898043\n#> [1] 0.1112676\n```\n\n\n:::\n:::\n\n\nThe effect size is very small.\n\n::::\n:::::\n\n\n\n\n\n###### lsr\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-cohens-d-dependent-samples-lsr}\n: Compute Cohen’s d of dependent samples with {**lsr**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlsr::cohensD(\n    x = bp_clean$systolic, \n    y = bp_clean$systolic2, \n    method = \"paired\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.1112676\n```\n\n\n:::\n:::\n\n***\n\nInstead of the default method (\"pooled\") we need \"paired\" as method for the dependent-samples t-test.\n\n\n::::\n:::::\n\n\n###### effectsize\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-cohens-d-dependent-samples-effectsize}\n: Compute Cohen’s d for dependent-samples t-tests with {**effectsize**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\neffectsize::cohens_d(\n    x = bp_clean$systolic,\n    y = bp_clean$systolic2,\n    paired = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Missing values detected. NAs dropped.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Cohen's d |       95% CI\n#> ------------------------\n#> 0.11      | [0.09, 0.13]\n```\n\n\n:::\n:::\n\n***\n\nThe {**effectsize**} packages discusses two alternatives for Cohen’s d:\n\n- **Hedges' g** provides a correction for small-sample bias (using the exact method) to Cohen's d. For sample sizes > 20, the results for both statistics are roughly equivalent. \n- **Glass’s delta** is appropriate when the standard deviations are significantly different between the populations, as it uses only the second group's standard deviation.\n::::\n:::::\n\n###### rstatix\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-cohens-d-dependent-samples-rstatix}\n: Compute Cohen’s d for dependent-samples t-tests with {**rstatix**}\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean |> \n    dplyr::select(systolic, systolic2) |> \n    tidyr::drop_na() |> \n    tidyr::pivot_longer(\n        cols = c(\"systolic\", \"systolic2\"), \n        names_to = \"treatment\", \n        values_to = \"value\") |> \n    rstatix::cohens_d(\n        formula = value ~ treatment,\n        paired = TRUE   \n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 7\n#>   .y.   group1   group2    effsize    n1    n2 magnitude \n#> * <chr> <chr>    <chr>       <dbl> <int> <int> <ord>     \n#> 1 value systolic systolic2   0.111  7101  7101 negligible\n```\n\n\n:::\n:::\n\n\nIn order to get a working cohen’s d computation with {**rstatix**} I had to apply `tidyr::pivot_longer()`.\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n## Achievement 6: t-test assumptions {#sec-chap06-achievement6}\n\n### Introduction\n\nIn this section I am not following the sequence of the book. I start with an overview and will separate \n- checking visually the normality assumption\n- testing with different approaches the normality assumption\n\n***\n::: {#bul-t-test-assumptions}\n\n- One-sample t-test assumptions \n    - Continuous variable \n    - Independent observations \n    - Normal distribution \n- Independent-samples t-test assumptions \n    - Continuous variable and two independent groups \n    - Independent observations \n    - Normal distribution in each group \n    - Equal variances for each group\n- Dependent-samples t-test assumptions \n    - Continuous variable and two dependent groups \n    - Independent observations \n    - Normal distribution of differences\n   \nAssumptions for the different t-tests\n:::\n***\n\n\n\n### Checking normality assumption visually\n\n#### One-sample t-test\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-normality-one-sample}\n: One-sample t-test: Checking the normality assumption visually\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### 30 bins\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-normality-one-sample30}\n: Checking normality visually (30 bins)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean <-  base::readRDS(\"data/chap06/bp_clean.rds\")\nbp_clean2 <- bp_clean |> \n    dplyr::select(systolic) |> \n    tidyr::drop_na() \n    \nhist_dnorm(\n    df = bp_clean2,\n    v = bp_clean2$systolic,\n    n_bins = 30,\n    x_label = \"Systolic blood pressure (mmHg)\"\n)\n```\n\n::: {.cell-output-display}\n![Distribution of systolic blood pressure in mmHg for 20152016 NHANES participants (30 bins)](06-t-test_files/figure-html/fig-normality-one-sample30-1.png){#fig-normality-one-sample30 width=672}\n:::\n:::\n\n\n***\n\nThe data appear right-skewed, e.g. the right tail is longer than the left tail. There is also the assumption that the distribution is bimodal, e.g., has two modes.\n\nThe best way to check if this assumption is correct is to change the number of bins.\n\n::::\n:::::\n\n\n###### 60 bins\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-normality-one-sample60}\n: Checking normality visually (60 bins)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nhist_dnorm(\n    df = bp_clean2,\n    v = bp_clean2$systolic,\n    n_bins = 60,\n    x_label = \"Systolic blood pressure (mmHg)\"\n)\n```\n\n::: {.cell-output-display}\n![Distribution of systolic blood pressure in mmHg for 20152016 NHANES participants (60 bins)](06-t-test_files/figure-html/fig-normality-one-sample60-1.png){#fig-normality-one-sample60 width=672}\n:::\n:::\n\n***\nAnother graph --- this time with 60 bins --- shows that we have a unimodal distribution.\n::::\n:::::\n\n###### Q-Q plot\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-normality-one-sample-q-q-plot}\n: Checking normality visually with a Q-Q plot\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean2 |> \n    ggplot2::ggplot(\n        ggplot2::aes(sample = systolic)\n    ) +\n    ggplot2::stat_qq(\n        ggplot2::aes(color = \"NHANES participant\")\n    ) +\n    ggplot2::stat_qq_line(\n        ggplot2::aes(linetype = \"Normally distributed\"),\n        linewidth = 1\n    ) +\n    ggplot2::labs(\n        x = \"Theoretical normal distribution\",\n        y = \"Observes systolic blood pressure (mmHG)\"\n    ) +\n    ggplot2::scale_color_manual(\n        name = \"\",\n        values = (\"NHANES participant\" = \"purple3\")\n    ) +\n    ggplot2::scale_linetype_manual(\n        name = \"\",\n        values = (\"Normally distributed\" = \"solid\")\n    )\n```\n\n::: {.cell-output-display}\n![Distribution of systolic blood pressure in mmHg for 2015-2016 NHANES participants](06-t-test_files/figure-html/fig-normality-one-sample-q-q-plot-1.png){#fig-normality-one-sample-q-q-plot width=672}\n:::\n:::\n\n***\n\n(This is the replication of book Figure 6.12, that has no accompanying code.)\n\nThis <a class='glossary' title='A quantile-quantile plot is a visualization of data using probabilities to show how closely a variable follows a normal distribution. (SwR, Glossary) This plot is made up of points below which a certain percentage of the observations fall. On the x-axis are normally distributed values with a mean of 0 and a standard deviation of 1. On the y-axis are the observations from the data. If the data are normally distributed, the values will form a diagonal line through the graph. (SwR, chapter 6)'>q-q-plot</a> shows clearly that we have failed the assumption of a normal distribution. But often the visible check is not so obvious. Then we would need a statistical test to check for normality.\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n#### Independent-samples t-test\n\nNormality has to be checked for each group for the independent-samples t-test (and dependent-samples t-test).\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-normality-independent-samples}\n: Independent-samples t-test: Checking the normality assumption visually\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Histograms\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-normality-independent-histograms}\n: Checking normality with histogram\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean <- base::readRDS(\"data/chap06/bp_clean.rds\")\n\nbp_clean |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = systolic)\n    ) +\n    ggplot2::geom_histogram(\n        fill = \"purple3\",\n        col = \"white\",\n        na.rm = TRUE,\n        bins = 30\n    ) +\n    ggplot2::facet_grid(\n        cols = ggplot2::vars(sex)\n    ) +\n    ggplot2::labs(\n        x = \"Systolic blood pressure (mmHg)\", \n        y = \"NHANES participants\"\n    )\n```\n\n::: {.cell-output-display}\n![Distribution of systolic blood pressurein mmHg for 20152016 NHANES participants by sex](06-t-test_files/figure-html/fig-normality-independent-histograms-1.png){#fig-normality-independent-histograms width=672}\n:::\n:::\n\n***\n\nBoth histograms look right-skewed.\n::::\n:::::\n\n\n###### Q-Q plot\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-normality-independent-q-q-plot}\n: Checking normality with Q-Q plot\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean |> \n    tidyr::drop_na(systolic) |> \n    ggplot2::ggplot(\n        ggplot2::aes(sample = systolic)\n    ) +\n    ggplot2::stat_qq(\n        ggplot2::aes(color = \"NHANES participant\"),\n        alpha = 0.5\n    ) +\n    ggplot2::stat_qq_line(\n        ggplot2::aes(linetype = \"Normally distributed\"),\n        linewidth = 1\n    ) +\n    ggplot2::facet_grid(\n        cols = ggplot2::vars(sex)\n    ) +\n    ggplot2::labs(\n        x = \"Theoretical normal distribution\",\n        y = \"Observes systolic blood pressure (mmHG)\"\n    ) +\n    ggplot2::scale_color_manual(\n        name = \"\",\n        values = (\"NHANES participant\" = \"purple3\")\n    ) +\n    ggplot2::scale_linetype_manual(\n        name = \"\",\n        values = (\"Normally distributed\" = \"solid\")\n    )\n```\n\n::: {.cell-output-display}\n![Checking normality of systolic blood pressure in mmHg for 20152016 NHANES participants by sex with Q-Q plots](06-t-test_files/figure-html/fig-normality-independent-q-q-plot-1.png){#fig-normality-independent-q-q-plot width=672}\n:::\n:::\n\n\nHarris uses in the book a more complicated code passage to build the line for the normal distribution. Instead using the function `ggplot2::stat_qq_line()` she calculates intercept and slopes and applies the `ggplot2::abline()` function.\n\nThe q-q plot shows that both blood pressures (males and females) are not normally distributed.\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n#### Dependent samples t-test\n\nHere we are going to check the distribution of the difference between the first (systolic) and second blood pressure measure (systolic2).\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-normality-dependent}\n: Dependent-samples t-test: Checking the normality assumption visually\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Histogram\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-normality-dependent-histogram}\n: Checking the normality assumption visually with a histogram and an overlaid normal distribution\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean <- base::readRDS(\"data/chap06/bp_clean.rds\")\n\nbp_clean3 <- bp_clean |> \n    tidyr::drop_na(diff_syst)\n    \nhist_dnorm(\n    df = bp_clean3,\n    v = bp_clean3$diff_syst,\n    n_bins = 30,\n    x_label = \"Difference of two systolic blood pressures in mmHg taken from the same person\"\n)\n```\n\n::: {.cell-output-display}\n![Checking the normality assumption visually of a dependent group distribution with a histogram and an overlaid normal distribution](06-t-test_files/figure-html/fig-normality-dependent-histogram-1.png){#fig-normality-dependent-histogram width=672}\n:::\n:::\n\n***\n\nThe histogram looks like a normal distribution. But let's verify this rsult with a q-q plot.\n::::\n:::::\n\n\n###### Q-Q plot\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-normality-dependent-q-q-plot}\n: Checking the normality assumption visually with a quantile-quantile plot\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean3 |> \n    ggplot2::ggplot(\n        ggplot2::aes(sample = diff_syst)\n    ) +\n    ggplot2::stat_qq(\n        ggplot2::aes(color = \"NHANES participant\")\n    ) +\n    ggplot2::stat_qq_line(\n        ggplot2::aes(linetype = \"Normally distributed\"),\n        linewidth = 1\n    ) +\n    ggplot2::labs(\n        x = \"Theoretical normal distribution\",\n        y = \"Observes systolic blood pressure (mmHG)\"\n    ) +\n    ggplot2::scale_color_manual(\n        name = \"\",\n        values = (\"NHANES participant\" = \"purple3\")\n    ) +\n    ggplot2::scale_linetype_manual(\n        name = \"\",\n        values = (\"Normally distributed\" = \"solid\")\n    )\n```\n\n::: {.cell-output-display}\n![Checking the normality assumption visually of a dependent group distribution with a quantile-qunatile plot](06-t-test_files/figure-html/fig-normality-dependent-q-q-plot-1.png){#fig-normality-dependent-q-q-plot width=672}\n:::\n:::\n\n***\n\nBad news! This is a disappointing result: Th Q-Q plot shows that we do not have normal distribution. It just looked like one as the lower and higher values cancel each out.\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n\n#### Summary of the visual inspections\n\nFor all t-tests we applied in this chapter we got the clear result that they all failed the normality assumption. \n\nBut often the visual inspection is not conclusive. Therefore it is important also to apply statistical tests.\n\n### Checking normality assumption with statistical tests\n\n#### Skewness\n\n:::::{.my-important}\n:::{.my-important-header}\nDifferent statistical checks for normality for different situations\n:::\n::::{.my-important-container}\n\nDifferent statistical checks of normality are useful in different situations. \n\n- The mean of a variable is sensitive to skew, so checking for <a class='glossary' title='Skewness is the extent to which a variable has extreme values in one of the two tails of its distribution (SwR, Glossary)'>skewness</a> (@sec-chap02-skewness) is important when a statistical test relies on means (like t-tests). \n- When the focus of a statistical test is on variance, it is a good idea to examine <a class='glossary' title='Kurtosis is a measure of how many observations are in the tails of a distribution; distributions that look bell-shaped, but have a lot of observations in the tails (platykurtic) or very few observations in the tails (leptokurtic) (SwR, Glossary)'>kurtosis</a> (@sec-chap02-kurtosis) because variance is sensitive to problems with kurtosis (e.g., a <a class='glossary' title='Platykurtic is a distribution of a numeric variable that has more observations in the tails than a normal distribution would have; platykurtic distributions often look flatter than a normal distribution. (SwR, Glossary)'>platykurtic</a> or <a class='glossary' title='Leptokurtic is a distribution of a numeric variable that has many values clustered around the middle of the distribution; leptokurtic distributions often appear tall and pointy compared to mesokurtic or platykurtic distributions. (SwR, Glossary)'>leptokurtic</a> distribution)\n::::\n:::::\n\n:::::{.my-resource}\n:::{.my-resource-header}\nPackages with tests for skewness \n:::\n::::{.my-resource-container}\nAfter another research I will --- in addition to @sec-chap02-skewness --- propose other packages with tests for skewness. A comparison shows that these tests are much more popular given their download figures as the two packages (@pak-semTools and @pak-statpsych) I have already reviewed. All of the mentioned packages have functions for tests for kurtosis as well.\n\n- {**datavizard**}: Easy Data Wrangling and Statistical Transformations\n- {**e1071**}: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien\n- {**moments**}: Moments, Cumulants, Skewness, Kurtosis and Related Tests\n- {**psych**}: Procedures for Psychological, Psychometric, and Personality Research \n\n\n::: {#tbl-download-pkgs-skewness .cell tbl-cap='Download average numbers of packages with skewness functions'}\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 6 × 4\n#>   package    average from       to        \n#>   <chr>        <dbl> <date>     <date>    \n#> 1 e1071        10808 2024-03-21 2024-03-27\n#> 2 psych         7565 2024-03-21 2024-03-27\n#> 3 datawizard    3618 2024-03-21 2024-03-27\n#> 4 moments       1792 2024-03-21 2024-03-27\n#> 5 semTools       475 2024-03-21 2024-03-27\n#> 6 statpsych       12 2024-03-21 2024-03-27\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-skewness-systolic-bp}\n: Skewness of systolic blood pressure\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### semTools\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-skewness-systolic-bp-semtools}\n: Compute skewness of systolic blood pressure with {**semTools**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsemTools::skew(bp_clean$systolic)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning in semTools::skew(bp_clean$systolic): Missing observations are removed\n#> from a vector.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   skew (g1)          se           z           p \n#>  1.07037232  0.02897841 36.93689298  0.00000000\n```\n\n\n:::\n:::\n\n***\n\nComparing the z-value in @tbl-chap02-skewness we see that our value is 5 times higher than 7, meaning our distribution is highly right skewed. With a p-value < 0.001 we have to reject the null (that we have a normal distribution).\n\n::::\n:::::\n\n\n###### e1071\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-skewness-systolic-bp-e1071}\n: Compute skewness of systolic blood pressure with {**e1071**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ne1071::skewness(\n    x = bp_clean$systolic,\n    na.rm = TRUE,\n    type = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1.070372\n```\n\n\n:::\n:::\n\n***\n\nThere are three slightly different types of skewness computation with {**e1071**}.\n\n- Type 1: Older textbooks ('classic')\n- Type 2: SAS and SPSS\n- Type 3: MINITAB and BMDP (default value)\n\nIf we compare with the skewness computation  of {**semTools**} we see that SAS/SPSSS calculation is used with type 2 in {**e1071**}.\n\nWe can't assess the skewness with @tbl-chap02-skewness, because {**e1071**} does not calculate the z- and p-value. We therefore add another assessment criteria for skewness:\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-chap06-assessment-skewness}\n: Magnitude of skewness\n::::::\n:::\n::::{.my-assessment-container}\n\n- If the skewness value is close to 0 (between -0.5 and 0.5), the distribution is approximately symmetric.\n- If the skewness value is significantly negative (below -1), it suggests strong left skewness.\n- If the skewness value is significantly positive (above 1), it suggests strong right skewness. ([GeeksforGeeks](https://www.geeksforgeeks.org/skewness-measures-and-interpretation/)) [@parmarraman442023].\n::::\n:::::\n \nA value above 1.0 suggests a strong right skewness.\n\n::::\n:::::\n\n###### moments\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-skewness-systolic-bp-moments}\n: Compute skewness of systolic blood pressure with {**moments**}\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nmoments::skewness(\n    x = bp_clean$systolic,\n    na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1.070148\n```\n\n\n:::\n:::\n\n***\n\nThis slightly different value would be the result in {**e1071**} with `type = 1` (older textbooks).\n::::\n:::::\n\n###### datawizard\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-skewness-systolic-bp-datawizard}\n: Compute skewness of systolic blood pressure with {**datawizard**}\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n(\n    sys_skew_datawizard <- datawizard::skewness(\n        x = bp_clean$systolic,\n        remove.na = TRUE,\n        type = 2 # default\n    )\n)\n\nbase::summary(\n    sys_skew_datawizard, \n    test = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Skewness |    SE\n#> ----------------\n#>    1.070 | 0.029\n#> Skewness |    SE |      z | p\n#> -----------------------------\n#>    1.070 | 0.029 | 36.952 | 0\n```\n\n\n:::\n:::\n\n***\n\nThere are two versions with {**datawizard**}. One with just the skewness and standard error, another one --- similar as with {**semTools**} --- with skewness, standard error, z-value and p-value.\n\n::::\n:::::\n\n###### psych\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-skewness-systolic-bp-psych}\n: Compute skewness of systolic blood pressure with {**psych**}\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\npsych::skew(\n    x = bp_clean$systolic,\n    type = 2\n)\n\n(\n    sys_mardia_psych <- psych::mardia(bp_clean$systolic)\n)\n```\n\n::: {.cell-output-display}\n![](06-t-test_files/figure-html/skewness-systolic-bp-psych-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1.070372\n#> Call: psych::mardia(x = bp_clean$systolic)\n#> \n#> Mardia tests of multivariate skew and kurtosis\n#> Use describe(x) the to get univariate tests\n#> n.obs = 7145   num.vars =  1 \n#> b1p =  1.14   skew =  1363.19  with probability  <=  2.1e-298\n#>  small sample skew =  1364.33  with probability <=  1.2e-298\n#> b2p =  4.99   kurtosis =  34.33  with probability <=  0\n```\n\n\n:::\n:::\n\n\n***\n\nThere are two different approaches with {**psych**}:\n\n- A simple calculation of the skewness, very similar to the computation of {**e1071**}\n- A much more detailed result with the option to plot a q-q- plot to test normality.\n::::\n:::::\n\n###### Independent t-test\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-skewness-systolic-independent}\n: Statistical test of normality for systolic blood pressure by sex\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean |> \n    tidyr::drop_na(systolic) |> \n    dplyr::group_by(sex) |> \n    dplyr::summarize(\n        skew = semTools::skew(object = systolic)[1],\n        se_skew = semTools::skew(object = systolic)[2],\n        z_skew = semTools::skew(object = systolic)[3],\n        p_skew = semTools::skew(object = systolic)[4],\n        e1071 = e1071::skewness(x = systolic, type = 2),\n        moments = moments::skewness(x = systolic),\n        psych = psych::skew(x = systolic, type = 2)\n        )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 8\n#>   sex     skew se_skew z_skew p_skew e1071 moments psych\n#>   <fct>  <dbl>   <dbl>  <dbl>  <dbl> <dbl>   <dbl> <dbl>\n#> 1 Male    1.06  0.0414   25.6      0  1.06    1.06  1.06\n#> 2 Female  1.12  0.0406   27.6      0  1.12    1.12  1.12\n```\n\n\n:::\n:::\n\n***\n\nOur data fail the assumption for normality for the independent-samples t-test.\n\nThe columns 2-5 are calculated with {**semTools**}. For comparison I have added other the result of other packages as well.\n::::\n:::::\n\n###### Dependent t-test\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-skewness-systolic-dependent}\n: Statistical test of normality for difference of systolic blood pressure measure 1 and 2\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nsemTools::skew(object = bp_clean3$diff_syst)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    skew (g1)           se            z            p \n#> 2.351789e-01 2.906805e-02 8.090632e+00 6.661338e-16\n```\n\n\n:::\n:::\n\n\n***\nThe skewness value is much lower but still over the limit of 7 for our sample size. So the null has to be rejected (p-value < 0001).\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n***\n\n\n#### Different omnibus tests {#sec-chap06-omnibus-tests}\n\nBesides of testing skewness there are also other statistical tests available. I have researched the following <a class='glossary' title='An omnibus is a statistical test that identifies that there is some relationship going on between variables, but not what that relationship is. (SwR, Glossary)'>omnibus</a> tests:\n\n- **Shapiro-Wilk**: This is the book’s recommendation. The test is also called Shapiro-Francia and is a built-in test in R, available with `stats::shapiro.test()`. The test statistic of the Shapiro-Francia test is simply the squared correlation between the ordered sample values and the (approximated) expected ordered quantiles from the standard normal distribution. The <a class='glossary' title='The Shapiro-Wilk test is a statistical test to determine or confirm whether a variable has a normal distribution; it is sensitive to small deviations from normality and not useful for sample sizes above 5,000 because it will nearly always find non-normality. (SwR, Glossary)'>Shapiro-Wilk test</a> is known to perform well. But this test is very sensitive and can only be applied when the non-missing values are between 5 and 5000. With samples greater than 5,000 it will always result in statistically significant p-values. Therefore we can't use it as we have a sample size of about 7000 non-missing values.\n- **Anderson-Darling**: The <a class='glossary' title='The Anderson-Darling Goodness of Fit Test (AD-Test) is a measure of how well your data fits a specified distribution. It’s commonly used as a test for normality. (Statistics How-To)'>Anderson-Darling test</a> is the recommended <a class='glossary' title='In statistics, an empirical distribution function (commonly also called an empirical cumulative distribution function, eCDF) is the distribution function associated with the empirical measure of a sample. This cumulative distribution function is a step function that jumps up by 1/n at each of the n data points. Its value at any specified value of the measured variable is the fraction of observations of the measured variable that are less than or equal to the specified value. (Wikipedia) A CDF is a hypothetical model of a distribution, the ECDF models empirical (i.e. observed) data. (Statistics How To)'>EDF</a> test if there are more than seven values. It can be assessed via the {**nortest**} package, which is specialized for *nor*mality *test*s.\n- **Cramer von Mises**: It needs also more than 7 data values. Compared to the Anderson-Darling test (as a first choice) it gives less weight to the tails of the distribution.\n- **Kolmogorov-Smirnov**: Also called Liliefors test, needs at least 4 data values. The Lilliefors (Kolomorov-Smirnov) test is the most famous EDF omnibus test for normality. Compared to the Anderson-Darling test and the Cramer-von Mises test it is known to perform worse. Although the test statistic obtained from `nortest::lillie.test()` is the same as that obtained from `stats::ks.test()`, it is not correct to use the p-value from the latter for the composite hypothesis of normality (mean and variance unknown), since the distribution of the test statistic is different when the parameters are estimated.\n- **Pearson**: The Pearson chi-square test for normality is usually not recommended for testing the composite hypothesis of normality due to its inferior power properties compared to other tests. \n\n#### Anderson-Darling test\n\n:::::{.my-resource}\n:::{.my-resource-header}\nPackages for the Anderson-Darling test\n:::\n::::{.my-resource-container}\n- {**cmstatr**}: Statistical Methods for Composite Material Data (aerospace applications)\n- {**DescTools**}: Tools for Descriptive Statistics (@pak-DescTools)\n- {**kSamples**}: K-Sample Rank Tests and their Combinations\n- {**nortest**}: Tests for Normality (@pak-nortest)\n\n\n::: {#tbl-pkgs-anderson-darling-test .cell tbl-cap='Download average numbers of packages with Anderson-Darling test functions'}\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 4 × 4\n#>   package   average from       to        \n#>   <chr>       <dbl> <date>     <date>    \n#> 1 nortest      2448 2024-03-21 2024-03-27\n#> 2 DescTools    2099 2024-03-21 2024-03-27\n#> 3 kSamples      420 2024-03-21 2024-03-27\n#> 4 cmstatr        20 2024-03-21 2024-03-27\n```\n\n\n:::\n:::\n\n\nI will use only the two most downloaded packages: {**DescTools**} because I have already installed it (see @pak-DescTools) and {**nortest**} because it is a specialized package for test for normality and most articles refer to its usage.\n\n::::\n:::::\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-anderson-darling-tests}\n: Anderson-Darling Test of Normality\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### nortest\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-anderson-darling-test-nortest}\n: One-sample t-test: Test of normality using the Anderson-Darling test with the {**nortest**} package\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnortest::ad.test(bp_clean$systolic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tAnderson-Darling normality test\n#> \n#> data:  bp_clean$systolic\n#> A = 88.39, p-value < 2.2e-16\n```\n\n\n:::\n:::\n\n\n***\n\nWe've got a very tiny p-value so we have to reject the null hypothesis that the systolic blood pressure is normally distributed.\n\n\n::::\n:::::\n\n\n###### DescTools\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-anderson-darling-test-desctools}\n: One-sample t-test: Test of normality using the Anderson-Darling test with the {**DescTools**} package\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDescTools::AndersonDarlingTest(\n    x = bp_clean2$systolic, \n    null = \"pnorm\", \n    mean = mean(bp_clean2$systolic), \n    sd = sd(bp_clean2$systolic)\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tAnderson-Darling test of goodness-of-fit\n#> \tNull hypothesis: Normal distribution\n#> \twith parameters mean = 120.539, sd = 18.617\n#> \n#> data:  bp_clean2$systolic\n#> An = 88.39, p-value = 8.397e-08\n```\n\n\n:::\n:::\n\n\n***\nAgain, we have to reject the Null. \n\nThe {**DescTools**} packages has the advantage that it is a general test of goodness-of-fit. Besides normality it could also be used to test other kind of distributions.\n\n::::\n:::::\n\n###### Dependent t-test\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-anderson-darling-dependent-test-nortest-desctools}\n: Dependent-samples t-test: Test of normality using the Anderson-Darling test with {**nortest**} and {**DescTools**} package\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglue::glue(\"Computation with nortest package\")\nnortest::ad.test(bp_clean$diff_syst)\n\nglue::glue(\" \")\nglue::glue(\"###################################################\")\nglue::glue(\"Computation with DescTools package\")\nDescTools::AndersonDarlingTest(\n    x = bp_clean3$diff_syst, \n    null = \"pnorm\", \n    mean = mean(bp_clean3$diff_syst), \n    sd = sd(bp_clean3$diff_syst)\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Computation with nortest package\n#> \n#> \tAnderson-Darling normality test\n#> \n#> data:  bp_clean$diff_syst\n#> A = 69.694, p-value < 2.2e-16\n#> \n#>  \n#> ###################################################\n#> Computation with DescTools package\n#> \n#> \tAnderson-Darling test of goodness-of-fit\n#> \tNull hypothesis: Normal distribution\n#> \twith parameters mean = 0.545, sd = 4.898\n#> \n#> data:  bp_clean3$diff_syst\n#> An = 69.694, p-value = 8.45e-08\n```\n\n\n:::\n:::\n\n\n***\n\nBoth tests agree, even their p-values differ: We have to reject the Null. \n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n### Testing homogeneity of variances\n\n#### Levene’s test\n\n<a class='glossary' title='Homogeneity of variances is equal variances among groups; homogeneity of variance is one of the assumptions tested for independent and dependent t-tests and analysis of variance. (SwR, Glossary)'>Homogeneity of variances</a> is another assumption valid for the independent t-test. The data should be equally spread out in each group. Although we had used the <a class='glossary' title='Welch’s t-test is a variation on the Student’s t-test that does not assume equal variances in group (SwR, Glossary).'>Welch’s</a> version of the t-test, which does not require homogeneity of variances, we will apply Levene’s test anyway. (An additional visual inspection via group-specific <a class='glossary' title='Boxplots are a visual representation of data that shows central tendency (usually the median) and spread (usually the interquartile range) of a numeric variable for one or more groups; boxplots are often used to compare the distribution of a continuous variable across several groups. (SwR, Glossary)'>boxplots</a> would helpful to gain a visual understanding of Levene’s test results.)\n\n:::::{.my-resource}\n:::{.my-resource-header}\nPackages for Levene's test\n:::\n::::{.my-resource-container}\n{**car**}: Companion to Applied Regression\n{**DescTools**}: Tools for Descriptive Statistics\n{**misty**}: Miscellaneous Functions 'T. Yanagida'\n{**rstatix**}: Pipe-Friendly Framework for Basic Statistical Tests\n\n\n\n::: {#tbl-pkgs-downloads-levene-test .cell tbl-cap='Download average numbers of packages with Levene’s test functions'}\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 4 × 4\n#>   package   average from       to        \n#>   <chr>       <dbl> <date>     <date>    \n#> 1 car         13044 2024-03-21 2024-03-27\n#> 2 rstatix      5551 2024-03-21 2024-03-27\n#> 3 DescTools    2099 2024-03-21 2024-03-27\n#> 4 misty         804 2024-03-21 2024-03-27\n```\n\n\n:::\n:::\n\n\nThe {**car**} packages is most of time used for Levene’s test. There is also a specific `tidy()` function in {**broom**} for the results of `car::leveneTest()`.\n::::\n:::::\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-levene-tests}\n: Levene’s test with different packages\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### car\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-levene-test-car}\n: Testing for equal variances for systolic by sex with {**car**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::leveneTest(\n    y = systolic ~ sex, \n    data = bp_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>         Df F value  Pr(>F)  \n#> group    1   3.552 0.05952 .\n#>       7143                  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n***\n\n::: {.callout-tip}\nThe variances of systolic blood pressure for men and women are not statistically significantly different (p = .06), and the independent-samples t-test meets the assumption of homogeneity of variances.\n:::\n::::\n:::::\n\n\n###### DescTools\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-levene-test-desctools}\n: Testing for equal variances for systolic by sex with {**DescTools**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDescTools::LeveneTest(\n    formula = systolic ~ sex, \n    data = bp_clean\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>         Df F value  Pr(>F)  \n#> group    1   3.552 0.05952 .\n#>       7143                  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n***\nExactyl the same result as with `car::leveneTest()`.\n\n::::\n:::::\n\n###### rstatix\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-levene-test-rstatix}\n: Testing for equal variances for systolic by sex with {**rstatix**}\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nrstatix::levene_test(\n    formula = systolic ~ sex,\n    data = bp_clean\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 4\n#>     df1   df2 statistic      p\n#>   <int> <int>     <dbl>  <dbl>\n#> 1     1  7143      3.55 0.0595\n```\n\n\n:::\n:::\n\n***\n\nAgain exact the same result but this time it returns a tibble which is easier to work with.\n::::\n:::::\n\n###### misty\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-levene-test-misty}\n: Testing for equal variances for systolic by sex with {**misty**}\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nmisty::test.levene(\n    formula = systolic ~ sex,\n    data = bp_clean,\n    plot = TRUE\n)\n```\n\n::: {.cell-output-display}\n![](06-t-test_files/figure-html/levene-test-misty-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Levene's Test based on the Median\n#> \n#>   Null hypothesis        H0: sigma2.1 = sigma2.2\n#>   Alternative hypothesis H1: sigma2.1 != sigma2.2\n#> \n#>   Group      n nNA      M    SD    Var    Low    Upp\n#>    Male   3498   0 122.18 18.15 329.30 307.52 353.01\n#>    Female 3647   0 118.97 18.93 358.23 335.99 382.36\n#> \n#>               Df     Sum Sq Mean Sq    F  pval\n#>   Group        1     558.73  558.73 3.55 0.060\n#>   Residuals 7143 1123605.77  157.30\n```\n\n\n:::\n:::\n\n\n***\n\nThis is most detailed computation! The function has 35 arguments, most of them are dedicated to the appearance of the plot.\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n## Alternate tests when assumptions fail {#sec-chap06-achievement7}\n\n### Introduction\n\nThe systolic data fails the assumption of normality for *all* different kind of t-tests. This means that we cannot rely resp. apply the t-test as we had done in this chapter! Practically we would need to start with our analysis anew. \n\nThese are our alternatives:\n\n- One-sample t-test: <a class='glossary' title='Sign-test is a a statistical test that compares the median of a variable to a hypothesized or population value; used in lieu of the one-sample t-test when the t-test assumptions are not met. (SwR, Glossary)'>Sign-test</a>\n- Dependent-samples t-test: Wilcoxon Signed-Rank Test \n- Independent-samples t-test: Mann-Whitney U or Kolmogorov-Smirnoff\n\n### Sign test (one-sample)\n\nIf the normality assumption of the one-sample t-test fails, the median could be examined rather than the mean --- just like for descriptive statistics when the variable is not normally distributed.\n\nThe median for the systolic distribution is 118. We will compare this value to our hypothetical population median of 120 mmHG. So our null hypothesis is, that the median of 118 is coming from a population with a median of 120 mmHG systolic blood pressure.\n\n:::::{.my-resource}\n:::{.my-resource-header}\nPackages with function for the sign test\n:::\n::::{.my-resource-container}\n- {**BSDA**}: Basic Statistics and Data Analysis \n- {**DescTools**}: Tools for Descriptive Statistics\n- {**rstatix**}: Pipe-Friendly Framework for Basic Statistical Tests\n\n\n::: {#tbl-pkgs-downloads-sign-test .cell tbl-cap='Download average numbers of packages with sign-test functions'}\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 3 × 4\n#>   package   average from       to        \n#>   <chr>       <dbl> <date>     <date>    \n#> 1 rstatix      5551 2024-03-21 2024-03-27\n#> 2 DescTools    2099 2024-03-21 2024-03-27\n#> 3 BSDA          625 2024-03-21 2024-03-27\n```\n\n\n:::\n:::\n\nGenerally I prefer packages where I am using several functions *and* with a high number of downloads. This is the case for {rstatix} and {**DescTools**}. Additionally I will also test the {**BSDA**} recommendation of the book.\n\n::::\n:::::\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-sign-tests}\n: Applying sign test to the systolic blood pressure of NHANES participants\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### BSDA\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-systolic-sign-test-bsda}\n: Compare observed median of systolic blood pressure with 120 mmHG using {**BSDA**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBSDA::SIGN.test(\n    x = bp_clean$systolic,\n    md = 120\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tOne-sample Sign-Test\n#> \n#> data:  bp_clean$systolic\n#> s = 3004, p-value < 2.2e-16\n#> alternative hypothesis: true median is not equal to 120\n#> 95 percent confidence interval:\n#>  116 118\n#> sample estimates:\n#> median of x \n#>         118 \n#> \n#> Achieved and Interpolated Confidence Intervals: \n#> \n#>                   Conf.Level L.E.pt U.E.pt\n#> Lower Achieved CI     0.9477    116    118\n#> Interpolated CI       0.9500    116    118\n#> Upper Achieved CI     0.9505    116    118\n```\n\n\n:::\n:::\n\n\nWe have reason to reject the null hypothesis, e.g. our observed median is not coming from a population median of 120 mmHG. Our sample came likely from a population where the median systolic blood pressure was between 116 and 118 mmHG.\n\n::: {.callout-tip}\nThe median systolic blood pressure for NHANES participants was 118 mmHG. A sign test comparing the median to a hypothesized median of 120 mmHG had a statistically significant (s = 3004; p < .05) result. The sample with a median systolic blood pressure of 118 was unlikely to have come from a population with a median systolic blood pressure of 120. The 95% confidence interval indicates this sample likely came from a population where the median systolic blood pressure was between 116 and 118. This suggests that the median systolic blood pressure in the U.S. population is between 116 and 118.\n:::\n::::\n:::::\n\n\n###### DescTool\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-systolic-sign-test-desctools}\n: Compare observed median of systolic blood pressure with 120 mmHG using {**DescTools**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDescTools::SignTest(\n    x = bp_clean$systolic,\n    mu = 120\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tOne-sample Sign-Test\n#> \n#> data:  bp_clean$systolic\n#> S = 3004, number of differences = 6844, p-value < 2.2e-16\n#> alternative hypothesis: true median is not equal to 120\n#> 95 percent confidence interval:\n#>  116 118\n#> sample estimates:\n#> median of the differences \n#>                       118\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-systolic-sign-test-rstatix}\n: Compare observed median of systolic blood pressure with 120 mmHG using {**rstatix**}\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nrstatix::sign_test(\n    data = bp_clean,\n    formula = systolic ~ 1,\n    mu = 120,\n    detailed = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 12\n#>   estimate .y.   group1 group2     n statistic        p    df conf.low conf.high\n#> *    <dbl> <chr> <chr>  <chr>  <int>     <dbl>    <dbl> <dbl>    <dbl>     <dbl>\n#> 1      118 syst… 1      null …  7145      3004 5.21e-24  6844      116       118\n#> # ℹ 2 more variables: method <chr>, alternative <chr>\n```\n\n\n:::\n:::\n\n\n***\n\nAs always {**rstatix**} returns a tibble and has all the details. It is most of the time my favorite package for tests.  \n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n### Wilcoxon signed-ranks test (dependent-samples)\n\n#### Introduction\n\nThe Wilcoxon signed-ranks test is an alternative to the dependent-samples t-test when the continuous variable is not normally distributed. The Wilcoxon test determines if the differences between paired values of two related samples are symmetrical around zero. That is, instead of comparing the mean difference to zero, the test compares the distribution of the differences around zero.\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-chap06-wilcoxon-signed-ranks-test}\n: Wilcoxon signed-ranks test\n::::::\n:::\n::::{.my-procedure-container}\n- **Step 1**: Find the differences between the two paired measures (Measure 1 – Measure 2)\n- **Step 2**: Put the *absolute values* of the differences in order from smallest to largest and give each one a rank \n- **Step 3**: Sum the ranks for all the *positive* differences \n- **Step 4**: Sum the ranks for all the *negative* differences\n::::\n:::::\n\nSome confusing details:\n\n- The test statistic for the Wilcoxon test is usually `W`, although it was sometimes reported as `T` and called the Wilcoxon T-test. \n    - If the sum of the ranks of all the *positive* differences is smaller, that sum is `W`.\n    - If the sum of the ranks of the *negative* values is smaller, that sum is `W`.\n- The distribution of `W` is approximately normal when the sample size is more than 20.\n- Because it approximates a normal distribution, a <a class='glossary' title='A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (StatisticsHowTo)'>z-statistic</a> is used to test whether the `W` is statistically significant.\n- But the `stats::wilcox.test()` function shows neither `T` nor `W` bit `V` in its output. This is the sum of the *ranks of positive differences*. `V` would be the same as `W` when the sum of the ranks of positive differences was highest, but different from `W` when the sum of the ranks for negative differences was highest. In practical terms this difference is not important as I will take information for the decision (to reject or not to rejet the Null) from the <a class='glossary' title='The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary)'>p-value</a>.\n\n\n#### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: {.callout-note}\n- **H0**: The distribution of the difference between the systolic blood pressure measures taken at Time 1 and Time 2 in the U.S. population is symmetric around zero.\n- **HA**: The distribution of the difference between the systolic blood pressure measures taken at Time 1 and Time 2 in the U.S. population is not symmetric around zero.\n:::\n\n#### NHST Step 2\n\nCompute the test statistic. \n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-compute-wilcoxon-test}\n: Compute Wilcoxon test with different packages \n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### stats\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-compute-wilcoxon-test-stats}\n: Compute Wilcoxon test with `stats::wilcox.test()` \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::wilcox.test(\n    x = bp_clean$systolic, \n    y = bp_clean$systolic2 , \n    conf.int = TRUE,\n    paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tWilcoxon signed rank test with continuity correction\n#> \n#> data:  bp_clean$systolic and bp_clean$systolic2\n#> V = 9549959, p-value < 2.2e-16\n#> alternative hypothesis: true location shift is not equal to 0\n#> 95 percent confidence interval:\n#>  6.335319e-05 9.999790e-01\n#> sample estimates:\n#> (pseudo)median \n#>   2.923076e-05\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n###### rstatix\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-compute-wilcoxon-test-rstatix}\n: Compute Wilcoxon test with `rstatix::pairwise_wilcox_test()` \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {#tbl-compute-wilcoxon-test-rstatix .cell tbl-cap='Compute Wilcoxon test to judge the distribution of the difference between the systolic blood pressure measures taken at time 1 (`systolic`) and time 2 (`systolic2`)'}\n\n```{.r .cell-code}\nwilcox_rstatix <- bp_clean |> \n    dplyr::select(systolic, systolic2) |> \n    tidyr::drop_na() |> \n    tidyr::pivot_longer(\n        cols = c(\"systolic\", \"systolic2\"), \n        names_to = \"treatment\", \n        values_to = \"value\") |> \n    rstatix::pairwise_wilcox_test(\n        formula = value ~ treatment,\n        paired = TRUE,\n        detailed = TRUE\n    )\n\nknitr::kable(wilcox_rstatix)\n```\n\n::: {.cell-output-display}\n\n\n| estimate|.y.   |group1   |group2    |   n1|   n2| statistic|  p| conf.low| conf.high|method   |alternative | p.adj|p.adj.signif |\n|--------:|:-----|:--------|:---------|----:|----:|---------:|--:|--------:|---------:|:--------|:-----------|-----:|:------------|\n| 2.92e-05|value |systolic |systolic2 | 7101| 7101|   9549959|  0| 6.34e-05|  0.999979|Wilcoxon |two.sided   |     0|****         |\n\n\n:::\n:::\n\n\n***\n\nSimilar as in @exm-chap06-cohens-d-dependent-samples we had to apply `tidyr::pivot_longer()` to get the data into the right shape.\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n\n#### NHST Step 3\n\nReview and interpret the test statistics: \nCalculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\nThe p-value is well below .05.\n\n#### NHST Step 4\n\nConclude and write report.\n\nWe have to reject the NULL and assume the alternative: The distribution of the difference between the systolic blood pressure measures taken at Time 1 and Time 2 in the U.S. population is not symmetric around zero.\n\n::: {.callout-tip}\nWe used a <a class='glossary' title='Wilcoxon signed-ranks test is an alternative to the dependent-samples t-test when the continuous variable is not normally distributed; it uses ranks to determine whether the values of a numeric variable are different across two related groups. (SwR, Glossary)'>Wilcoxon signed-ranks test</a> to determine whether the distribution of the difference in systolic blood pressure measured at Time 1 and Time 2 was symmetrical around zero. The resulting test statistic and p-value indicated that the sample likely came from a population where the differences were not symmetrical around zero (p < .05). That is, we found a significant difference between the first and second blood pressure measures.\n:::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Interpreting the results as comparing medians can be misleading \n:::\n::::{.my-watch-out-container}\n\n- Wilcoxon signed-ranks test,\n- Mann-Whitney U test and\n- Kolmogorov-Smirnov test \n\nare often interpreted as testing for *equal medians*. \n\n> While none of these tests examine medians directly, the ordering and ranking of values is similar to how medians are identified, so there is logic to this interpretation. However, if the distribution shape or spread (or both) are different, interpreting the results as comparing medians can be misleading.\n\n:::::{.my-important}\n:::{.my-important-header}\nConduct visual and descriptive analyses before (or with) these tests to make sure you interpret the results accurately.\n:::\n:::::\n\n::::\n:::::\n\n\n### Mann-Whitney U test (independent-samples)\n\n#### Introduction\n\n<a class='glossary' title='Mann-Whitney U test, also called Wilcoxon rank sum test, is an alternative for comparing a numeric or ordinal variable across two groups when the independent-samples t-test assumption of normality is not met. (SwR, Glossary)'>Mann-Whitney U test</a> is used when the t-test assumption of normality for independent group is not met. It is considered to be the non-parametric equivalent to the two-sample independent t-test. The `U` test also relaxes the variable type assumption and can be used for ordinal variables in addition to continuous variables. It works similar as the <a class='glossary' title='Wilcoxon signed-ranks test is an alternative to the dependent-samples t-test when the continuous variable is not normally distributed; it uses ranks to determine whether the values of a numeric variable are different across two related groups. (SwR, Glossary)'>Wilcoxon signed-ranks test</a>:\n\n- It puts puts the values for the continuous (or ordinal) variable in order.\n- It assigns each value a rank.\n- It compares ranks across the two groups of the categorical variable.\n- It The computes the test statistic using the sums of the ranks for each group.\n- It approximates normality as long as the sample size is greater than 20.\n- It uses a z-score to determine the corresponding p-value.\n\n\n#### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: {.callout-note}\n- **H0**: There is no difference in ranked systolic blood pressure values for males and females in the U.S. population.\n- **HA**: There is a difference in ranked systolic blood pressure values for males and females in the U.S. population.\n:::\n\n#### NHST Step 2\n\nCompute the test statistic. \n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-compute-mann-whitney-u-test}\n: Compute Mann-Whitney U test with different packages \n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### stats\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-compute-mann-whitney-u-test-stats}\n: Compute Mann-Whitney U test with `stats::wilcox.test()` \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmann_whitney_stats <- stats::wilcox.test(\n    formula = bp_clean$systolic ~ bp_clean$sex,\n    conf.int = TRUE,\n    paired = FALSE)\n\nmann_whitney_stats\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tWilcoxon rank sum test with continuity correction\n#> \n#> data:  bp_clean$systolic by bp_clean$sex\n#> W = 7186882, p-value < 2.2e-16\n#> alternative hypothesis: true location shift is not equal to 0\n#> 95 percent confidence interval:\n#>  2.000038 4.000012\n#> sample estimates:\n#> difference in location \n#>                3.99998\n```\n\n\n:::\n:::\n\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! <a class='glossary' title='Mann-Whitney U test, also called Wilcoxon rank sum test, is an alternative for comparing a numeric or ordinal variable across two groups when the independent-samples t-test assumption of normality is not met. (SwR, Glossary)'>Mann-Whitney U test</a> is also called Wilcoxon rank sum test! And this is not the same as <a class='glossary' title='Wilcoxon signed-ranks test is an alternative to the dependent-samples t-test when the continuous variable is not normally distributed; it uses ranks to determine whether the values of a numeric variable are different across two related groups. (SwR, Glossary)'>Wilcoxon signed-ranks test</a>.\n:::\n:::::\n\nFor more information on the different Wilcoxon tests read [Wilcoxon Test in R](https://www.datanovia.com/en/lessons/wilcoxon-test-in-r/) [@kassambaran.d].\n\n\n::::\n:::::\n\n\n###### rstatix\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-compute-mann-whitney-u-test-rstatix}\n: Compute Mann-Whitney U test with `rstatix::pairwise_wilcox_test()` \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {#tbl-compute-mann-whitney-u-test-rstatix .cell tbl-cap='Compute Mann-Whitney U test comparing the systolic blood pressure for male and females'}\n\n```{.r .cell-code}\nwilcox_rstatix <- bp_clean |> \n    dplyr::select(systolic, sex) |> \n    tidyr::drop_na() |> \n    rstatix::pairwise_wilcox_test(\n        formula = systolic ~ sex,\n        paired = FALSE,\n        detailed = TRUE\n    )\n\nknitr::kable(wilcox_rstatix)\n```\n\n::: {.cell-output-display}\n\n\n| estimate|.y.      |group1 |group2 |   n1|   n2| statistic|  p| conf.low| conf.high|method   |alternative | p.adj|p.adj.signif |\n|--------:|:--------|:------|:------|----:|----:|---------:|--:|--------:|---------:|:--------|:-----------|-----:|:------------|\n|  3.99998|systolic |Male   |Female | 3498| 3647|   7186882|  0| 2.000038|  4.000012|Wilcoxon |two.sided   |     0|****         |\n\n\n:::\n:::\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n##### Effect size for Mann-Whitney U\n\n$$\nr = \\frac{z}{\\sqrt{n}}\n$$ {#eq-chap06-effect-size-mann-whitney-u-test}\n\nThe Pearson correlation coefficient `r` is calculated as <a class='glossary' title='A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (StatisticsHowTo)'>z-score</a> divided by square root of the total observations.\n\n:::::{.my-assessment}\n:::{.my-assessment-header}\n:::::: {#cor-chap06-effect-size-mann-whitney-u-test}\n: Effect size `r` of the Mann-Whitney U test\n::::::\n:::\n::::{.my-assessment-container}\n- **Small**: r = .1 to r < .3 \n- **Medium**: r = .3 to r < .5\n- **Large**: r ≥ .5\n::::\n:::::\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap06-effect-size-mann-whitney-u-test}\n: Compute effect size for the Mann-Whitney U test\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Manually\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-effect-size-mann-whitney-u-test-manually}\n: Compute effect size for the Mann-Whitney U test manually\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::qnorm(mann_whitney_stats$p.value) / \n    base::sqrt(base::nrow(bp_clean2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] -0.108912\n```\n\n\n:::\n:::\n\nWe will interpret only the absolute value of the effect size because the direction is not of interest, just the size of the effect. An `r` with 0.11 is a small effect size.\n::::\n:::::\n\n\n###### rcompanion\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-effect-size-mann-whitney-u-test-rcompanion}\n: Compute effect size for the Mann-Whitney U test with {**rcompanion**}\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean4 <- bp_clean |> \n    tidyr::drop_na(systolic, sex)\n\nrcompanion::wilcoxonR(\n    x = bp_clean4$systolic, \n    g = bp_clean4$sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    r \n#> 0.11\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n#### NHST Step 3\n\nReview and interpret the test statistics: \nCalculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\nAs the p-value < 2.2e-16 with a small effect size of r = 0.11 is statistically significant, we reject the Null.\n\n#### NHST Step 4\n\nConclude and write report.\n\n::: {.callout-tip}\nA Mann-Whitney U test comparing systolic blood pressure for males and females in the United States found a statistically significant difference between the two groups (p < .05). Histograms demonstrated the differences, with notably more females with systolic blood pressure below 100 compared to males along with some other differences. The effect size was small, r = .11, indicating a weak but statistically significant relationship between sex and systolic blood pressure.\n:::\n\n### Kolmogorov-Smirnov test: Variance assumption failed\n\n#### Introduction\n\nAll the tests under the heading \"Alternate tests\" (@sec-chap06-achievement7) discussed so far are alternatives for a failed normality assumption. Kolmogorov-Smirnov test is an alternative when the assumption of equal variances (homogeneity of variances) has failed. \n\nWhen the variances are unequal, the homogeneity of variances assumption is not met, whether or not the normality assumption is met. The larger variance has a bigger influence on the size of the t-statistic, so one group is dominating the t-statistic calculations.\n\nAlthough we have used the <a class='glossary' title='Welch’s t-test is a variation on the Student’s t-test that does not assume equal variances in group (SwR, Glossary).'>Welch’s t-test</a> the <a class='glossary' title='The Kolmogorov-Smirnov test is used when the assumption of equal variances (homogeneity of variances) fails for the independent-samples t-test; the test compares the distributions of the groups rather than their means. (SwR, Glossary)'>Kolmogorov-Smirnov test</a> is an alternative test when both (the normality *and* the variance assumption) have failed.\n\n\n#### NHST Step 1\n\nWrite the null and alternate hypotheses:\n\n::: {.callout-note}\n- **H0**: The distribution of systolic blood pressure for males and females is the same in the U.S. population.\n- **HA**: The distribution of systolic blood pressure for males and females is not the same in the U.S. population.\n:::\n\n#### NHST Step 2\n\nCompute the test statistic. \n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-compute-ks-test-stats}\n: Compute the Kolmogorov-Smirnov test with {**stats**}\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmale_sys <- bp_clean |> \n    dplyr::filter(sex == \"Male\") |> \n    dplyr::pull(var = systolic)\n\nfemale_sys <- bp_clean |> \n    dplyr::filter(sex == \"Female\") |> \n    dplyr::pull(var = systolic)\n    \nstats::ks.test(\n    x = male_sys,\n    y = female_sys\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning in ks.test.default(x = male_sys, y = female_sys): p-value will be\n#> approximate in the presence of ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tAsymptotic two-sample Kolmogorov-Smirnov test\n#> \n#> data:  male_sys and female_sys\n#> D = 0.11408, p-value < 2.2e-16\n#> alternative hypothesis: two-sided\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\nMost of the examples in the Internet uses the base R version with `stats::ks.test()` for the Kolmogorov-Smirnov test. I have found with {**FSA**} and {**dgof**} two other packages with this test, but as they do not differ with their input parameters nor with their results I will stick with the {**stats**} function.\n\n\n#### NHST Step 3\n\nReview and interpret the test statistics: \nCalculate the probability that your test statistic is at least as big as it is if there is no relationship (i.e., the null is true).\n\nThe p-value is well below .05.\n\n#### NHST Step 4\n\nConclude and write report.\n\nThe test statistic, D, is the maximum distance between the two empirical cumulative distribution functions (<a class='glossary' title='In statistics, an empirical distribution function (commonly also called an empirical cumulative distribution function, eCDF) is the distribution function associated with the empirical measure of a sample. This cumulative distribution function is a step function that jumps up by 1/n at each of the n data points. Its value at any specified value of the measured variable is the fraction of observations of the measured variable that are less than or equal to the specified value. (Wikipedia) A CDF is a hypothetical model of a distribution, the ECDF models empirical (i.e. observed) data. (Statistics How To)'>ECDFs</a>). ECDFs are a special type of probability distribution showing the cumulative probability of the values of a variable. We can examine the difference between the ECDFs for systolic blood pressure of males and females in the sample with the following graph:\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap06-ecdf}\n: Compute empirical cumulative distributions (ECDFs)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\nbp_clean |> \n    \n    ggplot2::ggplot(\n        ggplot2::aes(\n            x = systolic,\n            color = sex\n        )\n    ) +\n    ggplot2::stat_ecdf(\n        geom = \"step\",\n        linewidth = 1,\n        na.rm = TRUE\n        ) +\n    ggplot2::labs(\n        x = \"Systolic blood pressure (mmHg)\", \n        y = \"Cumulative probability\"\n        ) + \n    ggplot2::scale_color_manual(\n        values = c(\n            \"Male\" = \"gray\", \n            \"Female\" = \"purple3\"\n            )\n    )\n```\n\n::: {.cell-output-display}\n![ECDF of systolic blood pressure in mmHg by sex for 20152016 NHANES participants](06-t-test_files/figure-html/fig-ecdf-1.png){#fig-ecdf width=672}\n:::\n:::\n\n***\n\nAt the widest gap between these two curves, males and females were $.11$ apart, giving a test statistic of $D = .11$.\n\n::: {.callout-tip}\nA K-S test comparing systolic blood pressure for males and females found a statistically significant difference between the two groups (D = .11; p < .05). This sample likely came from a population where the distribution of systolic blood pressure was different for males and females.\n:::\n::::\n:::::\n\n\n### Miscellaneous\n\n:::::{.my-resource}\n:::{.my-resource-header}\nMiscellaneous packages for (possible) alternative tests\n:::\n::::{.my-resource-container}\nDuring my research for packages with functions for alternative test if the t-test assumption fail I came about several very interesting packages. Some of them work together with {**ggplot2**}, so that these statistics can be plotted easily as well.\n\n- {**rcompanion**}: I have outlined the table of content to demonstrate that this package contains many different tests. (See @pak-rcompanion)\n- {**ggstats**}: Provides new statistics, new geometries and new positions for \n    {**ggplot2**} and a suite of functions to facilitate the creation of \n    statistical plots.\n- {**ggstatsplot**}: As an extension of {**ggplot2**} the packages creates graphics with details from statistical tests included in the plots themselves.\n- {**statsExpressions**}: Tidy Data Frames and Expressions with Statistical Details. Utilities for producing data frames with rich details for the most common types of statistical approaches and tests.\n\n\n::: {#tbl-pkgs-dl-misc-tests .cell tbl-cap='Download average numbers of miscellanous packages with test functions'}\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 4 × 4\n#>   package          average from       to        \n#>   <chr>              <dbl> <date>     <date>    \n#> 1 ggstats             3191 2024-03-21 2024-03-27\n#> 2 rcompanion           768 2024-03-21 2024-03-27\n#> 3 statsExpressions     547 2024-03-21 2024-03-27\n#> 4 ggstatsplot          380 2024-03-21 2024-03-27\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n\n\n## Exercises (empty)\n\n\n\n## Glossary\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Anderson-Darling </td>\n   <td style=\"text-align:left;\"> The Anderson-Darling Goodness of Fit Test (AD-Test) is a measure of how well your data fits a specified distribution. It’s commonly used as a test for normality. (&lt;a href=\"https://www.statisticshowto.com/anderson-darling-test/\"&gt;Statistics How-To&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Boxplots </td>\n   <td style=\"text-align:left;\"> Boxplots are a visual representation of data that shows central tendency (usually the median) and spread (usually the interquartile range) of a numeric variable for one or more groups; boxplots are often used to compare the distribution of a continuous variable across several groups. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Cohen’s d </td>\n   <td style=\"text-align:left;\"> Cohen’s d is a standardized effect size for measuring the difference between two group means. It is frequently used to compare a treatment to a control group. It can be a suitable effect size to include with t-test and ANOVA results. (&lt;a href= \"https://statisticsbyjim.com/basics/cohens-d/\"&gt;Statistics by Jim&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Degrees of Freedom </td>\n   <td style=\"text-align:left;\"> Degree of Freedom (df) is the number of pieces of information that are allowed to vary in computing a statistic before the remaining pieces of information are known; degrees of freedom are often used as parameters for distributions (e.g., chi-squared, F). (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ECDF </td>\n   <td style=\"text-align:left;\"> In statistics, an empirical distribution function (commonly also called an empirical cumulative distribution function, eCDF) is the distribution function associated with the empirical measure of a sample. This cumulative distribution function is a step function that jumps up by 1/n at each of the n data points. Its value at any specified value of the measured variable is the fraction of observations of the measured variable that are less than or equal to the specified value. (&lt;a href=\"https://en.wikipedia.org/wiki/Empirical_distribution_function\"&gt;Wikipedia&lt;/a&gt;) A CDF is a hypothetical model of a distribution, the ECDF models empirical (i.e. observed) data. (&lt;a href=\"https://www.statisticshowto.com/empirical-distribution-function/\"&gt;Statistics How To&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Effect Size </td>\n   <td style=\"text-align:left;\"> Effect size is a measure of the strength of a relationship; effect sizes are important in inferential statistics in order to determine and communicate whether a statistically significant result has practical importance. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Homogeneity of Variances </td>\n   <td style=\"text-align:left;\"> Homogeneity of variances is equal variances among groups; homogeneity of variance is one of the assumptions tested for independent and dependent t-tests and analysis of variance. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Independent </td>\n   <td style=\"text-align:left;\"> Independent-samples t-test or unpaired sample t-test is an inferential test comparing two independent means. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Kolmogorov-Smirnov </td>\n   <td style=\"text-align:left;\"> The Kolmogorov-Smirnov test is used when the assumption of equal variances (homogeneity of variances) fails for the independent-samples t-test; the test compares the distributions of the groups rather than their means. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Kurtosis </td>\n   <td style=\"text-align:left;\"> Kurtosis is a measure of how many observations are in the tails of a distribution; distributions that look bell-shaped, but have a lot of observations in the tails (platykurtic) or very few observations in the tails (leptokurtic) (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Leptokurtic </td>\n   <td style=\"text-align:left;\"> Leptokurtic is a distribution of a numeric variable that has many values clustered around the middle of the distribution; leptokurtic distributions often appear tall and pointy compared to mesokurtic or platykurtic distributions. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Mann-Whitney </td>\n   <td style=\"text-align:left;\"> Mann-Whitney U test, also called Wilcoxon rank sum test, is an alternative for comparing a numeric or ordinal variable across two groups when the independent-samples t-test assumption of normality is not met. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Omnibus </td>\n   <td style=\"text-align:left;\"> An omnibus is a statistical test that identifies that there is some relationship going on between variables, but not what that relationship is. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> One-sample </td>\n   <td style=\"text-align:left;\"> One-sample t-test, also known as the single-parameter t-test or single-sample t-test, is an inferential statistical test comparing the mean of a numeric variable to a population or hypothesized mean. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Outcome </td>\n   <td style=\"text-align:left;\"> Outcome is the variable being explained or predicted by a model; in linear and logistic regression, the outcome variable is on the left-hand side of the equal sign. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p-value </td>\n   <td style=\"text-align:left;\"> The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Paired </td>\n   <td style=\"text-align:left;\"> Dependent-samples test or paired-samples t-test is an inferential test comparing two related means . (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Platykurtic </td>\n   <td style=\"text-align:left;\"> Platykurtic is a distribution of a numeric variable that has more observations in the tails than a normal distribution would have; platykurtic distributions often look flatter than a normal distribution. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Pooled Variance </td>\n   <td style=\"text-align:left;\"> Pooled variance is the assumption that the variances in two groups are equal, so these variances are combined ('pooled') (SwR, Glossary). </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Predictor Variable </td>\n   <td style=\"text-align:left;\"> Predictor variable -- also known sometimes as the independent or explanatory variable -- is the counterpart to the response or dependent variable. Predictor variables are used to make predictions for dependent variables. ([DeepAI](https://deepai.org/machine-learning-glossary-and-terms/predictor-variable), [MiniTab](https://support.minitab.com/en-us/minitab/21/help-and-how-to/statistical-modeling/regression/supporting-topics/basics/what-are-response-and-predictor-variables/)) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Q-Q-Plot </td>\n   <td style=\"text-align:left;\"> A quantile-quantile plot is a visualization of data using probabilities to show how closely a variable follows a normal distribution. (SwR, Glossary) This plot is made up of points below which a certain percentage of the observations fall. On the x-axis are normally distributed values with a mean of 0 and a standard deviation of 1. On the y-axis are the observations from the data. If the data are normally distributed, the values will form a diagonal line through the graph. (SwR, chapter 6) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Quantile </td>\n   <td style=\"text-align:left;\"> Quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities (&lt;a href=\"https://en.wikipedia.org/wiki/Quantile\"&gt;Wikipedia&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Rejection Region </td>\n   <td style=\"text-align:left;\"> Rejection region is the area under the curve of a sampling distribution where the probability of obtaining a value is very small, often below 5%; the rejection region is in the end of the tail or tails of the distribution. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Shapiro-Wilk </td>\n   <td style=\"text-align:left;\"> The Shapiro-Wilk test is a statistical test to determine or confirm whether a variable has a normal distribution; it is sensitive to small deviations from normality and not useful for sample sizes above 5,000 because it will nearly always find non-normality. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Sign-Test </td>\n   <td style=\"text-align:left;\"> Sign-test is a a statistical test that compares the median of a variable to a hypothesized or population value; used in lieu of the one-sample t-test when the t-test assumptions are not met. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Skewness </td>\n   <td style=\"text-align:left;\"> Skewness is the extent to which a variable has extreme values in one of the two tails of its distribution (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standard Deviation </td>\n   <td style=\"text-align:left;\"> The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter $\\sigma$ (sigma), for the population standard deviation, or the Latin letter $s$ for the sample standard deviation. ([Wikipedia] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Standard Error </td>\n   <td style=\"text-align:left;\"> The standard error (SE) of a statistic is the standard deviation of its [sampling distribution]. If the statistic is the sample mean, it is called the standard error of the mean (SEM). (&lt;a href=\"https://en.wikipedia.org/wiki/Standard_error\"&gt;Wikipedia&lt;/a&gt;) The standard error is a measure of variability that estimates how much variability there is in a population based on the variability in the sample and the size of the sample. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> T-Statistic </td>\n   <td style=\"text-align:left;\"> The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (&lt;a href=\"https://www.statisticshowto.com/t-statistic/\"&gt;Statistics How-To&lt;/a&gt;) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Welch-T </td>\n   <td style=\"text-align:left;\"> Welch’s t-test is a variation on the Student’s t-test that does not assume equal variances in group (SwR, Glossary). </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Wilcoxon </td>\n   <td style=\"text-align:left;\"> Wilcoxon signed-ranks test is an alternative to the dependent-samples t-test when the continuous variable is not normally distributed; it uses ranks to determine whether the values of a numeric variable are different across two related groups. (SwR, Glossary) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Z-score </td>\n   <td style=\"text-align:left;\"> A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (&lt;a href=\"https://www.statisticshowto.com/probability-and-statistics/z-score/#Whatisazscore\"&gt;StatisticsHowTo&lt;/a&gt;) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n## Session Info {.unnumbered}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\nSession Info\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.3.3 (2024-02-29)\n#>  os       macOS Sonoma 14.4.1\n#>  system   x86_64, darwin20\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       Europe/Vienna\n#>  date     2024-04-07\n#>  pandoc   3.1.12.3 @ /usr/local/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package      * version    date (UTC) lib source\n#>  abind          1.4-5      2016-07-21 [1] CRAN (R 4.3.0)\n#>  backports      1.4.1      2021-12-13 [1] CRAN (R 4.3.0)\n#>  base64enc      0.1-3      2015-07-28 [1] CRAN (R 4.3.0)\n#>  bayestestR     0.13.2     2024-02-12 [1] CRAN (R 4.3.2)\n#>  boot           1.3-30     2024-02-26 [2] CRAN (R 4.3.2)\n#>  broom          1.0.5      2023-06-09 [1] CRAN (R 4.3.0)\n#>  BSDA           1.2.2      2023-09-18 [1] CRAN (R 4.3.0)\n#>  car            3.1-2      2023-03-30 [1] CRAN (R 4.3.0)\n#>  carData        3.0-5      2022-01-06 [1] CRAN (R 4.3.0)\n#>  cellranger     1.1.0      2016-07-27 [1] CRAN (R 4.3.0)\n#>  chromote       0.2.0      2024-02-12 [1] CRAN (R 4.3.2)\n#>  class          7.3-22     2023-05-03 [2] CRAN (R 4.3.3)\n#>  cli            3.6.2      2023-12-11 [1] CRAN (R 4.3.0)\n#>  coda           0.19-4.1   2024-01-31 [1] CRAN (R 4.3.2)\n#>  codetools      0.2-20     2024-03-31 [1] CRAN (R 4.3.3)\n#>  coin           1.4-3      2023-09-27 [1] CRAN (R 4.3.0)\n#>  colorspace     2.1-1      2024-01-03 [1] R-Forge (R 4.3.2)\n#>  commonmark     1.9.1      2024-01-30 [1] CRAN (R 4.3.2)\n#>  crayon         1.5.2      2022-09-29 [1] CRAN (R 4.3.0)\n#>  curl           5.2.1      2024-03-01 [1] CRAN (R 4.3.2)\n#>  data.table     1.15.4     2024-03-30 [1] CRAN (R 4.3.2)\n#>  datawizard     0.10.0     2024-03-26 [1] CRAN (R 4.3.2)\n#>  DescTools      0.99.54    2024-02-03 [1] CRAN (R 4.3.2)\n#>  digest         0.6.35     2024-03-11 [1] CRAN (R 4.3.2)\n#>  dplyr          1.1.4      2023-11-17 [1] CRAN (R 4.3.0)\n#>  e1071          1.7-14     2023-12-06 [1] CRAN (R 4.3.0)\n#>  effectsize     0.8.6      2023-09-14 [1] CRAN (R 4.3.0)\n#>  emmeans        1.10.0     2024-01-23 [1] CRAN (R 4.3.2)\n#>  estimability   1.5        2024-02-20 [1] CRAN (R 4.3.2)\n#>  evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.0)\n#>  Exact          3.2        2022-09-25 [1] CRAN (R 4.3.0)\n#>  expm           0.999-9    2024-01-11 [1] CRAN (R 4.3.0)\n#>  fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.0)\n#>  farver         2.1.1      2022-07-06 [1] CRAN (R 4.3.0)\n#>  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#>  foreign        0.8-86     2023-11-28 [1] CRAN (R 4.3.2)\n#>  generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#>  ggplot2        3.5.0      2024-02-23 [1] CRAN (R 4.3.2)\n#>  gld            2.6.6      2022-10-23 [1] CRAN (R 4.3.0)\n#>  glossary     * 1.0.0.9000 2023-08-12 [1] Github (debruine/glossary@819e329)\n#>  glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.0)\n#>  gtable         0.3.4      2023-08-21 [1] CRAN (R 4.3.0)\n#>  here           1.0.1      2020-12-13 [1] CRAN (R 4.3.0)\n#>  highr          0.10       2022-12-22 [1] CRAN (R 4.3.0)\n#>  htmltools      0.5.8      2024-03-25 [1] CRAN (R 4.3.2)\n#>  htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.3.0)\n#>  httr           1.4.7      2023-08-15 [1] CRAN (R 4.3.0)\n#>  insight        0.19.10    2024-03-22 [1] CRAN (R 4.3.3)\n#>  jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.0)\n#>  kableExtra     1.4.0      2024-01-24 [1] CRAN (R 4.3.2)\n#>  knitr          1.45       2023-10-30 [1] CRAN (R 4.3.0)\n#>  labeling       0.4.3      2023-08-29 [1] CRAN (R 4.3.0)\n#>  later          1.3.2      2023-12-06 [1] CRAN (R 4.3.0)\n#>  lattice        0.22-6     2024-03-20 [2] CRAN (R 4.3.2)\n#>  lavaan         0.6-17     2023-12-20 [1] CRAN (R 4.3.0)\n#>  libcoin        1.0-10     2023-09-27 [1] CRAN (R 4.3.0)\n#>  lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.3.0)\n#>  lmom           3.0        2023-08-29 [1] CRAN (R 4.3.0)\n#>  lmtest         0.9-40     2022-03-21 [1] CRAN (R 4.3.0)\n#>  lsr            0.5.2      2021-12-01 [1] CRAN (R 4.3.0)\n#>  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#>  markdown       1.12       2023-12-06 [1] CRAN (R 4.3.0)\n#>  MASS           7.3-60.0.1 2024-01-13 [2] CRAN (R 4.3.3)\n#>  Matrix         1.6-5      2024-01-11 [1] CRAN (R 4.3.0)\n#>  matrixStats    1.2.0      2023-12-11 [1] CRAN (R 4.3.0)\n#>  misty          0.6.2      2024-02-05 [1] CRAN (R 4.3.2)\n#>  mnormt         2.1.1      2022-09-26 [1] CRAN (R 4.3.0)\n#>  modeltools     0.2-23     2020-03-05 [1] CRAN (R 4.3.0)\n#>  moments        0.14.1     2022-05-02 [1] CRAN (R 4.3.0)\n#>  multcomp       1.4-25     2023-06-20 [1] CRAN (R 4.3.0)\n#>  multcompView   0.1-10     2024-03-08 [1] CRAN (R 4.3.2)\n#>  munsell        0.5.0      2018-06-12 [1] CRAN (R 4.3.0)\n#>  mvtnorm        1.2-4      2023-11-27 [1] CRAN (R 4.3.2)\n#>  nhanesA        1.0        2024-01-09 [1] CRAN (R 4.3.0)\n#>  nlme           3.1-164    2023-11-27 [1] CRAN (R 4.3.2)\n#>  nortest        1.0-4      2015-07-30 [1] CRAN (R 4.3.0)\n#>  parameters     0.21.6     2024-03-18 [1] CRAN (R 4.3.2)\n#>  pbivnorm       0.6.0      2015-01-23 [1] CRAN (R 4.3.0)\n#>  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#>  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#>  plyr           1.8.9      2023-10-02 [1] CRAN (R 4.3.0)\n#>  processx       3.8.4      2024-03-16 [1] CRAN (R 4.3.2)\n#>  promises       1.2.1      2023-08-10 [1] CRAN (R 4.3.0)\n#>  proxy          0.4-27     2022-06-09 [1] CRAN (R 4.3.0)\n#>  ps             1.7.6      2024-01-18 [1] CRAN (R 4.3.0)\n#>  psych          2.4.3      2024-03-18 [1] CRAN (R 4.3.2)\n#>  purrr          1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n#>  quadprog       1.5-8      2019-11-20 [1] CRAN (R 4.3.0)\n#>  R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#>  rcompanion     2.4.35     2024-02-17 [1] CRAN (R 4.3.2)\n#>  Rcpp           1.0.12     2024-01-09 [1] CRAN (R 4.3.0)\n#>  readxl         1.4.3      2023-07-06 [1] CRAN (R 4.3.0)\n#>  repr           1.1.7      2024-03-22 [1] CRAN (R 4.3.3)\n#>  rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.0)\n#>  rmarkdown      2.26       2024-03-05 [1] CRAN (R 4.3.2)\n#>  rootSolve      1.8.2.4    2023-09-21 [1] CRAN (R 4.3.1)\n#>  rprojroot      2.0.4      2023-11-05 [1] CRAN (R 4.3.0)\n#>  rstatix        0.7.2      2023-02-01 [1] CRAN (R 4.3.0)\n#>  rstudioapi     0.16.0     2024-03-24 [1] CRAN (R 4.3.2)\n#>  rversions      2.1.2      2022-08-31 [1] CRAN (R 4.3.0)\n#>  rvest          1.0.4      2024-02-12 [1] CRAN (R 4.3.2)\n#>  sandwich       3.1-0      2023-12-11 [1] CRAN (R 4.3.0)\n#>  scales         1.3.0      2023-11-28 [1] CRAN (R 4.3.2)\n#>  selectr        0.4-2      2019-11-20 [1] CRAN (R 4.3.0)\n#>  semTools       0.5-6      2022-05-10 [1] CRAN (R 4.3.0)\n#>  sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n#>  skimr          2.1.5      2022-12-23 [1] CRAN (R 4.3.0)\n#>  stringi        1.8.3      2023-12-11 [1] CRAN (R 4.3.0)\n#>  stringr        1.5.1      2023-11-14 [1] CRAN (R 4.3.0)\n#>  survival       3.5-8      2024-02-14 [2] CRAN (R 4.3.3)\n#>  svglite        2.1.3      2023-12-08 [1] CRAN (R 4.3.0)\n#>  systemfonts    1.0.6      2024-03-07 [1] CRAN (R 4.3.2)\n#>  TH.data        1.1-2      2023-04-17 [1] CRAN (R 4.3.0)\n#>  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#>  tidyr          1.3.1      2024-01-24 [1] CRAN (R 4.3.2)\n#>  tidyselect     1.2.1      2024-03-11 [1] CRAN (R 4.3.2)\n#>  utf8           1.2.4      2023-10-22 [1] CRAN (R 4.3.0)\n#>  vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n#>  viridisLite    0.4.2      2023-05-02 [1] CRAN (R 4.3.0)\n#>  websocket      1.4.1      2021-08-18 [1] CRAN (R 4.3.0)\n#>  withr          3.0.0      2024-01-16 [1] CRAN (R 4.3.0)\n#>  xfun           0.43       2024-03-25 [1] CRAN (R 4.3.2)\n#>  xml2           1.3.6      2023-12-04 [1] CRAN (R 4.3.0)\n#>  xtable         1.8-4      2019-04-21 [1] CRAN (R 4.3.0)\n#>  yaml           2.3.8      2023-12-11 [1] CRAN (R 4.3.0)\n#>  zoo            1.8-12     2023-04-13 [1] CRAN (R 4.3.0)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.3-x86_64/library\n#>  [2] /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n\n\n::::\n:::::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}