# Linear regression {#sec-chap09}

```{r}
#| label: setup
#| include: false

base::source(file = "R/helper.R")
ggplot2::theme_set(ggplot2::theme_bw()) 
```

::: {.callout-note}
As I have already read some books on linear regression I will not follow exactly the text in this chapter. I will leave out those passages that are not new for me and where I feel confident. Other passages I will only summarize to have content to consult whenever I would need it.
:::





## Achievements to unlock

::: my-objectives
::: my-objectives-header
Objectives
:::

::: my-objectives-container
**SwR Achievements**

- **Achievement 1**: Using exploratory data analysis to learn about the data before developing a linear regression model (@sec-chap09-achievement1)
- **Achievement 2**: Exploring the statistical model for a line (@sec-chap09-achievement2)
- **Achievement 3**: Computing the slope and intercept in a simple linear regression (@sec-chap09-achievement3)
- **Achievement 4**: Slope interpretation and significance ($b_{1}$, p-value, CI) (@sec-chap09-achievement4)
- **Achievement 5**: Model significance and model fit (@sec-chap09-achievement5)
- **Achievement 6**: Checking assumptions and conducting diagnostics (@sec-chap09-achievement6)
- **Achievement 7**: Adding variables to the model and using transformation (@sec-chap09-achievement7)

:::
:::


## The needle exchange examination

Some infectious diseases like HIV and Hepatitis C are on the rise again with young people in non-urban areas having the highest increases and needle-sharing being a major factor. Clean needles are distributed by syringe services programs (SSPs), which can also provide a number of other related services including overdose prevention, referrals for substance use treatment, and infectious disease testing. But even there are programs in place --- which is not allowed legally in some US states! --- some people have to travel long distances for health services, especially for services that are specialized, such as needle exchanges.

In discussing the possible quenstion one could analyse it turned out that for some questions critical data are missing: 

- There is a distance-to-syringe-services-program variable among the health services data sources of `r glossary("amfAR")` (https://opioid.amfar.org/). 
- Many of the interesting variables were not available for much of the nation, and many of them were only at the state level.

Given these limitations, the book focuses whether the distance to a syringe program could be explained by 

- whether a county is urban or rural, 
- what percentage of the county residents have insurance (as a measure of both access to health care and socioeconomic status [SES]), 
- HIV prevalence, 
- and the number of people with opioid prescriptions.

As there is no variable for rural or urban status in the amfAR database, the book will tale a variable from the U.S. Department of Agriculture Economic Research Services website (https://www.ers.usda.gov/data-products/county-typology-codes/) that classifies all counties as metro or non-metro.

## Resources & Chapter Outline

### Data, codebook, and R packages {#sec-chap04-data-codebook-packages}

::: my-resource
::: my-resource-header
Data, codebook, and R packages for learning about descriptive statistics
:::

::: my-resource-container

**Data**

Two options for accessing the data:

1. Download the clean data set `dist_ssp_amfar_ch9.csv` from https://edge.sagepub.com/harris1e.
2. Follow the instructions in Box 9.1 to import, merge, and clean the data from multiple files or from the original online source 



**Codebook**

Two options for accessing the codebook: 

1. Download the codebook file `opioid_county_codebook.xlsx` from https://edge.sagepub.com/harris1e.
2. Use the online codebook from the amfAR Opioid & Health Indicators Database website (https://opioid.amfar.org/)


**Packages**

1. Packages used with the book (sorted alphabetically)

-   {**tidyverse**}: @pak-tidyverse (Hadley Wickham)
-   {**tableone**}: @pak-tableone (Kazuki Yoshida) 
-   {**lmtest**}: @pak-lmtest (Achim Zeileis) 
-   {**broom**}, @pak-broom (David Robinson and Alex Hayes)
-   {**car**}, @pak-car (John Fox)

    
2. My additional packages (sorted alphabetically)

- {**gt**}: @pak-gt (Richard Iannone)
- {**gtsummary**}: @pak-gtsummary (Daniel D. Sjoberg)

:::
:::


### Get, recode and show data

I will use the data file provided by the book because I am feeling quite confident with reading and recoding the original data. But I will change the columns names so that the variable names conform to the [tidyverse style guide](https://style.tidyverse.org/).

:::::{.my-example}
:::{.my-example-header}
:::::: {#exm-chap09-data}
: Data for chapter 9
::::::
:::
::::{.my-example-container}

::: {.panel-tabset}

###### Get & recode

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap09-get-recode-data}
: Get & recode data for chapter 9
::::::
:::
::::{.my-r-code-container}
```{r}
#| label: get-recode-data
#| eval: FALSE

## run only once (manually)
distance_ssp <- readr::read_csv(
    "data/chap09/dist_ssp_amfar_ch9.csv",
    show_col_types = FALSE)

distance_ssp_clean <- distance_ssp |> 
    dplyr::rename(
        state = "STATEABBREVIATION",
        dist_ssp = "dist_SSP",
        hiv_prevalence = "HIVprevalence",
        opioid_rate = "opioid_RxRate",
        no_insurance = "pctunins"
    ) |> 
    dplyr::mutate(
        state = forcats::as_factor(state),
        metro = forcats::as_factor(metro)
    ) |> 
    dplyr::mutate(
        hiv_prevalence = dplyr::na_if(
            x = hiv_prevalence,
            y = -1
        )
    )

save_data_file("chap09", distance_ssp_clean, "distance_ssp_clean.rds")
    

```

(*For this R code chunk is no output available*)

::::
:::::


###### Show data

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap09-show-data}
: Show data for chapter 9
::::::
:::
::::{.my-r-code-container}

```{r}
#| label: tbl-chap09-show-data
#| tbl-cap: "Descriptive statistics for data of chapter 9"

distance_ssp_clean <- base::readRDS("data/chap09/distance_ssp_clean.rds")

skimr::skim(distance_ssp_clean)
```

::::
:::::

- **county**: the county name 
- **state**: the two-letter abbreviation for the state the county is in 
- **dist_ssp**: distance in miles to the nearest syringe services program 
- **hiv_prevalence**: people age 13 and older living with diagnosed HIV per 100,000
- **opioid_rate**: number of opioid prescriptions per 100 people 
- **no_insurance**:percentage of the civilian non-institutionalized population with no health insurance coverage 
- **metro**: county is either nonmetro, which includes open countryside, rural towns, or smaller cities with up to 49,999 people, or metro



:::

::::
:::::

:::::{.my-watch-out}
:::{.my-watch-out-header}
WATCH OUT! Do missing values have a pattern?
:::
::::{.my-watch-out-container}
We know from @tbl-chap09-show-data that the variable `hiv_prevalence` has many missing values. In all the forthcoming analyses we will remove those 70 `NAs` and work with complete cases. 70 NA’s in a sample of 500 is with 14% a big proportion from the available data. The question arises: Is there a reason why there are so many missing values? Could it be that this reason is distorting our analysis?

Most of the time I have provided code that suppresses these warnings. This is a dangerous enterprise as it could bias results and conclusions without knowledge of the researcher. I think that a more prudent approach would need an analysis of the missing values. I do not know how to do this yet, but with {**naniar**} (@pak-naniar) there is a package for exploring missing data structures. Its website and package has [several vignettes](https://naniar.njtierney.com/) to learn its functions and there is also an scientific article about the package [@tierney2023].

Exploring missing data structures is in the book no planned achievement, therefore it is here enough to to get rid of the NA’s and to follow the books outline. But I am planning coming back to this issue and learn how to address missing data structures appropriately.

::::
:::::


## Achievement 1: Explorative Data Analysis {#sec-chap09-achievement1}

### Introduction

Instead following linearly the chapter I will try to compute my own `r glossary("ExDA", "EDA")`. I will try three different method:

1. Manufacturing the data and graphs myself. Writing own functions and using {**tidyverse**} packages to provide summary plots and statistics.
2. Trying out the `graphics::pairs()` function.
3. Experimenting with {**GGally**}, an extension package to {**ggplot2**} where one part (`GGally::ggpairs()`) is the equivalent to the base R `graphics::pairs()` function.

I will apply the following steps:

:::::{.my-procedure}
:::{.my-procedure-header}
:::::: {#prp-chap09-eda-steps}
: Some useful steps to explore data for regression analysis
::::::
:::
::::{.my-procedure-container}
Order and completeness of the following tasks is not mandatory.

1. **Browse the data**: 
    - **RStudio Data Explorer**: I am always using the data explorer in RStudio to get my first impression of the data. Although this step is not reproducible it forms my frame of mind what EDA steps I should follow and if there are issues I need especially to care about. 
    - **Skim data**: Look at the data with `skimr::skim()` to get a holistic view of the data: names, data types, missing values, ordered (categorical) minimum, maximum, mean, sd, distribution (numerical).
    - **Read the codebook:** It is important to understand what the different variables mean.
    - **Check structure:** Examine with `utils::str()` if the dataset has special structures, e.g. labelled data, attributes etc.
    - **Glimpse actual data**: To get a feeling about data types and actual values use `dplyr::glimpse()`.
    - **Glance at example rows**: As an alternative of `utils::head()` / `utils::tails()` get random row examples including first and last row of the dataset with my own function `glance_data()`.
2. **Check normality assumption**:
    - **Draw histograms of numeric variables**: To get a better idea I have these histogram overlaid with the theoretical normal distribution and the density curve of the current data. The difference between these two curves gives a better impression if normality is met or not.
    - **Draw Q-Q plot of numeric variables** Q-Q plots gives even a more detailed picture if normality is met.
    - **Compute normality tests**: If your data has less than 5.000 rows then use the Shapiro-Wilk test, otherwise the Anderson-Darling test. 
3. **Check homogeneity assumption**: If the normality assumption is not met, then test if the homogeneity of variance assumption between groups is met with Levene’s test or with the more robust Fligner-Killeen’s test. In the following steps use always median instead of mean and do not compute the Pearson’s r but the Spearman’s rho coefficient. 
4. **Compute correlation coefficient**: Apply either Pearson’s r or the Spearman’s rho coefficient. There are function like `graphics::pairs()` or `GGally::ggpairs()` that provide a graphical and statistical representation of all combinations of bivariate relationships.
5. **Explore categorical data with box plots or violin plots**: Box plots work well between a numerical and categorical variable. You could also overlaid the data and violin plots to maximize the information in one single graph.
::::
:::::


:::::{.my-example}
:::{.my-example-header}
:::::: {#exm-chap09-eda}
: Explorative Data Analysis for chapter 9
::::::
:::
::::{.my-example-container}

::: {.panel-tabset}


###### tableone

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap09-tableone}
: Numbered R Code Title
::::::
:::
::::{.my-r-code-container}
```{r}
#| label: tbl-tableone
#| tbl-cap: "Descriptive statics with the 'tableone' package"

tableone::CreateTableOne(data = distance_ssp_clean,
                         vars = c('dist_ssp', 'hiv_prevalence',
                                  'opioid_rate', 'no_insurance',
                                  'metro'))

```
***

`skimr::skim()` is a much better alternative! The second version with the median instead of the mean is not necessary because it is in `skimr::skim()` integrated.
::::
:::::


###### Histograms

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap09-eda-histograms}
: Histograms of numeric variables
::::::
:::
::::{.my-r-code-container}

```{r}
#| label: fig-eda-histograms
#| fig-cap: "Histograms for numeric variables of chapter 9"
#| fig-height: 8


hist_distance <- my_hist_dnorm(
    df = distance_ssp_clean,
    v = distance_ssp_clean$dist_ssp,
    n_bins = 30,
    x_label = "Nearest syringe services program in miles"
) 

hist_hiv <- my_hist_dnorm(
    df = distance_ssp_clean,
    v = distance_ssp_clean$hiv_prevalence,
    n_bins = 30,
    x_label = "People with diagnosed HIV per 100,000"
) 

hist_opioid <- my_hist_dnorm(
    df = distance_ssp_clean,
    v = distance_ssp_clean$opioid_rate,
    n_bins = 30,
    x_label = "Opioid prescriptions per 100 people"
)

hist_insurance <- my_hist_dnorm(
    df = distance_ssp_clean,
    v = distance_ssp_clean$no_insurance,
    n_bins = 30,
    x_label = "Percentage with no health insurance coverage"
)

gridExtra::grid.arrange(
   hist_distance, hist_hiv, hist_opioid, hist_insurance, nrow = 2
)
```

***
I developed a function where I can overlay the theoretical normal distribution and the density of the current data. The difference between the two curves gives an indication if we have a normal distribution.

From our data we see that the biggest difference is between SPP distance and HIV prevalence. This right skewed distribution could also be detected from other indicator already present in the `skimr::skim()`view of @tbl-chap09-show-data:
- The small histogram on the right is the most right skewed distribution.
- The standard deviation of `hiv_prevalence` is the only one, that is bigger than the mean of the variable. 
- There is a huge difference between mean and the median (p50) where the mean is much bigger than the median (= right skewed distribution), e.g. there is a long tail to the right as can also be seen in the tiny histogram.

Aside from `hiv_prevalence` the variable `distance_ssp` is almost equally right skewed. The situation seems better for the rest of the numeric variables. But let's manufacture `r glossary("Q-Q-Plot", "Q-Q plots")` for all of them to see more in detail if they are normally distributed or not.





::::
:::::


###### Q-Q plots

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap09-eda-qq-plots}
: Q-Q plots of numeric variables
::::::
:::
::::{.my-r-code-container}

```{r}
#| label: fig-eda-qq-plots
#| fig-cap: "Q-Q plots for numeric variables of chapter 9"
#| fig-height: 8

qq_distance <- my_qq_plot(
    df = distance_ssp_clean,
    v  = distance_ssp_clean$dist_ssp,
    col_qq = "Distance to SSP"
)

qq_hiv <- my_qq_plot(
    df = distance_ssp_clean,
    v  = distance_ssp_clean$hiv_prevalence,
    col_qq = "HIV diagnosed"
)

qq_opioid <- my_qq_plot(
    df = distance_ssp_clean,
    v  = distance_ssp_clean$opioid_rate,
    col_qq = "Opioid prescriptions"
)

qq_insurance <- my_qq_plot(
    df = distance_ssp_clean,
    v  = distance_ssp_clean$no_insurance,
    col_qq = "Health insurance"
)


gridExtra::grid.arrange(
   qq_distance, qq_hiv, qq_opioid, qq_insurance, nrow = 2
)
```

***
It turned out that all four numeric variables are not normally distributed. Some of them looked in the histograms quite OK, because the differences to the normal distribution on the lower and upper end of the data compensate each other.

Testing normality with `r glossary("Shapiro-Wilk")` or `r glossary("Anderson-Darling")` test will show that they are definitely not normally distributed.

::::
:::::

###### Normality

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap09-normality-test}
: Normality checking with Shapiro-Wilk & Anderson-Darling tests
::::::
:::
::::{.my-r-code-container}
```{r}
#| label: tbl-normality-test
#| tbl-cap: "Testing normality with Shapiro-Wilk & Anderson-Darling tests"

dist_test <-  stats::shapiro.test(distance_ssp_clean$dist_ssp)
hiv_test <-  stats::shapiro.test(distance_ssp_clean$hiv_prevalence)
opioid_test <- stats::shapiro.test(distance_ssp_clean$opioid_rate)
insurance_test <- stats::shapiro.test(distance_ssp_clean$no_insurance)

dist_test2 <-  nortest::ad.test(distance_ssp_clean$dist_ssp)
hiv_test2 <-  nortest::ad.test(distance_ssp_clean$hiv_prevalence)
opioid_test2 <- nortest::ad.test(distance_ssp_clean$opioid_rate)
insurance_test2 <- nortest::ad.test(distance_ssp_clean$no_insurance)


normality_test <- 
    dplyr::bind_rows(
        broom:::glance.htest(dist_test),
        broom:::glance.htest(hiv_test),
        broom:::glance.htest(opioid_test),
        broom:::glance.htest(insurance_test),
        broom:::glance.htest(dist_test2),
        broom:::glance.htest(hiv_test2),
        broom:::glance.htest(opioid_test2),
        broom:::glance.htest(insurance_test2)
    ) |> 
    dplyr::bind_cols(
        variable = c("dist_ssp", "hiv_prevalence",
                     "opioid_rate", "no_insurance",
                     "dist_ssp", "hiv_prevalence",
                     "opioid_rate", "no_insurance")
    ) |> 
    dplyr::relocate(variable)

normality_test
```

***

The `r glossary("p-value", "p-values")` from both tests are for all four variables very small, e.g. statistically significant. Therefore we have to reject the Null that they are normally distributed.


::::
:::::


::: {.callout-tip}
It turned out that all four variable are not normally distributed. We can't therefore not use `r glossary("Pearson", "Pearson’s r coefficient")`. 
:::

Before we are going to use `r glossary("Spearman", "Spearman’s rho")` let's check the homogeneity of variance assumption (`r glossary("homoscedasticity")`) with a scatterplot with `lm` and `loess` curve  and using `r glossary("Levene", "Levene’s Test")` and the `r glossary("Fligner", "Fligner-Killeen’s test")`.

###### Scatterplots

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap09-eda-scatterplots}
: Scatterplots of numeric variables
::::::
:::
::::{.my-r-code-container}
```{r}
#| label: fig-eda-scatterplots
#| fig-cap: "Scatterplots of numeric variables"
#| fig-height: 10

scatter_dist_hiv <-  my_scatter(
    df = distance_ssp_clean,
    v =  distance_ssp_clean$hiv_prevalence,
    w =  distance_ssp_clean$dist_ssp,
    x_label = "HIV prevalence",
    y_label = "Distance to SSP"
)

scatter_dist_opioid <-  my_scatter(
    df = distance_ssp_clean,
    v =  distance_ssp_clean$opioid_rate,
    w =  distance_ssp_clean$dist_ssp,
    x_label = "Opioid rate",
    y_label = "Distance to SSP"
)

scatter_dist_insurance <-  my_scatter(
    df = distance_ssp_clean,
    v =  distance_ssp_clean$no_insurance,
    w =  distance_ssp_clean$dist_ssp,
    x_label = "No insurance",
    y_label = "Distance to SSP"
)

gridExtra::grid.arrange(
   scatter_dist_hiv, scatter_dist_opioid, scatter_dist_insurance, nrow = 3
)

```

::::
:::::





###### Homogeneity

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap09-text-homogeneity}
: Testing homogeneity of variances with Levene’s and Fligner-Killeen’s test
::::::
:::
::::{.my-r-code-container}
```{r}
#| label: tbl-chap09-text-homogeneity
#| tbl-cap: "Homogeneity of variances tested with Levene’s and Fligner-Killeen’s test"

hiv_test <-  stats::fligner.test(
    distance_ssp_clean$dist_ssp,
    distance_ssp_clean$hiv_prevalence
    )
opioid_test <- stats::fligner.test(
    distance_ssp_clean$dist_ssp,
    distance_ssp_clean$opioid_rate
    )
insurance_test <- stats::fligner.test(
    distance_ssp_clean$dist_ssp,
    distance_ssp_clean$no_insurance
    )

hiv_test2 <-  car::leveneTest(
    distance_ssp_clean$dist_ssp,
    distance_ssp_clean$hiv_prevalence
    )
opioid_test2 <- car::leveneTest(
    distance_ssp_clean$dist_ssp,
    distance_ssp_clean$opioid_rate
    )
insurance_test2 <- car::leveneTest(
    distance_ssp_clean$dist_ssp,
    distance_ssp_clean$no_insurance
    )


homogeneity_test <- 
    dplyr::bind_rows(
        broom::tidy(hiv_test2),
        broom::tidy(opioid_test2),
        broom::tidy(insurance_test2)
    ) |> 
    dplyr::mutate(method = "Levene's Test for Homogeneity of Variance") |> 
    dplyr::bind_rows(
        broom:::glance.htest(hiv_test),
        broom:::glance.htest(opioid_test),
        broom:::glance.htest(insurance_test),
    ) |> 
    dplyr::bind_cols(
        variable = c("dist_hiv",
                     "dist_opioid", 
                     "dist_insurance",
                     "dist_hiv",
                     "dist_opioid", 
                     "dist_insurance"
                 )
    ) |> 
    dplyr::relocate(variable)

homogeneity_test
```
***

All p-values are higher than the threshold of .05 and are therefore not statistically significant. The Null must not rejected, the homogeneity of variance assumption for all variables is met.
::::
:::::


###### pairs

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap09-plot-pairs}
: Scatterplots of variable pairs from dataset of chapter 9
::::::
:::
::::{.my-r-code-container}
```{r}
#| label: fig-plot-pairs
#| fig-cap: "Scatterplots of variable pairs from dataset of chapter 9"
#| fig-height: 10
#| fig-width: 10


lt_purple <- t_col("purple3", perc = 50, name = "lt.purple")

panel.hist <- function(x, ...)
{
    usr <- par("usr")
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "grey80", ...)
}


graphics::pairs(distance_ssp_clean[3:6], 
                pch = 23, 
                panel = panel.smooth,
                cex = 1.5, 
                bg = lt_purple, 
                horOdd = TRUE,
                diag.panel = panel.hist, 
                cex.labels = 2, 
                font.labels = 2,
                gap = 0
                )

```

::::
:::::

###### metro

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap09-plot-pairs-metro}
: Distance to SSP with different numeric variable and metro/nonmetro location
::::::
:::
::::{.my-r-code-container}
```{r}
#| label: fig-plot-pairs-metro
#| fig-cap: "Distance to SSP -- Metro (red) & Nonmetro (blue)"
#| fig-height: 10
#| fig-width: 10

panel.hist <- function(x, ...) {
    usr <- par("usr")
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "grey80", ...)
}

graphics::pairs(distance_ssp_clean[3:6], 
                main = "Distance to SSP -- Metro (red) & Nonmetro (blue)",
                panel = panel.smooth,
                horOdd = TRUE,
                diag.panel = panel.hist, 
                pch = 21, 
                gap = 0,
                bg = c("red", "blue")[unclass(distance_ssp_clean$metro)])

```

::::
:::::

###### ggpairs

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap09-plot-ggpairs}
: Scatterplots of variable pairs from dataset of chapter 9
::::::
:::
::::{.my-r-code-container}
```{r}
#| label: fig-plot-ggpairs
#| fig-cap: "Scatterplots of variable pairs from dataset of chapter 9"
#| fig-height: 10
#| fig-width: 10

GGally::ggpairs(distance_ssp_clean,
                columns = 3:7)

```

::::
:::::




:::

::::
:::::



## Exercises (empty)

## Glossary

```{r}
#| label: glossary-table
#| echo: false

glossary_table()
```

------------------------------------------------------------------------


## Session Info {.unnumbered}

:::::{.my-r-code}
:::{.my-r-code-header}
Session Info
:::
::::{.my-r-code-container}

```{r}
#| label: session-info

sessioninfo::session_info()
```


::::
:::::
